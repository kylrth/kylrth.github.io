<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kyle Roth</title><link>https://kylrth.com/</link><description>Recent content on Kyle Roth</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 17 Apr 2025 13:19:47 -0400</lastBuildDate><atom:link href="https://kylrth.com/index.xml" rel="self" type="application/rss+xml"/><item><title>Nonviolent communication: a language of life</title><link>https://kylrth.com/book/nonviolent-communication/</link><pubDate>Fri, 25 Oct 2024 15:39:56 -0400</pubDate><guid>https://kylrth.com/book/nonviolent-communication/</guid><description>&lt;h2 id="feelings-over-judgments">feelings over judgments &lt;a class="header-link" href="#feelings-over-judgments">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>To communicate nonviolently, we need to clearly communicate our feelings, rather than our judgments. Maturity means differentiating our feelings and experiences into as many parts and distinctions as a well-trained orchestral listener. &amp;ldquo;I feel that&amp;rdquo; or &amp;ldquo;I feel when&amp;rdquo; is expressing an opinion, rather than a feeling. Even &amp;ldquo;I feel inadequate&amp;rdquo; would be better expressed as &amp;ldquo;I feel disappointed with my performance&amp;hellip;&amp;rdquo; &amp;ldquo;I feel good about &amp;hellip;&amp;rdquo; is not specific enough.&lt;/p></description></item><item><title>Thinking in systems: a primer</title><link>https://kylrth.com/book/thinking-in-systems/</link><pubDate>Mon, 15 Jul 2024 11:17:09 -0400</pubDate><guid>https://kylrth.com/book/thinking-in-systems/</guid><description>&lt;p>A system is something that is composed of parts that have interrelations between them.&lt;/p>
&lt;p>Each system has purposes, and those purposes may have nothing to do with stated goals. For example, if a government says it wants to protect the environment but allocates few resources to that goal, its purpose is not to protect the environment, but something else. A university may have the purpose of teaching students, while the components of the university have different goals: the student is trying to get good grades, the professor is trying to produce good research, and the administrator is trying to balance the budget.&lt;/p></description></item><item><title>Bowling alone: the collapse and revival of American community</title><link>https://kylrth.com/book/bowling-alone/</link><pubDate>Mon, 15 Jul 2024 10:36:57 -0400</pubDate><guid>https://kylrth.com/book/bowling-alone/</guid><description>&lt;h2 id="the-trends">the trends &lt;a class="header-link" href="#the-trends">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>politics&lt;/strong>: There is now more money in politics, and less individual involvement. There are more total associations, but they look more like the AARP and Greenpeace, instead of unions, churches, bridge clubs, rotary.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>religion&lt;/strong>: Attendance has definitely gone down in the last 40 years, even if the number of people who profess stays nearly the same. (This has changed since the book was published on January 1, 2000.) The 60s and 70s saw people make their religion more private, more personal, making people less concerned with the communal support of a shared faith. Fundamentalist religion (including evangelical) results in more service and giving, but primarily within the church community. So as the mainline middle falls out, religion is becoming a less helpful societal lubricant overall, despite the fact that religiosity has generally meant more volunteerism and giving.&lt;/p></description></item><item><title>The whole-brain child: revolutionary strategies to nurture your child's developing mind</title><link>https://kylrth.com/book/whole-brain-child/</link><pubDate>Mon, 15 Jul 2024 10:21:23 -0400</pubDate><guid>https://kylrth.com/book/whole-brain-child/</guid><description>&lt;p>&lt;em>The &amp;ldquo;refrigerator sheet&amp;rdquo; found &lt;a href="https://drdansiegel.com/whole-brain-child-handouts/">here&lt;/a> has a nice summary of the 12 strategies presented in this book.&lt;/em>&lt;/p>
&lt;p>This was a great book full of really insightful strategies to approaching mental integration. My only complaint is that they claim the strategies are neuroscientific, when in fact what they&amp;rsquo;re doing is using brain metaphors to teach real concepts. The left- and right-brain metaphor is &lt;a href="https://en.wikipedia.org/wiki/Lateralization_of_brain_function#Society_and_culture">taken too far&lt;/a> in popular culture, and I think this book over-emphasizes the importance of these metaphors in order to argue for the parenting strategies here. I was especially bothered by how they used the concept of &lt;a href="https://en.wikipedia.org/wiki/Mirror_neuron">mirror neurons&lt;/a> to argue for the sociality of the brain, as if that alone were a sufficient argument that we should teach kids to think about other people. The recommended strategies for teaching kids to see others are &amp;ldquo;enjoy each other&amp;rdquo; and &amp;ldquo;connect through conflict&amp;rdquo; which, while probably useful, do not depend on mirror neurons in any particular way.&lt;/p></description></item><item><title>Speaking as from the Dust: ideologies of AI and digital resurrection in Mormon culture</title><link>https://kylrth.com/paper/speaking-as-from-the-dust/</link><pubDate>Wed, 08 May 2024 15:03:18 -0400</pubDate><guid>https://kylrth.com/paper/speaking-as-from-the-dust/</guid><description>&lt;p>I wrote this paper with Stephen Betts, a friend of mine doing Mormon Studies at the University of Virginia. (Click the title to download the preprint.) Our interest in the topic was initially piqued when we stumbled across the &lt;a href="https://wilfordwoodruffpapers.org/wilford-woodruff-ai-learning-experience">Wilford Woodruff AI Learning Experience&lt;/a>, and this paper explores the unique intersection of Mormonism, transhumanism, and machine learning that makes such a thing possible.&lt;/p>
&lt;p>We&amp;rsquo;re currently looking for a venue to publish this work. &lt;strong>Any feedback on the preprint is appreciated!&lt;/strong>&lt;/p></description></item><item><title>Better, Nicer, Clearer, Fairer: a critical assessment of the movement for ethical artificial intelligence and machine learning</title><link>https://kylrth.com/paper/better-nicer-cleaner-fairer/</link><pubDate>Tue, 24 Oct 2023 12:53:31 -0400</pubDate><guid>https://kylrth.com/paper/better-nicer-cleaner-fairer/</guid><description>&lt;p>&lt;em>I will present this paper in the FATE (fairness, accountability, transparency, ethics) reading group tomorrow (2023-10-25). You can view the slides I&amp;rsquo;ll use &lt;a href="https://docs.google.com/presentation/d/1Bl4Bfh-ryFfmsEVrmdnYBe69hKjZP4mYZppvZHU5-yM/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p>
&lt;p>There are unresolved tensions in the algorithmic ethics world. Here are two examples:&lt;/p>
&lt;ul>
&lt;li>Is inclusion always good?
&lt;ul>
&lt;li>Gebru: &amp;ldquo;you can&amp;rsquo;t have ethical A.I. that&amp;rsquo;s not inclusive&amp;hellip; [a]nd whoever is creating the technology is setting the standards&amp;rdquo;&lt;/li>
&lt;li>Nelson: &amp;ldquo;&amp;hellip; I struggle to understand why we want to make black communities more cognizant in facial recognition systems that are disproportionately used for surveillance.&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>academic activism
&lt;ul>
&lt;li>O&amp;rsquo;Neil: why is there a lack of academic efforts to inform policymakers and regulators?&lt;/li>
&lt;li>PERVADE: Academics have been doing this work for a while but it is underfunded, marginalized, and at odds with a US political apparatus generally favorable towards Silicon Valley.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Ethics manifestos or value statements mask these tensions behind a business ethics lens.&lt;/p></description></item><item><title>The right side of history: how reason and moral purpose made the West great</title><link>https://kylrth.com/book/right-side-of-history/</link><pubDate>Thu, 12 Oct 2023 13:36:03 -0400</pubDate><guid>https://kylrth.com/book/right-side-of-history/</guid><description>&lt;p>&lt;em>The main text below is my summary of the points in the book, and I&amp;rsquo;ve put my commentary in sidenotes. If you want a Marxist, postmodern response to these ideas from the devil himself, check out&lt;/em> &lt;a href="https://www.youtube.com/watch?v=SEMB1Ky2n1E">Jordan Peterson &amp;amp; the meaning of life&lt;/a> &lt;em>by Philosophy Tube.&lt;/em>&lt;/p>
&lt;p>Reason and moral purpose come from Athens and Jerusalem, and without those things the West would not be where it is today. Socialism means taking handouts from the nanny state, and while our society continues to function with capitalism bearing the weight of socialist programs, it&amp;rsquo;s in the process of crumbling. People on the left like to blame institutions and systems for current woes, when in reality this is the freest, most egalitarian society that has ever existed. That&amp;rsquo;s why we see the conflict over political divides strengthening in America. The West is losing its attachment to reason and moral purpose, instead shifting to intersectionality, hedonism, and scientific materialism. Happiness is built on a sense of moral purpose, and is achieved by maximizing individuals&amp;rsquo; ability to &lt;em>pursue&lt;/em> that happiness. Scientific materialism and atheism are problematic because they take away that moral purpose.&lt;/p></description></item><item><title>Killing sacred cows: overcoming the financial myths that are destroying your prosperity</title><link>https://kylrth.com/book/killing-sacred-cows/</link><pubDate>Thu, 12 Oct 2023 13:06:34 -0400</pubDate><guid>https://kylrth.com/book/killing-sacred-cows/</guid><description>&lt;p>&lt;em>This book made me cringe pretty often. I&amp;rsquo;ll leave exclamation points (!) on particularly cringe-worthy ideas, just so you can &lt;a href="https://twitter.com/metalgearobama/status/1344102150259974150?lang=en">see how based I am&lt;/a>.&lt;/em>&lt;/p>
&lt;p>This book functions on the idea that if we accept a new set of words: poverty mindset, abundance mindset, etc, we&amp;rsquo;ll become wealthy.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Zero-sum mindset&lt;/strong>: any critique of &amp;ldquo;success&amp;rdquo;, including environmental devastation&lt;/li>
&lt;li>&lt;strong>Scarcity/poverty mindset&lt;/strong>: saving your way to wealth, judging or being jealous of those who have things, being too safe or too risky with your money, avoiding the risk of seeking your dreams, any ideas that make you make bad decisions&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>If more people live with an abundance mindset, we will all experience less hardship.&lt;/p></description></item><item><title>Raising an emotionally intelligent child: the heart of parenting</title><link>https://kylrth.com/book/raising-emotionally-intelligent-child/</link><pubDate>Thu, 12 Oct 2023 12:57:20 -0400</pubDate><guid>https://kylrth.com/book/raising-emotionally-intelligent-child/</guid><description>&lt;p>Really fantastic book. Here is a quote that was extremely key for me:&lt;/p>
&lt;blockquote>
&lt;p>Understand your base of power as a parent. By base of power, I mean the element in the parent-child relationship that makes it possible for parents to set limits on children&amp;rsquo;s misbehavior, something all kids want and need. For some parents, the base of power is threats, humiliation, or spanking. Others, who are overly permissive, may feel they have no base of power at all. For emotion coaching parents, the base of power is the emotional bond between parent and child. When you are emotionally connected to your child, limit-setting comes out of your genuine reactions to your child&amp;rsquo;s misbehavior. Your child responds to your anger, disappointment, and worries, so you don&amp;rsquo;t have to resort to negative consequences such as spanking and time-outs to amplify your feelings. The respect and affection you and your child have for each other become your primary vehicle for limit-setting.&lt;/p></description></item><item><title>Why we sleep: unlocking the power of sleep and dreams</title><link>https://kylrth.com/book/why-we-sleep/</link><pubDate>Wed, 11 Oct 2023 22:00:32 -0400</pubDate><guid>https://kylrth.com/book/why-we-sleep/</guid><description>&lt;p>&lt;em>There is &lt;a href="https://guzey.com/books/why-we-sleep/">controversy&lt;/a> around some of the claims made in this book, so I don&amp;rsquo;t take any single point of evidence extremely seriously. But the person who critiqued the book also has some weird ideas about sleep,&lt;span class="sidenote-number">&lt;small class="sidenote">For instance, he believes that we evolved in a sleep-deprived environment so sleep deprivation must be healthier for us.&lt;/small>&lt;/span>
so in the end I think I probably side with the sleep scientist for most issues except when he seems extreme. Isn&amp;rsquo;t epistemology fun?&lt;/em>&lt;/p></description></item><item><title>Team Human interview with Dennis Yi Tenen</title><link>https://kylrth.com/post/team-human-tenen/</link><pubDate>Mon, 09 Oct 2023 10:34:12 -0400</pubDate><guid>https://kylrth.com/post/team-human-tenen/</guid><description>&lt;p>&lt;em>You can view this episode &lt;a href="https://www.teamhuman.fm/episodes/265-dennis-yi-tenen">here&lt;/a>, and you can download a transcript I made with whisper-medium &lt;a href="https://dl.kylrth.com/teamhuman_tenen.txt">here&lt;/a>. I accept responsibility for errors in the transcript, alongside OpenAI, all people whose voices exist on the web, and the rest of humanity. :)&lt;/em>&lt;/p>
&lt;h2 id="writing-technology">writing technology &lt;a class="header-link" href="#writing-technology">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;blockquote>
&lt;p>&lt;strong>DYT&lt;/strong>: You know, it took humans like centuries to perfect the technology of a dictionary and it took hundreds, thousands of people, probably like millions of hours to actually get to the point where you can easily look up a word.&lt;/p></description></item><item><title>InstructEval: systematic evaluation of instruction selection methods</title><link>https://kylrth.com/paper/instructeval/</link><pubDate>Mon, 25 Sep 2023 11:38:24 -0400</pubDate><guid>https://kylrth.com/paper/instructeval/</guid><description>&lt;p>&lt;em>This was a paper I presented about in Bang Liu&amp;rsquo;s research group meeting on 2023-09-25. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/1Qo_KPNnkj2jQzYDKG1wSEisHAr18Fs_eHq5Iuzj0MNs/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>thoughts on technology ownership</title><link>https://kylrth.com/post/tech-ownership/</link><pubDate>Thu, 07 Sep 2023 12:11:06 -0400</pubDate><guid>https://kylrth.com/post/tech-ownership/</guid><description>&lt;p>Having heard about the &lt;a href="https://foundation.mozilla.org/en/privacynotincluded/articles/its-official-cars-are-the-worst-product-category-we-have-ever-reviewed-for-privacy/">terrible privacy policies of car manufacturers&lt;/a> and &lt;a href="https://www.theverge.com/2023/3/3/23624328/ford-self-repossessing-car-patent-connected-car-nightmare">Ford&amp;rsquo;s new patent to automate vehicle repossession&lt;/a>, I was finally able to formulate some thoughts I&amp;rsquo;ve had brewing for a while about ownership in this world of ubiquitous computing.&lt;/p>
&lt;p>Before the industrial revolution, humans could essentially understand what a thing did by looking at it, and ownership was about possession of the object. Now we don&amp;rsquo;t understand our tools, and so possession does not imply control.&lt;/p></description></item><item><title>"Low-resource" text classification: a parameter-free classification method with compressors</title><link>https://kylrth.com/paper/gzip-text-classification/</link><pubDate>Mon, 24 Jul 2023 11:35:13 -0400</pubDate><guid>https://kylrth.com/paper/gzip-text-classification/</guid><description>&lt;p>&lt;em>I presented this paper in Bang Liu&amp;rsquo;s research group meeting on 2023-07-24. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/1w4n4UCWegJlDTCjKqWyTHOb3QHpnz1G2kTV6j_MODqY/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p>
&lt;p>It seems like the authors made a mistake that inflated the scores for the multilingual experiments, &lt;a href="https://kenschutte.com/gzip-knn-paper/">according to Ken Schutte&lt;/a>.&lt;/p></description></item><item><title>the NYT AI explainer misses the point</title><link>https://kylrth.com/post/nyt-ai-explainer/</link><pubDate>Mon, 17 Apr 2023 10:07:35 -0400</pubDate><guid>https://kylrth.com/post/nyt-ai-explainer/</guid><description>&lt;p>In late March 2023, the NYT released a series of explainer articles about AI. The &lt;a href="https://www.nytimes.com/article/ai-artificial-intelligence-chatbot.html">first article in the series&lt;/a>&lt;span class="sidenote-number">&lt;small class="sidenote">You can also &lt;a href="https://web.archive.org/web/20230415130643/https://www.nytimes.com/article/ai-artificial-intelligence-chatbot.html">read it on Archive.org&lt;/a> if you don&amp;rsquo;t have a subscription.&lt;/small>&lt;/span>
characterizes the recent history of AI as a progression of new technological ideas appearing over time. Of course that&amp;rsquo;s partially true, but it gets the order wrong and misses important non-technical events that are key to understanding our current position.&lt;/p></description></item><item><title>the inherent subjectivity of reality</title><link>https://kylrth.com/post/edward-frenkel/</link><pubDate>Fri, 14 Apr 2023 14:45:44 -0400</pubDate><guid>https://kylrth.com/post/edward-frenkel/</guid><description>&lt;p>&lt;em>These are some thoughts I&amp;rsquo;ve had while listening to a Lex Fridman &lt;a href="https://lexfridman.com/edward-frenkel/">interview&lt;/a> with Edward Frenkel, a mathematician at UC Berkeley working on mathematical quantum physics.&lt;/em>&lt;/p>
&lt;p>In the information age, we like to see everything as computation. But what do we mean when we say that something is computation? We mean that a physical system with predictable interactions has a meaningful result. If we somehow learned that the universe was computational in nature, the only thing that adds is that the universe&amp;rsquo;s state is meaningful somehow.&lt;/p></description></item><item><title>Winners take all: the elite charade of changing the world</title><link>https://kylrth.com/book/winners-take-all/</link><pubDate>Mon, 20 Mar 2023 14:48:27 -0400</pubDate><guid>https://kylrth.com/book/winners-take-all/</guid><description>&lt;p>This book was a good one for quotable critique of modern capitalism. Here are some good ones:&lt;/p>
&lt;blockquote>
&lt;p>These elites believe and promote the idea that social change should be pursued principally through the free market and voluntary action, not public life and the law and the reform of the systems that people share in common; that it should be supervised by the winners of capitalism and their allies, and not be antagonistic to their needs; and that the biggest beneficiaries of the status quo should play a leading role in the status quo’s reform.&lt;/p></description></item><item><title>for a socially beneficial and responsible development of AI</title><link>https://kylrth.com/post/bengio-crawford/</link><pubDate>Mon, 20 Mar 2023 14:04:51 -0400</pubDate><guid>https://kylrth.com/post/bengio-crawford/</guid><description>&lt;p>&lt;em>These are my notes from a conversation between &lt;a href="https://en.wikipedia.org/wiki/Yoshua_Bengio">Yoshua Bengio&lt;/a> and &lt;a href="https://en.wikipedia.org/wiki/Kate_Crawford">Kate Crawford&lt;/a> held at &lt;a href="https://mila.quebec/">Mila&lt;/a> on 2023-03-20, announcing the release of a new book created as a joint report between Mila and &lt;a href="https://en.wikipedia.org/wiki/UNESCO">UNESCO&lt;/a> called&lt;/em> Missing links in AI governance &lt;em>(&lt;a href="https://unesdoc.unesco.org/ark:/48223/pf0000384787">link&lt;/a>). There were news articles in French (&lt;a href="https://www.ledevoir.com/societe/785960/intelligence-artificielle-l-onu-et-le-mila-inquiets-des-derapages-potentiels-d-ia-comme-chatgpt">Le Devoir&lt;/a>), but not as many in English unfortunately (&lt;a href="https://www.datanami.com/this-just-in/mila-and-unesco-join-forces-to-emphasize-urgent-need-for-better-ai-governance/">Datanami&lt;/a>).&lt;/em>&lt;/p>
&lt;p>&lt;strong>Bengio&lt;/strong>: What motivates you to do what you do? This topic has turned from an academic move to a societal one really quickly, and it&amp;rsquo;s scary. We need to figure out what to do.&lt;/p></description></item><item><title>Think again: the power of knowing what you don't know</title><link>https://kylrth.com/book/think-again/</link><pubDate>Wed, 08 Mar 2023 15:08:24 -0500</pubDate><guid>https://kylrth.com/book/think-again/</guid><description>&lt;p>The book is divided into 3 parts, covering the value of rethinking, how to help others rethink, and how to help communities rethink.&lt;/p>
&lt;h2 id="the-value-of-rethinking">the value of rethinking &lt;a class="header-link" href="#the-value-of-rethinking">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>Who you are should be a question of what you value, not what you believe.&lt;/p>
&lt;p>When people change their answers on a test, they&amp;rsquo;re far more likely to change to the right answer than a wrong answer. Rethinking is effective!&lt;/p>
&lt;h2 id="helping-others-rethink">helping others rethink &lt;a class="header-link" href="#helping-others-rethink">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>Good families allow for healthy conflict, rather than avoiding it. Orville and Wilbur Wright fought a lot but it wasn&amp;rsquo;t relational conflict, it was task conflict. Having a good relationship with colleagues is important because it&amp;rsquo;s what keeps task conflict from bleeding into relational conflict. You need a network of people who will disagree with you. Silence disrespects the value of your views, and our ability to have civil disagreement.&lt;/p></description></item><item><title>the AI art debate</title><link>https://kylrth.com/post/ai-art/</link><pubDate>Wed, 08 Mar 2023 12:09:25 -0500</pubDate><guid>https://kylrth.com/post/ai-art/</guid><description>&lt;p>On 2022-11-03 a &lt;a href="https://githubcopilotlitigation.com/">class action lawsuit&lt;/a> was announced against GitHub Copilot on the basis of copyright infringement, and now (2023-01-13) there&amp;rsquo;s &lt;a href="https://web.archive.org/web/20230114170709/https://stablediffusionlitigation.com/">one&lt;/a> for stable diffusion (against StabilityAI and friends). Browsing through r/StableDiffusion, I&amp;rsquo;m seeing lots of posts like &lt;a href="https://www.reddit.com/r/StableDiffusion/comments/10e13r3/the_lawyers_suing_stable_diffusion_when_you_tell/">this&lt;/a> making the very memeable point that 5 billion images can&amp;rsquo;t be stored in a 4 GB model. &lt;a href="https://www.reddit.com/r/StableDiffusion/comments/10e13r3/the_lawyers_suing_stable_diffusion_when_you_tell/j4oeny6/?context=3">From the original poster&lt;/a>:&lt;span class="sidenote-number">&lt;small class="sidenote">The thumbnail for this post was generated with stable diffusion. See the alt text for details. Yes, I&amp;rsquo;m not great at this.&lt;/small>&lt;/span>&lt;/p></description></item><item><title>Army of none: autonomous weapons and the future of war</title><link>https://kylrth.com/book/army-of-none/</link><pubDate>Tue, 14 Feb 2023 13:33:19 -0500</pubDate><guid>https://kylrth.com/book/army-of-none/</guid><description>&lt;p>The examples in this book make it clear that there is no easy line we can draw between autonomous and non-autonomous weapons (and by extension, autonomous AI agents). There is a smooth gradient of autonomy, which makes the question of allowing autonomous weapons much more nuanced. It&amp;rsquo;s probably the case that higher-level alignment becomes important proportionally to the level of autonomy and intelligence.&lt;/p>
&lt;p>He analyzes the Patriot fratricides,&lt;span class="sidenote-number">&lt;small class="sidenote">In a military context, the word &lt;em>fratricide&lt;/em> means the killing of someone on the same side of a conflict.&lt;/small>&lt;/span>
and ends up blaming the individuals involved for automation bias. I would say that these humans in the system were set up to fail by the training and the functioning of the system. They&amp;rsquo;re expected to decide whether the computer is right, with only seconds to decide. He acknowledges this later when he talks about Aegis.&lt;/p></description></item><item><title>Conscious: a brief guide to the fundamental mystery of the mind</title><link>https://kylrth.com/book/conscious/</link><pubDate>Mon, 13 Feb 2023 16:57:49 -0500</pubDate><guid>https://kylrth.com/book/conscious/</guid><description>&lt;p>The first thing you should know is that this book is only 2 hours long in audio form. That&amp;rsquo;s short! She managed to get into the difficult details while keeping the jargon accessible. (It helps that we literally know nothing about consciousness.)&lt;/p>
&lt;p>This book argued alternately for neuroscientific, illusionist, and panpsychist theories of consciousness. It explains the hard problem of consciousness, and then later &lt;em>really&lt;/em> explains why it&amp;rsquo;s so hard. As a science-oriented person, t&amp;rsquo;s so easy to forget why it&amp;rsquo;s hard and start to explain things purely physically. When I do that, I end up thinking there&amp;rsquo;s got to be a neuroscience explanation. When I pay attention once again to my own experience, I lean towards some kind of panpsychism. The author really lays bare her own thoughts, and carefully spends time walking us through simple thought experiments to fight off the assumptions that we bring to the word &amp;ldquo;consciousness&amp;rdquo;.&lt;/p></description></item><item><title>Artificial intelligence, values, and alignment</title><link>https://kylrth.com/paper/ai-values-alignment/</link><pubDate>Fri, 10 Feb 2023 08:09:15 -0500</pubDate><guid>https://kylrth.com/paper/ai-values-alignment/</guid><description>&lt;p>&lt;em>I presented this paper in Bang Liu&amp;rsquo;s research group meeting in two installments on 2023-02-20 and 2023-02-27, and also in Irina Rish&amp;rsquo;s scaling and alignment course (&lt;a href="https://sites.google.com/view/towards-agi-course/course-description">IFT6760A&lt;/a>) on 2023-03-07. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/1I4VPhMF32CDB3W3vWQl3TTy1GR5jxqSAAfSgjuk8enI/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;span class="sidenote-number">&lt;small class="sidenote">The thumbnail for this post was generated with stable diffusion! See the alt text for details.&lt;/small>&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>Behind each vision for ethically-aligned AI sits a deeper question. How are we to decide which principles or objectives to encode in AI—and who has the right to make these decisions—given that we live in a pluralistic world that is full of competing conceptions of value? Is there a way to think about AI value alignment that avoids a situation in which some people simply impose their views on others?&lt;/p></description></item><item><title>Unsolved problems in ML safety</title><link>https://kylrth.com/paper/unsolved-problems-ml-safety/</link><pubDate>Mon, 06 Feb 2023 11:39:33 -0500</pubDate><guid>https://kylrth.com/paper/unsolved-problems-ml-safety/</guid><description>&lt;p>&lt;em>This was a paper we presented about in Irina Rish&amp;rsquo;s neural scaling laws course (&lt;a href="https://sites.google.com/view/towards-agi-course">IFT6760A&lt;/a>) in winter 2023. You can view the slides we used &lt;a href="https://docs.google.com/presentation/d/11VtXg-sfLkjtIQWXEEEm875esK-8QrNFr09woMYzrV8/edit?usp=sharing">here&lt;/a>, and the recording &lt;a href="https://sites.google.com/view/towards-agi-course/schedule#h.lmlkbq72t3iz">here&lt;/a> (or my backup &lt;a href="https://dl.kylrth.com/videos/2023-02-02-ml-safety.mp4">here&lt;/a>).&lt;/em>&lt;/p></description></item><item><title>Being mortal: medicine and what matters in the end</title><link>https://kylrth.com/book/being-mortal/</link><pubDate>Wed, 01 Feb 2023 09:04:11 -0500</pubDate><guid>https://kylrth.com/book/being-mortal/</guid><description>&lt;p>Read this book before you turn 60. In fact, read it before you&amp;rsquo;re 40. Read it before your parents are 60. Read it if you have parents! Read it if you&amp;rsquo;re a person who is likely to die sometime in the future.&lt;/p>
&lt;p>This book gave me tools for the hard decisions that exist for people in the last decade(s) of their lives. Before reaching this stage of life ourselves, we don&amp;rsquo;t think about it much because we&amp;rsquo;ve tended to separate the aged from our communities. We&amp;rsquo;ve lost a cultural understanding of what a good death looks like. This book brings that knowledge back to a world with modern medicine.&lt;/p></description></item><item><title>The big three in economics: Adam Smith, Karl Marx, and John Maynard Keynes</title><link>https://kylrth.com/book/big-three-economics/</link><pubDate>Tue, 24 Jan 2023 13:28:01 -0500</pubDate><guid>https://kylrth.com/book/big-three-economics/</guid><description>&lt;p>This book was published in 2007, before the Great Recession. It definitely reads that way. Very capitalist, very Christian, very neo-liberal.&lt;/p>
&lt;p>I enjoyed learning more about Adam Smith. I feel like Skousen does a good job painting the importance of his ideas as an invention that drove the Industrial Revolution.&lt;/p>
&lt;p>According to Skousen, Marx is the devil incarnate, and his ideas are a dangerous disease infecting the minds of intellectuals and workers. Skousen takes a strange interest in painting Marx and Keynes as deranged, sexual deviants, etc. (&amp;ldquo;The Truth about Keynes&amp;rsquo; Homosexuality&amp;rdquo; is a section in this book.)&lt;/p></description></item><item><title>The drunkard's walk: how randomness rules our lives</title><link>https://kylrth.com/book/drunkards-walk/</link><pubDate>Tue, 24 Jan 2023 13:23:22 -0500</pubDate><guid>https://kylrth.com/book/drunkards-walk/</guid><description>&lt;p>This is a good intro for non-statisticians to avoid some common pitfalls and misconceptions that the public often has. It&amp;rsquo;ll help you understand what it means when something is statistically significant, why sometimes studies contradict each other, and how to avoid believing in patterns that don&amp;rsquo;t exist. You&amp;rsquo;ll understand the following biases and fallacies:&lt;/p>
&lt;ul>
&lt;li>confirmation bias&lt;/li>
&lt;li>gambler&amp;rsquo;s fallacy&lt;/li>
&lt;li>hot hand fallacy&lt;/li>
&lt;li>&amp;ldquo;hindsight is 20/20&amp;rdquo;: the illusion of hindsight for explaining historical performance/events&lt;/li>
&lt;/ul>
&lt;p>One mistake I want to point out:&lt;/p></description></item><item><title>facts I've observed about how my brain stores knowledge</title><link>https://kylrth.com/post/brain-knowledge/</link><pubDate>Thu, 19 Jan 2023 09:47:24 -0500</pubDate><guid>https://kylrth.com/post/brain-knowledge/</guid><description>&lt;p>I&amp;rsquo;m writing these things down over time because I&amp;rsquo;d like to figure out how to make artificial neural networks better at storing and retrieving knowledge, and the human brain is pretty good at that.&lt;/p>
&lt;ul>
&lt;li>When knowledge is new, it sometimes only comes back to me when I&amp;rsquo;m in the right context. For example, right now I can only think vaguely of how to adjust a chain derailleur on a bike, but I know that if I walked up to my bike and looked closely at the derailleur some more knowledge would appear. And we all have the familiar experience of being able to predict whether we&amp;rsquo;d recognize the name of a person or a song once it&amp;rsquo;s been said to us, but we can&amp;rsquo;t think of what it is until then.&lt;/li>
&lt;/ul></description></item><item><title>The birth order book: why you are the way you are</title><link>https://kylrth.com/book/birth-order/</link><pubDate>Thu, 01 Dec 2022 09:48:05 -0500</pubDate><guid>https://kylrth.com/book/birth-order/</guid><description>&lt;p>This book really gave me the chance to think about what being the firstborn did to my personality and my life outlook. I think it&amp;rsquo;s pretty clear I fit the mold of a perfectionist firstborn, thanks to having usually met the high expectations of my parents growing up. It&amp;rsquo;s been especially valuable to think about how that will affect my own parenting, especially toward my first child. I think one of the most useful things about the book is that it prepares a parent to try to mediate some of the negative consequences of birth order. Here are the main tasks for a parent for each category:&lt;/p></description></item><item><title>thoughts on Effective Altruism</title><link>https://kylrth.com/post/effective-altruism/</link><pubDate>Thu, 17 Nov 2022 15:53:44 -0500</pubDate><guid>https://kylrth.com/post/effective-altruism/</guid><description>&lt;p>Thanks to the crash of FTX and Scott Aaronson&amp;rsquo;s subsequent &lt;a href="https://scottaaronson.blog/?p=6797">post&lt;/a> about SBF, I read a &lt;a href="https://www.newyorker.com/magazine/2022/08/15/the-reluctant-prophet-of-effective-altruism">very interesting deep-dive&lt;/a> into Effective Altruism by &lt;em>the New Yorker&lt;/em>. I&amp;rsquo;m seeing a lot of important characters show up that I&amp;rsquo;ve seen before: Eliezer Yudkowsky, earn-to-donate, 80,000 hours, etc. It&amp;rsquo;s really fascinating to see this all coming together in one narrative so I can understand a little better the inspiration for these ideas and the way that the movement has interacted with the world up till now. Here are my notes and critiques of the ethical ideas presented in the article.&lt;/p></description></item><item><title>AlphaTensor: discovering faster matrix multiplication algorithms with reinforcement learning</title><link>https://kylrth.com/paper/alphatensor/</link><pubDate>Fri, 21 Oct 2022 14:18:00 -0400</pubDate><guid>https://kylrth.com/paper/alphatensor/</guid><description>&lt;p>&lt;em>This was a paper I presented about in Bang Liu&amp;rsquo;s research group meeting on 2022-10-21. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/1iei7rogURT0GFU_jPIktMat-Cusb0KJciW-WLRANfzc/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>Whisper: robust speech recognition via large-scale weak supervision</title><link>https://kylrth.com/paper/whisper/</link><pubDate>Fri, 30 Sep 2022 09:46:29 -0400</pubDate><guid>https://kylrth.com/paper/whisper/</guid><description>&lt;p>&lt;em>This was a paper I presented about in Bang Liu&amp;rsquo;s research group meeting on 2022-09-30. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/1bGjzq0f2KEh49F9eyaNYz_VNlEQVJW0HKR61d48Y9fs/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>Selective annotation makes language models better few-shot learners</title><link>https://kylrth.com/paper/selective-annotation/</link><pubDate>Tue, 13 Sep 2022 11:26:21 -0400</pubDate><guid>https://kylrth.com/paper/selective-annotation/</guid><description>&lt;p>Selective annotation chooses a pool of samples to annotate from a large set of unlabeled data. The main result of the paper is that when this is combined with item-specific prompt retrieval the performance drastically improves (&amp;gt;10% relative gain and lower performance variance). Interestingly, selective annotation does &lt;em>not&lt;/em> help for finetuning, or when the prompts are randomly selected. They call their selective annotation method &amp;ldquo;vote-&lt;code>\(k\)&lt;/code>&amp;rdquo;.&lt;/p>
&lt;h2 id="selective-annotation-method">selective annotation method &lt;a class="header-link" href="#selective-annotation-method">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>Vote-&lt;code>\(k\)&lt;/code> essentially creates a network of similar&lt;span class="sidenote-number">&lt;small class="sidenote">according to Sentence-BERT&lt;/small>&lt;/span>
unlabeled instances, and then selects from them with a network importance score that is discounted to promote diversity&lt;span class="sidenote-number">&lt;small class="sidenote">The discounting is performed by iteratively adding to the selection set, each time penalizing new nodes for being close to nodes that are already in the selection set.&lt;/small>&lt;/span>
.&lt;/p></description></item><item><title>Trivial or impossible—dichotomous data difficulty masks model differences (on ImageNet and beyond)</title><link>https://kylrth.com/paper/dichotomous-data-difficulty/</link><pubDate>Mon, 15 Aug 2022 14:21:56 -0400</pubDate><guid>https://kylrth.com/paper/dichotomous-data-difficulty/</guid><description>&lt;blockquote>
&lt;p>We observe that 48.2% [of] images [in ImageNet] are learned by all models regardless of their inductive bias; 14.3% [of] images are consistently misclassified by all models; only roughly a third (37.5%) of images are responsible for the differences between two models&amp;rsquo; decisions. We call this phenomenon dichotomous data difficulty (DDD).&lt;/p>&lt;/blockquote>
&lt;p>The authors varied hyperparameters, optimizers, architectures, supervision modes, and sampling methods, finding that models only varied in performance on about a third of the images in the dataset. And this isn&amp;rsquo;t specific to ImageNet; they found similar results for CIFAR-100 and a synthetic Gaussian dataset. They use this measure to divide the dataset into &amp;ldquo;trivials&amp;rdquo;, &amp;ldquo;impossibles&amp;rdquo;, and &amp;ldquo;in-betweens&amp;rdquo;.&lt;/p></description></item><item><title>Beyond neural scaling laws: beating power law scaling via data pruning</title><link>https://kylrth.com/paper/beyond_nsl/</link><pubDate>Thu, 11 Aug 2022 14:09:08 -0400</pubDate><guid>https://kylrth.com/paper/beyond_nsl/</guid><description>&lt;p>In this paper they show that we can achieve exponential performance scaling over dataset size, when the samples added are pruned to be only the best examples. This beats power law scaling in a big way. There is still no free lunch, in some sense, because in most cases it will become progressively harder to add new useful samples as the dataset gets bigger. But this is a big deal for computation, because it means that the number of samples in the dataset is not nearly as important as the coverage and quality that the dataset provides.&lt;span class="sidenote-number">&lt;small class="sidenote">This also means that scaling laws for &lt;em>compute&lt;/em> (usually expressed as a function of dataset and model size) are dataset-specific and not generalizable, because of how much sample quality affects data scaling.&lt;/small>&lt;/span>&lt;/p></description></item><item><title>LocoProp: enhancing backprop via local loss optimization</title><link>https://kylrth.com/paper/locoprop/</link><pubDate>Fri, 05 Aug 2022 09:52:06 -0400</pubDate><guid>https://kylrth.com/paper/locoprop/</guid><description>&lt;p>&lt;em>This was a paper I presented about in Bang Liu&amp;rsquo;s research group meeting on 2022-08-05. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/10f-vVG9yLaHHfZmGvKb2roaTkOVE6flbHv1yE5pkxX4/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>the effects of scale on worst-group performance</title><link>https://kylrth.com/post/worst-group-scale/</link><pubDate>Mon, 18 Jul 2022 15:31:12 -0400</pubDate><guid>https://kylrth.com/post/worst-group-scale/</guid><description>&lt;p>I think it&amp;rsquo;s valuable to be working in the open whenever possible, so I&amp;rsquo;m going to keep my research notes here. These notes will hopefully be full of good (and bad) ideas, so if someone borrows a good idea and publishes on it, that&amp;rsquo;s great!&lt;/p>
&lt;p>This post contains my research notes as I try to understand how model scaling affects worst-group performance. This started as a group project in the neural scaling laws course at Mila in winter 2022. We presented about an existing &lt;a href="https://kylrth.com/paper/effect-of-model-size-on-worst-group-generalization/">paper&lt;/a> and presented our preliminary results &lt;a href="https://sites.google.com/view/nsl-course/schedule#h.o7ntdr3dzoiv">in class&lt;/a>. The repository for this project is &lt;a href="https://github.com/kylrth/worst_group_scale/">here&lt;/a>.&lt;/p></description></item><item><title>Continual-T0: progressively instructing 50+ tasks to language models without forgetting</title><link>https://kylrth.com/paper/continual-t0/</link><pubDate>Thu, 02 Jun 2022 15:28:55 -0400</pubDate><guid>https://kylrth.com/paper/continual-t0/</guid><description>&lt;p>&lt;em>This was a paper I presented about in Bang Liu&amp;rsquo;s research group meeting on 2022-06-06. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/1-L5TnQvh-4WQHRSlIU-gcCyzudFxzZC0ur7vtLS28gs/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p>
&lt;p>Continual-T0 (CT0) extends &lt;a href="https://kylrth.com/paper/t0/">T0&lt;/a> by progressively training it on 8 unseen language generation tasks, while retaining a replay buffer of 1% of the original training data to preserve performance. The result is a model that maintains nearly all of its performance on previous tasks while learning the new tasks. In addition, CT0 maintains the original T0&amp;rsquo;s performance on unseen tasks (which is a big deal because those tasks could not appear in the replay buffer) and it extends the compositionality of T0 to even more unseen tasks.&lt;/p></description></item><item><title>learning French</title><link>https://kylrth.com/post/learning-french/</link><pubDate>Wed, 01 Jun 2022 12:19:08 -0400</pubDate><guid>https://kylrth.com/post/learning-french/</guid><description>&lt;p>Here I&amp;rsquo;m going to document my efforts to learn French. I speak Spanish pretty well, which together with English gives me a strong base for comprehension.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>2021-08-01&lt;/strong>: I started using Duolingo every day, and got into the XP challenges to the point where I was getting like hundreds of points most days. I&amp;rsquo;ve currently (2022-06-01) got 15821 XP and 103 lesson crowns in the French course, and most of that came from Fall 2021. That really gave me a good sense for basic grammar and function words.&lt;/p></description></item><item><title>Multitask prompted training enables zero-shot task generalization (T0)</title><link>https://kylrth.com/paper/t0/</link><pubDate>Fri, 27 May 2022 17:05:02 -0400</pubDate><guid>https://kylrth.com/paper/t0/</guid><description>&lt;p>T0 builds on T5 by fine-tuning on more natural prompts and testing the model&amp;rsquo;s generalization to held-out tasks.&lt;/p>
&lt;p>Compare the training format diagrams for T5 (top) and T0 (bottom):&lt;/p>
&lt;p>&lt;img src="t5.png" alt="Multitask prompted training enables zero-shot task generalization (T0) t5.png" class="img-zoomable">

&lt;br />&lt;br />&lt;br />
&lt;img src="prompt_format.png" alt="Multitask prompted training enables zero-shot task generalization (T0) prompt_format.png" class="img-zoomable">
&lt;/p>
&lt;p>Intuitively, the T0 prompts are more likely to be similar to implicit/explicit prompting that&amp;rsquo;s present in the pretraining data. The authors created several prompts for each dataset.&lt;/p></description></item><item><title>WordBurner beta</title><link>https://kylrth.com/post/wordburner/</link><pubDate>Mon, 18 Apr 2022 23:08:52 -0400</pubDate><guid>https://kylrth.com/post/wordburner/</guid><description>&lt;p>&lt;strong>Update 2022-04-27&lt;/strong>: The beta is over, but the apk is still installable with the instructions below and any feedback sent from inside the app will be received by me. I&amp;rsquo;m going to be working on this more over the summer, and eventually publishing it on the app store. :)&lt;/p>
&lt;p>Ever since learning Spanish, it has been a dream of mine to create a vocabulary study app that meets my needs. Duolingo won&amp;rsquo;t cover advanced vocabulary, Anki requires manually-generated decks, and other apps have expensive subscription plans.&lt;/p></description></item><item><title>PaLM</title><link>https://kylrth.com/paper/palm/</link><pubDate>Mon, 11 Apr 2022 12:17:25 -0400</pubDate><guid>https://kylrth.com/paper/palm/</guid><description>&lt;p>&lt;em>This was a paper I presented about in Bang Liu&amp;rsquo;s research group meeting on 2022-04-11. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/19TWSV9rACztA2Umw6VU2Bo61EwWycjyx-AoNlFdsk64/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>QA-GNN: reasoning with language models and knowledge graphs for question answering</title><link>https://kylrth.com/paper/qa-gnn/</link><pubDate>Tue, 05 Apr 2022 22:54:43 -0400</pubDate><guid>https://kylrth.com/paper/qa-gnn/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Bang Liu&amp;rsquo;s &lt;a href="https://www-labs.iro.umontreal.ca/~liubang/IFT%206289%20-%20Winter%202022.htm">IFT6289&lt;/a> course in winter 2022. The structure of the post follows the structure of the assignment: summarization followed by my own comments.&lt;/em>&lt;/p>
&lt;p>The authors create a novel system for combining an LM and a knowledge graph by performing reasoning over a joint graph produced by the LM and the KG, thus solving the problem of irrelevant entities appearing in the knowledge graph and unifying the representations across the LM and KG.&lt;/p></description></item><item><title>ethics drift within bubbles</title><link>https://kylrth.com/post/ethics-drift/</link><pubDate>Fri, 01 Apr 2022 08:35:54 -0400</pubDate><guid>https://kylrth.com/post/ethics-drift/</guid><description>&lt;p>Here are some snippets from a Lex Fridman &lt;a href="https://web.archive.org/web/20221222002108/https://lexfridman.com/john-abramson/">interview&lt;/a> with John Abramson, outspoken critic of Big Pharma.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Lex&lt;/strong>: Are people corrupt? Are people malevolent? Are people ignorant that work at the low level and at the high level, at Pfizer for example? How is this possible? I believe that most people are good, and I actually believe if you join Big Pharma your life trajectory often involves dreaming, wanting, and enjoying helping people. And then we look at the outcomes that you&amp;rsquo;re describing and that&amp;rsquo;s why the narrative takes hold that Pfizer CEO Albert Bourla is malevolent. The sense is that these companies are evil. So if the different parts are people that are good and they want to do good, how are we getting these outcomes?&lt;/p></description></item><item><title>notes about neuroscience</title><link>https://kylrth.com/post/neuroscience/</link><pubDate>Thu, 31 Mar 2022 10:02:44 -0400</pubDate><guid>https://kylrth.com/post/neuroscience/</guid><description>&lt;p>How much of brain structure is coded for in the genome? For example, the hippocampus is generally thought to be responsible for consolidating long-term memories. Is the specialization of this region an epigenetic phenomenon due to optimization in the environment, or is it coded more directly? Will we eventually see these structures emerge in artificial networks with sufficient scale and good optimization, or will we need to code it more directly?&lt;/p></description></item><item><title>Experienced well-being rises with income, even above $75,000 per year</title><link>https://kylrth.com/paper/experienced-well-being/</link><pubDate>Wed, 30 Mar 2022 13:34:53 -0400</pubDate><guid>https://kylrth.com/paper/experienced-well-being/</guid><description>&lt;p>&lt;strong>Turns out that money does buy happiness.&lt;/strong> You may have heard that people&amp;rsquo;s average happiness stops improving once you make more than $75,000/year? Researchers did a better survey with more data and found that that was not the case.&lt;/p>
&lt;p>The researchers cited 5 methodological improvements over the old research that suggested that it didn&amp;rsquo;t matter after $75,000:&lt;/p>
&lt;ol>
&lt;li>They measured people&amp;rsquo;s happiness in real time, instead of having people try to remember past happiness levels.&lt;/li>
&lt;li>They measured on a continuous scale instead of discrete. (The survey had a slider between &amp;ldquo;very bad&amp;rdquo; and &amp;ldquo;very good&amp;rdquo;, instead of discrete options to choose from.)&lt;/li>
&lt;li>They measured on &amp;ldquo;dozens of separate occasions per person&amp;rdquo; using an app, instead of just a single questionnaire.&lt;/li>
&lt;li>They used a comparable scale for experienced well-being (people’s evaluations of their lives) and evaluative well-being (how people feel during the day-to-day moments of their lives).&lt;/li>
&lt;li>They included a large number of high-earning participants and measured higher incomes in more granular increments.&lt;/li>
&lt;/ol>
&lt;p>The data were collected from 33,391 employed adults (ages 18-65) in the United States. There were a total of 1,725,994 reports collected, so that&amp;rsquo;s ~52 reports per person.&lt;/p></description></item><item><title>Neural message passing for quantum chemistry</title><link>https://kylrth.com/paper/neural-message-passing/</link><pubDate>Fri, 25 Mar 2022 14:46:11 -0400</pubDate><guid>https://kylrth.com/paper/neural-message-passing/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Bang Liu&amp;rsquo;s &lt;a href="https://www-labs.iro.umontreal.ca/~liubang/IFT%206289%20-%20Winter%202022.htm">IFT6289&lt;/a> course in winter 2022. The structure of the post follows the structure of the assignment: summarization followed by my own comments.&lt;/em>&lt;/p>
&lt;p>To summarize, the authors create a unifying framework for describing message-passing neural networks, which they apply to the problem of predicting the structural properties of chemical compounds in the QM9 dataset.&lt;/p>
&lt;h2 id="paper-summarization">paper summarization &lt;a class="header-link" href="#paper-summarization">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>The authors first demonstrate that many of the recent works applying neural nets to this problem can fit into a message-passing neural network (MPNN) framework. Under the MPNN framework, at each time step &lt;code>\(t\)&lt;/code> a message is computed for each vertex by summing the output of a learned function &lt;code>\(M_t\)&lt;/code> over the vertex and all edges and vertices connected to it. Then the next state for each vertex is a learned function &lt;code>\(U_t\)&lt;/code> of the previous state and the message. Finally, the &amp;ldquo;readout&amp;rdquo; function &lt;code>\(R\)&lt;/code> is applied to all the vertices to compute the result.&lt;/p></description></item><item><title>keep your tasks in the heap</title><link>https://kylrth.com/post/tasks-stack-heap/</link><pubDate>Tue, 22 Mar 2022 11:23:20 -0400</pubDate><guid>https://kylrth.com/post/tasks-stack-heap/</guid><description>&lt;p>Often when someone (usually a professor) is sharing their screen I see that their browser has so many tabs open that the descriptions are lost:&lt;/p>
&lt;img src="tabs.png" alt="keep your tasks in the heap tabs.png" class="img-zoomable">

&lt;p>That was my best impersonation as a Firefox user. Chrome will let you go a lot further (like ~113 tabs) before starting to provide a dropdown to show you the list of open tabs:&lt;/p>
&lt;img src="chrome_tabs.png" alt="keep your tasks in the heap chrome_tabs.png" class="img-zoomable">

&lt;p>Besides the obvious fact that this makes it hard to find a tab you&amp;rsquo;re looking for, you also waste computer memory and add to your cognitive load while you&amp;rsquo;re working.&lt;/p></description></item><item><title>The effect of model size on worst-group generalization</title><link>https://kylrth.com/paper/effect-of-model-size-on-worst-group-generalization/</link><pubDate>Thu, 17 Mar 2022 14:34:33 -0400</pubDate><guid>https://kylrth.com/paper/effect-of-model-size-on-worst-group-generalization/</guid><description>&lt;p>&lt;em>This was a paper we presented about in Irina Rish&amp;rsquo;s neural scaling laws course (&lt;a href="https://sites.google.com/view/nsl-course">IFT6167&lt;/a>) in winter 2022. You can view the slides we used &lt;a href="https://docs.google.com/presentation/d/1Fxs60aXvANsBj_k-m1h_KtjDfUrqPKY8f1UIh3j4XY0/edit?usp=sharing">here&lt;/a>, and the recording &lt;a href="https://sites.google.com/view/nsl-course/schedule#h.h6n8wkndidtb">here&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>quotes from a Lex Fridman interview with Philip Goff</title><link>https://kylrth.com/post/philip-goff/</link><pubDate>Wed, 09 Mar 2022 09:39:16 -0500</pubDate><guid>https://kylrth.com/post/philip-goff/</guid><description>&lt;p>Here are some snippets from a Lex Fridman &lt;a href="https://web.archive.org/web/20221222002116/https://lexfridman.com/philip-goff/">interview&lt;/a> with Philip Goff, a panpsychist.&lt;/p>
&lt;blockquote>
&lt;p>The Enlightenment ideal is to follow the evidence and the arguments where they lead, but it&amp;rsquo;s very hard for human beings to do that. I think we get stuck in some conception of how we think science ought to look. People talk about religion as a crutch, but I think a certain kind of scientism, a certain conception of how science is supposed to be, gets into people&amp;rsquo;s identity and their sense of themselves and their security.&lt;/p></description></item><item><title>hosting a Tor onion service</title><link>https://kylrth.com/post/tor-onion-service/</link><pubDate>Thu, 24 Feb 2022 18:28:49 -0500</pubDate><guid>https://kylrth.com/post/tor-onion-service/</guid><description>&lt;p>&lt;a href="https://community.torproject.org/onion-services/overview/">Tor onions&lt;/a> are a way to host secure services that protect the anonymity of you &lt;em>and&lt;/em> your clients. It also removes load from Tor exit nodes. If you open this page in the Tor browser it will redirect you to the following address:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-txt" data-lang="txt">&lt;span style="display:flex;">&lt;span>http://kylrthjj7mpvktolz7u6fnudt3hpdvjw4hzquanjpepgsf5vcq5divad.onion/post/tor-onion-service/

&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>which can only be opened from inside the Tor network.&lt;/p>
&lt;h2 id="getting-started">getting started &lt;a class="header-link" href="#getting-started">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>To host an onion service, we&amp;rsquo;ll have a Docker container running Tor that decodes requests and forwards them to another container hosting the service. I&amp;rsquo;ve got a GitHub repo that will help you build the Tor image and shows you an example &lt;code>docker-compose.yml&lt;/code>:&lt;/p></description></item><item><title>Scaling laws for the few-shot adaptation of pre-trained image classifiers</title><link>https://kylrth.com/paper/scaling-laws-few-shot-image-classifiers/</link><pubDate>Tue, 22 Feb 2022 13:19:12 -0500</pubDate><guid>https://kylrth.com/paper/scaling-laws-few-shot-image-classifiers/</guid><description>&lt;p>The unsurprising result here is that few-shot performance scales predictably with pre-training dataset size under traditional fine-tuning, matching network, and prototypical network approaches.&lt;/p>
&lt;p>The interesting result is that the exponents of these three approaches were substantially different (see Table 1 in the paper), which says to me that the few-shot inference approach matters a lot.&lt;/p>
&lt;p>The surprising result was that while more training on the &amp;ldquo;non-natural&amp;rdquo; &lt;a href="https://github.com/brendenlake/omniglot">Omniglot&lt;/a> dataset did not improve few-shot accuracy on other datasets, training on &amp;ldquo;natural&amp;rdquo; datasets &lt;em>did&lt;/em> improve accuracy on few-shot Omniglot.&lt;/p></description></item><item><title>Learning explanations that are hard to vary</title><link>https://kylrth.com/paper/learning-explanations-hard-to-vary/</link><pubDate>Tue, 22 Feb 2022 12:29:17 -0500</pubDate><guid>https://kylrth.com/paper/learning-explanations-hard-to-vary/</guid><description>&lt;p>The big idea here is to use the geometric mean instead of the arithmetic mean across samples in the batch when computing the gradient for SGD. This overcomes the situation where averaging produces optima that are not actually optimal for any individual samples, as demonstrated in their toy example below:&lt;/p>
&lt;img src="example.png" alt="Learning explanations that are hard to vary example.png" class="img-zoomable">

&lt;p>In practice, the method the authors test is not exactly the geometric mean for numerical and performance reasons, but effectively accomplishes the same thing by avoiding optima that are &amp;ldquo;inconsistent&amp;rdquo; (meaning that gradients from relatively few samples actually point in that direction).&lt;/p></description></item><item><title>In search of robust measures of generalization</title><link>https://kylrth.com/paper/robust-measures-of-generalization/</link><pubDate>Mon, 21 Feb 2022 15:33:22 -0500</pubDate><guid>https://kylrth.com/paper/robust-measures-of-generalization/</guid><description>&lt;p>These authors define &lt;em>robust error&lt;/em> as the least upper bound on the expected loss over a family of environmental settings (including dataset, model architecture, learning algorithm, etc.):&lt;/p>
&lt;p>&lt;code>\[\sup_{e\in\mathcal F}\mathbb E_{\omega\in P^e}\left[\ell(\phi,\omega)\right]\]&lt;/code>&lt;/p>
&lt;p>The fact that this is an &lt;strong>upper bound&lt;/strong> and not an average is very important and is what makes this work unique from previous work in this direction. Indeed, what we should be concerned about is not how poorly a model performs on the &lt;em>average&lt;/em> sample but on the &lt;em>worst-case&lt;/em> sample.&lt;/p></description></item><item><title>It's not just size that matters: small language models are also few-shot learners</title><link>https://kylrth.com/paper/not-just-size-that-matters/</link><pubDate>Fri, 18 Feb 2022 13:13:54 -0500</pubDate><guid>https://kylrth.com/paper/not-just-size-that-matters/</guid><description>&lt;p>&lt;em>We presented this paper as a mini-lecture in Bang Liu&amp;rsquo;s &lt;a href="https://www-labs.iro.umontreal.ca/~liubang/IFT%206289%20-%20Winter%202022.htm">IFT6289&lt;/a> course in winter 2022. You can view the slides we used &lt;a href="https://docs.google.com/presentation/d/1XPRSLC24AQK0MeZY5zww4gZ0t0B_gqDl8eztKZpt_v8/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>Educated</title><link>https://kylrth.com/book/educated/</link><pubDate>Thu, 17 Feb 2022 09:51:11 -0500</pubDate><guid>https://kylrth.com/book/educated/</guid><description>&lt;h2 id="thinking-for-yourself">thinking for yourself &lt;a class="header-link" href="#thinking-for-yourself">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>I recognized myself a little in this book, not in the events, severity, or locations but in the path to being &amp;ldquo;educated&amp;rdquo; in the sense that Westover intends. I&amp;rsquo;ll try to convey what that sense is with some quotes from the book.&lt;/p>
&lt;p>The first moment is after she takes a class on American history at BYU. She returns home and gets her face dirty while working, and her brother calls her a N&amp;mdash;r, a joke he had made many times before.&lt;/p></description></item><item><title>Scaling laws for transfer</title><link>https://kylrth.com/paper/scaling-laws-for-transfer/</link><pubDate>Wed, 16 Feb 2022 14:12:26 -0500</pubDate><guid>https://kylrth.com/paper/scaling-laws-for-transfer/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Irina Rish&amp;rsquo;s neural scaling laws course (&lt;a href="https://sites.google.com/view/nsl-course">IFT6167&lt;/a>) in winter 2022. The post contains no summarization, only questions and thoughts.&lt;/em>&lt;/p>
&lt;p>Sometimes these scaling laws can feel like pseudoscience because they&amp;rsquo;re a post hoc attempt to place a trend line on data. How can we be confident that the trends we observe actually reflect the scaling laws that we&amp;rsquo;re after? In the limitations section they mention that they didn&amp;rsquo;t tune hyperparameters for fine-tuning or for the code data distribution. How can we know that a confounding hyperparameter is not responsible for the trend we see? I wonder if we aren&amp;rsquo;t really being statistically rigorous until we can predict generalization error on an unseen &lt;em>training setup&lt;/em>, rather than just an unseen model size/dataset size.&lt;/p></description></item><item><title>Deep learning scaling is predictable, empirically</title><link>https://kylrth.com/paper/scaling-predictable-empirically/</link><pubDate>Mon, 14 Feb 2022 10:38:11 -0500</pubDate><guid>https://kylrth.com/paper/scaling-predictable-empirically/</guid><description>&lt;p>&lt;em>This was a paper we presented about in Irina Rish&amp;rsquo;s neural scaling laws course (&lt;a href="https://sites.google.com/view/nsl-course">IFT6167&lt;/a>) in winter 2022. You can view the slides we used &lt;a href="https://docs.google.com/presentation/d/1e0SXonZiW6o8VyqXTnjyYlMs97YCcntBznaoiBwlWFE/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p>
&lt;p>It&amp;rsquo;s important to note that in the results for NMT (Figure 1) we would expect the lines in the graph on the left to curve as the capacity of the individual models is exhausted. That&amp;rsquo;s why the authors fit the curves with an extra constant added. Meanwhile, the results in the graph on the right are curved because as the data size grows, the optimal model size also grows and it becomes increasingly difficult to find the right hyperparameters to train the model down to the optimal generalization error. (See the last paragraph in Section 4.1.)&lt;/p></description></item><item><title>Masked autoencoders are scalable vision learners</title><link>https://kylrth.com/paper/masked-autoencoders-are-scalable-vision-learners/</link><pubDate>Fri, 11 Feb 2022 14:18:30 -0500</pubDate><guid>https://kylrth.com/paper/masked-autoencoders-are-scalable-vision-learners/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Irina Rish&amp;rsquo;s neural scaling laws course (&lt;a href="https://sites.google.com/view/nsl-course">IFT6167&lt;/a>) in winter 2022. The post contains no summarization, only questions and thoughts.&lt;/em>&lt;/p>
&lt;p>In this paper they mention that the mask vector is learned, and it sounds like the positional embeddings are also learned. I remember in &lt;a href="https://kylrth.com/paper/attention-all-you-need/">&lt;em>Attention is all you need&lt;/em>&lt;/a> they found that cosine positional embeddings worked better than learned ones, especially for sequences of longer length. But now it seems like most papers are doing learned embeddings. If anyone knows why, send me an email.&lt;/p></description></item><item><title>Data scaling laws in NMT: the effect of noise and architecture</title><link>https://kylrth.com/paper/data-scaling-laws-nmt/</link><pubDate>Wed, 09 Feb 2022 20:47:59 -0500</pubDate><guid>https://kylrth.com/paper/data-scaling-laws-nmt/</guid><description>&lt;p>This paper is all about trying a bunch of different changes to the training setup to see what affects the power law exponent &lt;strong>over dataset size&lt;/strong>. Here are some of the answers:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>encoder-decoder size asymmetry&lt;/strong>: exponent not affected, but effective model capacity affected&lt;/li>
&lt;li>&lt;strong>architecture (LSTM vs. Transformer)&lt;/strong>: exponent not affected, but effective model capacity affected&lt;/li>
&lt;li>&lt;strong>dataset quality (filtered vs. not)&lt;/strong>: exponent and effective model capacity not effected, losses on smaller datasets affected&lt;/li>
&lt;li>&lt;strong>dataset source (ParaCrawl vs. in-house dataset)&lt;/strong>: exponent not affected&lt;/li>
&lt;li>&lt;strong>adding independent noise&lt;/strong>: exponent not affected, but effective model capacity affected&lt;/li>
&lt;/ul>
&lt;p>Here are some other things to test that I thought of while I read this:&lt;/p></description></item><item><title>Parallel training of deep networks with local updates</title><link>https://kylrth.com/paper/parallel-training-with-local-updates/</link><pubDate>Wed, 09 Feb 2022 10:50:21 -0500</pubDate><guid>https://kylrth.com/paper/parallel-training-with-local-updates/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Irina Rish&amp;rsquo;s neural scaling laws course (&lt;a href="https://sites.google.com/view/nsl-course">IFT6167&lt;/a>) in winter 2022. The post contains no summarization, only questions and thoughts.&lt;/em>&lt;/p>
&lt;p>Once I learned how the loss functions worked for each chunk, my first question was whether the earlier chunks were going to be able to learn the low-level features that later chunks would need. Figure 7 seems to show that they do, although their quality apparently decreases with increasingly local updates.&lt;/p></description></item><item><title>A sensitivity analysis of (and practitioners’ guide to) convolutional neural networks for sentence classification</title><link>https://kylrth.com/paper/cnn-sentence/</link><pubDate>Wed, 02 Feb 2022 15:35:00 -0500</pubDate><guid>https://kylrth.com/paper/cnn-sentence/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Bang Liu&amp;rsquo;s &lt;a href="https://www-labs.iro.umontreal.ca/~liubang/IFT%206289%20-%20Winter%202022.htm">IFT6289&lt;/a> course in winter 2022. The structure of the post follows the structure of the assignment: summarization followed by my own comments.&lt;/em>&lt;/p>
&lt;h2 id="paper-summarization">paper summarization &lt;a class="header-link" href="#paper-summarization">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>Word embeddings have gotten so good that state-of-the-art sentence classification can often be achieved with just a one-layer convolutional network on top of those embeddings. This paper dials in on the specifics of training that convolutional layer for this downstream sentence classification task.&lt;/p></description></item><item><title>Learning transferable visual models from natural language supervision (CLIP)</title><link>https://kylrth.com/paper/clip/</link><pubDate>Wed, 02 Feb 2022 12:35:03 -0500</pubDate><guid>https://kylrth.com/paper/clip/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Irina Rish&amp;rsquo;s neural scaling laws course (&lt;a href="https://sites.google.com/view/nsl-course">IFT6167&lt;/a>) in winter 2022. The post contains no summarization, only questions and thoughts.&lt;/em>&lt;/p>
&lt;p>This concept of wide vs. narrow supervision (rather than binary &amp;ldquo;supervised&amp;rdquo; and &amp;ldquo;unsupervised&amp;rdquo;) is an interesting and flexible way to think about the way these training schemes leverage data.&lt;/p>
&lt;p>The zero-shot CLIP matches the performance of 4-shot CLIP, which is a surprising result. What do the authors mean when they make this guess about zero-shot&amp;rsquo;s advantage:&lt;/p></description></item><item><title>Distributed representations of words and phrases and their compositionality</title><link>https://kylrth.com/paper/distributed-representations/</link><pubDate>Tue, 01 Feb 2022 16:09:19 -0500</pubDate><guid>https://kylrth.com/paper/distributed-representations/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Bang Liu&amp;rsquo;s &lt;a href="https://www-labs.iro.umontreal.ca/~liubang/IFT%206289%20-%20Winter%202022.htm">IFT6289&lt;/a> course in winter 2022. The structure of the post follows the structure of the assignment: summarization followed by my own comments.&lt;/em>&lt;/p>
&lt;h2 id="paper-summarization">paper summarization &lt;a class="header-link" href="#paper-summarization">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>This paper describes multiple improvements that are made to the original &lt;a href="https://arxiv.org/abs/1301.3781">Skip-gram&lt;/a> model:&lt;/p>
&lt;ol>
&lt;li>Decreasing the rate of exposure to common words improves the training speed and increases the model&amp;rsquo;s accuracy on infrequent words.&lt;/li>
&lt;li>A new training target they call &amp;ldquo;negative sampling&amp;rdquo; improves the training speed and the model&amp;rsquo;s accuracy on frequent words.&lt;/li>
&lt;li>Allowing the model to use phrase vectors improves the expressivity of the model.&lt;/li>
&lt;/ol>
&lt;h3 id="negative-sampling">negative sampling &lt;a class="header-link" href="#negative-sampling">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h3>
&lt;p>The original Skip-gram model computed probabilities using a hierarchical softmax, which allowed the model to compute only &lt;code>\(O(\log_2(|V|))\)&lt;/code> probabilities when estimating the probability of a particular word, rather than &lt;code>\(O(|V|)\)&lt;/code>. Negative sampling, on the other hand, deals directly with the generated vector representations. The negative sampling loss function basically tries to maximize cosine similarity between the input representation of the input word with the output representation of the neighboring word, while decreasing cosine similarity between the input word and a few random vectors. They find that the required number of negative examples decreases as the dataset size increases.&lt;/p></description></item><item><title>meaning-making in the post-modern world</title><link>https://kylrth.com/post/meaning-making/</link><pubDate>Tue, 01 Feb 2022 14:35:37 -0500</pubDate><guid>https://kylrth.com/post/meaning-making/</guid><description>&lt;p>Here are some snippets from a Lex Fridman &lt;a href="https://web.archive.org/web/20221227111818/https://lexfridman.com/peter-wang/">interview&lt;/a> with Peter Wang, co-founder and CEO of Anaconda:&lt;/p>
&lt;blockquote>
&lt;p>For a lot of human history, there wasn&amp;rsquo;t so much a meaning crisis as just a food and not getting eaten by bears crisis. Once you get to a point where you can make food there was a not getting killed by other humans crisis. Sitting around wondering what it&amp;rsquo;s all about is a relatively recent luxury.&lt;/p></description></item><item><title>Deep learning</title><link>https://kylrth.com/paper/deep-learning/</link><pubDate>Thu, 20 Jan 2022 15:11:00 -0500</pubDate><guid>https://kylrth.com/paper/deep-learning/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Bang Liu&amp;rsquo;s &lt;a href="https://www-labs.iro.umontreal.ca/~liubang/IFT%206289%20-%20Winter%202022.htm">IFT6289&lt;/a> course in winter 2022. The structure of the post follows the structure of the assignment: summarization followed by my own comments.&lt;/em>&lt;/p>
&lt;h2 id="paper-summarization">paper summarization &lt;a class="header-link" href="#paper-summarization">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>The authors use the example of distinguishing between a Samoyed and a white wolf to talk about the importance of learning to rely on very small variations while ignoring others. While shallow classifiers must rely on human-crafted features which are expensive to build and always imperfect, deep classifiers are expected to learn their own features by applying a &amp;ldquo;general-purpose learning procedure&amp;rdquo; to learn the features and the classification layer from the data simultaneously.&lt;/p></description></item><item><title>Cratylus</title><link>https://kylrth.com/book/cratylus/</link><pubDate>Sun, 16 Jan 2022 15:11:14 -0500</pubDate><guid>https://kylrth.com/book/cratylus/</guid><description>&lt;p>In this dialog Hermogenes comes to Socrates to discuss Cratylus&amp;rsquo; view of the nature of names, whether they are true to the objects they represent or are just conventional. Hermogenes believes that names are purely conventional, while Cratylus believes the opposite. Socrates falls somewhere in the middle:&lt;/p>
&lt;blockquote>
&lt;p>I quite agree with you that words should as far as possible resemble things; but I fear that this dragging in of resemblance, as Hermogenes says, is a shabby thing, which has to be supplemented by the mechanical aid of convention with a view to correctness; for I believe that if we could always, or almost always, use likenesses, which are perfectly appropriate, this would be the most perfect state of language; as the opposite is the most imperfect.&lt;/p></description></item><item><title>Crito</title><link>https://kylrth.com/book/crito/</link><pubDate>Mon, 27 Dec 2021 14:05:02 -0700</pubDate><guid>https://kylrth.com/book/crito/</guid><description>&lt;p>In this dialogue Crito comes to Socrates who is in prison waiting to be executed by the state. Crito has come to convince Socrates to come and escape with him. Crito&amp;rsquo;s escape plan will not cause great inconvenience for any of Socrates&amp;rsquo; friends, and he would be able to live well in Thessaly. Socrates ends up convincing Crito that it would be wrong for him to escape.&lt;/p>
&lt;h2 id="the-opinion-of-the-many">&amp;ldquo;the opinion of the many&amp;rdquo; &lt;a class="header-link" href="#the-opinion-of-the-many">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;blockquote>
&lt;p>CRITO: But you see, Socrates, that the opinion of the many must be regarded, for what is now happening shows that they can do the greatest evil to anyone who has lost their good opinion.&lt;/p></description></item><item><title>Apology of Socrates</title><link>https://kylrth.com/book/apology/</link><pubDate>Sun, 26 Dec 2021 13:40:45 -0700</pubDate><guid>https://kylrth.com/book/apology/</guid><description>&lt;p>I&amp;rsquo;m starting a course of foundational texts in philosophy with a friend of mine, and this is the first one we&amp;rsquo;ve read. Socrates is often considered a founder of Western philosophy, and it was easy for me to see in the text some common philosophical themes I&amp;rsquo;ve been exposed to growing up in the West.&lt;/p>
&lt;h2 id="the-fear-of-death-is-irrational">the fear of death is irrational &lt;a class="header-link" href="#the-fear-of-death-is-irrational">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>Socrates argues that the fear of death is irrational from two perspectives: one, that what happens after death cannot be bad; and two, that a righteous person needs to be more concerned with whether he is doing right or wrong than whether death occurs or not. You can see the first one here:&lt;/p></description></item><item><title>avatarify</title><link>https://kylrth.com/post/avatarify/</link><pubDate>Wed, 24 Nov 2021 11:58:34 -0500</pubDate><guid>https://kylrth.com/post/avatarify/</guid><description>&lt;p>&lt;a href="https://github.com/alievk/avatarify-python">Avatarify&lt;/a> is a cool project that lets you create a relatively realistic avatar that you can use during video meetings. It works by creating a fake video input device and passing your video input through a neural network in PyTorch. My laptop doesn&amp;rsquo;t have a GPU, so I used the server/client setup.&lt;/p>
&lt;h2 id="setting-up-the-server">setting up the server &lt;a class="header-link" href="#setting-up-the-server">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>Be sure you&amp;rsquo;ve installed the Nvidia Docker runtime so that the Docker container can use the GPU. You can see how I did that &lt;a href="https://kylrth.com/post/jupyter-lab/">here&lt;/a>. Run the following on the server:&lt;/p></description></item><item><title>hosting my own web services</title><link>https://kylrth.com/post/self-hosting/</link><pubDate>Tue, 23 Nov 2021 07:57:00 -0500</pubDate><guid>https://kylrth.com/post/self-hosting/</guid><description>&lt;img src="spaceship.jpg" alt="hosting my own web services spaceship.jpg" class="img-zoomable">

&lt;p>I host several services on an Alienware gaming computer I keep at my apartment. (We call it the spaceship.) I originally got the computer so I could have a computer with a GPU for machine learning projects, but I&amp;rsquo;ve since started using this computer to host a bunch of different services. Here I&amp;rsquo;ve documented how I set up the server.&lt;/p>
&lt;h2 id="operating-system">operating system &lt;a class="header-link" href="#operating-system">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>To keep things simple I use Ubuntu 20.04 LTS. Here&amp;rsquo;s a speedy way I&amp;rsquo;ve found to write installer ISOs to USB drives for installation:&lt;/p></description></item><item><title>moving to Québec</title><link>https://kylrth.com/post/qu%C3%A9bec/</link><pubDate>Fri, 29 Oct 2021 11:55:00 -0400</pubDate><guid>https://kylrth.com/post/qu%C3%A9bec/</guid><description>&lt;p>We just moved our family from Utah, USA, to Montréal, Québec, Canada. I entered Canada on August 18, 2021 &lt;a href="https://www.google.com/maps/dir/Salt+Lake+City,+UT/montreal/@42.7334015,-101.7162703,5z/data=!3m1!4b1!4m13!4m12!1m5!1m1!1s0x87523d9488d131ed:0x5b53b7a0484d31ca!2m2!1d-111.8910474!2d40.7607793!1m5!1m1!1s0x4cc91a541c64b70d:0x654e3138211fefef!2m2!1d-73.567256!2d45.5016889">by car&lt;/a>, and my wife and daughter entered a few days later by air. The process actually began on April 27 when I got my acceptance letter to the Université de Montréal as a master&amp;rsquo;s student in the &lt;a href="https://diro.umontreal.ca/accueil/">Département d&amp;rsquo;informatique et de recherche opérationelle&lt;/a>. After a few days of scrambling to find out if I would be able to study there without knowing French (turns out you can as a grad student at DIRO!), I started the process of applying to enter Canada and live in Québec as a student.&lt;/p></description></item><item><title>Jupyter Lab Hub in Docker with Nvidia GPU support</title><link>https://kylrth.com/post/jupyter-lab/</link><pubDate>Tue, 19 Oct 2021 20:28:25 -0400</pubDate><guid>https://kylrth.com/post/jupyter-lab/</guid><description>&lt;p>This is how I set up my headless home server with a Jupyter Lab Docker container with an Nvidia GPU runtime. Login is handled by a GitHub OAuth application.&lt;/p>
&lt;h2 id="nvidia-drivers-and-the-container-runtime">Nvidia drivers and the container runtime &lt;a class="header-link" href="#nvidia-drivers-and-the-container-runtime">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>First, check &lt;a href="https://docs.nvidia.com/cuda/archive/11.4.2/cuda-toolkit-release-notes/index.html">here&lt;/a> (replacing the CUDA version in the URL with your own) to see which Nvidia drivers you need for the CUDA toolkit version you want. I&amp;rsquo;m using CUDA 11.4.2, which means I need at least driver version 470.&lt;span class="sidenote-number">&lt;small class="sidenote">You can use &lt;code>sudo apt purge nvidia-*&lt;/code> to cleanly remove older drivers (or broken installs) before installing the desired version.&lt;/small>&lt;/span>&lt;/p></description></item><item><title>Minecraft in Docker</title><link>https://kylrth.com/post/minecraft/</link><pubDate>Mon, 02 Aug 2021 13:25:28 -0600</pubDate><guid>https://kylrth.com/post/minecraft/</guid><description>&lt;p>This guide shows how to host multiple Minecraft servers on a single machine with docker-compose.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>mkdir minecraft_server
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cd minecraft_server
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mkdir data/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>wget /post/minecraft/docker-compose.yml
 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -O docker-compose.yml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This docker-compose setup uses itzg&amp;rsquo;s Docker image, which you see further documentation for &lt;a href="https://github.com/itzg/docker-minecraft-server">here&lt;/a>.&lt;/p>
&lt;p>If you&amp;rsquo;re moving from a vanilla Minecraft world, do the following to get the different world directories in the right position:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>cp -r &lt;span style="color:#e6db74">${&lt;/span>OLD&lt;span style="color:#e6db74">}&lt;/span>/world data/server/world
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mkdir data/server/world_&lt;span style="color:#f92672">{&lt;/span>nether,the_end&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mv data/server/world/DIM-1 data/server/world_nether/DIM-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mv data/server/world/DIM1 data/server/world_the_end/DIM1
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Here&amp;rsquo;s the map from vanilla Minecraft directories to Spigot directories (which is what itzg&amp;rsquo;s container uses):&lt;/p></description></item><item><title>Matrix setup with Synapse, Postgres, Maubot, and matrix-registration</title><link>https://kylrth.com/post/matrix-setup/</link><pubDate>Mon, 02 Aug 2021 10:30:00 -0600</pubDate><guid>https://kylrth.com/post/matrix-setup/</guid><description>&lt;p>This is how I set up my own &lt;a href="https://matrix.org">Matrix&lt;/a> server with Docker.&lt;span class="sidenote-number">&lt;small class="sidenote">These instructions were originally for ARM, back when I ran this server on a Raspberry Pi. Unfortunately, the Matrix community stopped releasing ARM images, so the latest version that will work on ARM without QEMU is v1.26.0, which is very old now. These instructions have been updated to use &lt;code>amd64&lt;/code> images, but I&amp;rsquo;ll preserve the references to ARM images as comments. If you&amp;rsquo;re going to work from a Pi, be sure to switch it to run in 64-bit mode for optimal performance: &lt;code>echo 'arm_64bit=1' | sudo tee -a /boot/config.txt &amp;amp;&amp;amp; sudo systemctl reboot&lt;/code>.&lt;/small>&lt;/span>
There is an &lt;a href="https://github.com/spantaleev/matrix-docker-ansible-deploy">Ansible playbook&lt;/a> that&amp;rsquo;s quite popular, but I host a lot of other services with Docker on the same server and I wanted to continue managing all of them together, just with &lt;code>docker-compose&lt;/code>.&lt;/p></description></item><item><title>I really just want to edit and compile my LaTeX files in VS Code</title><link>https://kylrth.com/post/latex-vscode/</link><pubDate>Mon, 12 Jul 2021 21:43:47 -0600</pubDate><guid>https://kylrth.com/post/latex-vscode/</guid><description>&lt;p>LaTeX has a ton of different flavors, releases, and installations: &lt;a href="https://en.wikipedia.org/wiki/MacTeX">MacTeX&lt;/a>, &lt;a href="https://en.wikipedia.org/wiki/MiKTeX">MiKTeX&lt;/a>, &lt;a href="https://en.wikipedia.org/wiki/TeXworks">TeXworks&lt;/a>, &lt;a href="https://en.wikipedia.org/wiki/XeTeX">XeTeX&lt;/a>, &lt;a href="https://en.wikipedia.org/wiki/PdfTeX">pdfTeX&lt;/a>, &lt;a href="https://en.wikipedia.org/wiki/LuaTeX">LuaTeX&lt;/a>&amp;hellip; If you&amp;rsquo;re using Linux and just want to edit LaTeX files in Visual Studio Code and have them automatically rendered as PDFs, follow these instructions:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>On Arch-based distros, install the packages listed &lt;a href="https://wiki.archlinux.org/title/TeX_Live">here&lt;/a>. On Debian-based systems, &lt;code>sudo apt install texlive&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Install some Perl dependencies:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sudo cpan Log::Log4perl Log::LogDispatch Log::Dispatch::File YAML::Tiny File::HomeDir
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>If you want to use FontAwesome on Arch-based systems, install the &lt;code>oft-font-awesome&lt;/code> package and then do the following (&lt;a href="https://wiki.archlinux.org/title/TeX_Live#Making_fonts_available_to_Fontconfig">source&lt;/a>):&lt;/p></description></item><item><title>The last speakers: the quest to save the world's most endangered languages</title><link>https://kylrth.com/book/last-speakers/</link><pubDate>Mon, 31 May 2021 08:29:00 -0600</pubDate><guid>https://kylrth.com/book/last-speakers/</guid><description>&lt;p>This book argues that language loss is always bad, but that we can do something to save it. While the stories in the book leave me feeling like every language lost is a terrible cost, I think it&amp;rsquo;s inevitable as our species merges into a global society due to technology. I think we ought to prioritize the proper treatment and respect of marginalized and alternative cultures, including their languages and how these cultures want to maintain them. But there is a cost to stopping &lt;em>all&lt;/em> language loss that is just not worth it once a language has been documented for research purposes.&lt;/p></description></item><item><title>The seven principles for making marriage work: a practical guide from the country's foremost relationship expert</title><link>https://kylrth.com/book/seven-principles-for-marriage/</link><pubDate>Wed, 26 May 2021 06:52:44 -0600</pubDate><guid>https://kylrth.com/book/seven-principles-for-marriage/</guid><description>&lt;p>Better communication doesn&amp;rsquo;t really solve marriage problems. It has a low success rate, and that makes sense because there are plenty of marriages that yell and dispute. Disputation is not a sign of an unhealthy marriage. You&amp;rsquo;d have to be really magnanimous to take criticism about you, even if presented as softly as possible.&lt;/p>
&lt;p>Personality does not make a marriage incompatible. People can be friends but have very distinct personalities. Handle each other&amp;rsquo;s strange side with caring and respect, as you would a friend.&lt;/p></description></item><item><title>Harry Potter and the methods of rationality</title><link>https://kylrth.com/book/hpmor/</link><pubDate>Thu, 20 May 2021 07:25:00 -0600</pubDate><guid>https://kylrth.com/book/hpmor/</guid><description>&lt;p>&lt;em>&lt;strong>Spoiler warning: no plot held back in this review.&lt;/strong>&lt;/em>&lt;/p>
&lt;h2 id="science-is-at-least-as-beautiful-as-magic">science is at least as beautiful as magic &lt;a class="header-link" href="#science-is-at-least-as-beautiful-as-magic">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>In chapter 7 Harry introduces Draco to the beauty of scientific advancement, and it actually moved me to tears. You should read the &lt;a href="https://www.hpmor.com/chapter/7">whole thing&lt;/a>, but here are some of the best quotes:&lt;/p>
&lt;blockquote>
&lt;p>&amp;ldquo;Anyway,&amp;rdquo; Harry said, &amp;ldquo;I&amp;rsquo;m saying that you don&amp;rsquo;t seem to have been paying much attention to what goes on in the Muggle world.&amp;rdquo; Probably because the whole wizarding world seemed to regard the rest of Earth as a slum, deserving around as much news coverage as the Financial Times awarded to the routine agonies of Burundi. &amp;ldquo;All right. Quick check. Have wizards ever been to the Moon? You know, that thing?&amp;rdquo; Harry pointed up to that huge and distant globe.&lt;/p></description></item><item><title>Planted: belief and belonging in an age of doubt</title><link>https://kylrth.com/book/planted/</link><pubDate>Sun, 18 Apr 2021 10:21:00 -0600</pubDate><guid>https://kylrth.com/book/planted/</guid><description>&lt;p>&lt;em>(My own thoughts appear as sidenotes or in italics, to distinguish from the author&amp;rsquo;s thoughts.)&lt;/em>&lt;/p>
&lt;p>Richard Bushman categorizes those who leave the church into two broad categories: those who feel &amp;ldquo;switched off&amp;rdquo;, and those who feel &amp;ldquo;squeezed out&amp;rdquo;. Mason summarizes the switched-off group as those who encounter troubling information about church history or doctrine, and as they discover more information they become jaded by it until they can no longer see the good the church does for them or for others. The squeezed-out group &amp;ldquo;fully embrace[s] the basic principles and ordinances of the gospel. But sometimes they feel alienated by things like the dominant political conservatism among the members &amp;hellip; or how the church ministers to our LGBT &amp;hellip; brothers and sisters,&amp;rdquo; (p. 3).&lt;/p></description></item><item><title>using GPG to prove you wrote your code</title><link>https://kylrth.com/post/gpg/</link><pubDate>Mon, 12 Apr 2021 07:13:54 -0600</pubDate><guid>https://kylrth.com/post/gpg/</guid><description>&lt;p>GPG is cool. You can use GPG to send encrypted messages, sign files to prove you generated them, and sign git commits to prove you committed them. You can get my key &lt;a href="https://kylrth.com/gpg.pub">here&lt;/a>. DigitalOcean has a &lt;a href="https://www.digitalocean.com/community/tutorials/how-to-use-gpg-to-encrypt-and-sign-messages">neat guide&lt;/a> to getting started with GPG. It explains asymmetric encryption, key generation and revocation, and key signing and maintenance.&lt;/p>
&lt;p>Git commit authorship can be modified by anyone, as demonstrated by &lt;a href="https://github.com/jayphelps/git-blame-someone-else">this tool&lt;/a>. But by uploading your GPG public key to GitHub, you allow anyone who trusts GitHub to be sure that commits marked &amp;ldquo;verified&amp;rdquo; were actually created by you.&lt;/p></description></item><item><title>favorite art</title><link>https://kylrth.com/post/art/</link><pubDate>Sat, 03 Apr 2021 22:16:52 -0600</pubDate><guid>https://kylrth.com/post/art/</guid><description>&lt;p>Here&amp;rsquo;s some of my favorite art.&lt;/p>
&lt;h2 id="edvard-munch-the-scream-1893">Edvard Munch, &lt;em>The Scream&lt;/em>, 1893 &lt;a class="header-link" href="#edvard-munch-the-scream-1893">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;img src="scream.jpg" alt="favorite art scream.jpg" class="img-zoomable">

&lt;p>(&lt;a href="https://commons.wikimedia.org/wiki/File:Edvard_Munch,_1893,_The_Scream,_oil,_tempera_and_pastel_on_cardboard,_91_x_73_cm,_National_Gallery_of_Norway.jpg">source&lt;/a>)&lt;/p>
&lt;h2 id="ben-shahn-all-that-is-beautiful-1966">Ben Shahn, &lt;em>All That Is Beautiful&lt;/em>, 1966 &lt;a class="header-link" href="#ben-shahn-all-that-is-beautiful-1966">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;img src="all_that_is_beautiful.jpg" alt="favorite art all_that_is_beautiful.jpg" class="img-zoomable">

&lt;p>(&lt;a href="https://americanart.si.edu/artwork/all-beautiful-22167">source&lt;/a>)&lt;/p>
&lt;h2 id="peter-doig-architect-1991">Peter Doig, &lt;em>Architect&amp;rsquo;s Home in the Ravine&lt;/em>, 1991 &lt;a class="header-link" href="#peter-doig-architect-1991">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;img src="architect_home_in_the_ravine.jpg" alt="favorite art architect_home_in_the_ravine.jpg" class="img-zoomable">

&lt;p>(&lt;a href="https://www.wikiart.org/en/peter-doig/architect-s-home-in-the-ravine-1991">source&lt;/a>)&lt;/p></description></item><item><title>The life-changing magic of tidying up: the Japanese art of decluttering and organizing</title><link>https://kylrth.com/book/life-changing-magic/</link><pubDate>Wed, 31 Mar 2021 21:21:04 -0700</pubDate><guid>https://kylrth.com/book/life-changing-magic/</guid><description>&lt;p>This book was my first real exposure to minimalism, and it completely changed how I feel about the possession of objects. It was super fortunate that my wife and I listened to it together on a road trip, and became equally enthralled with the idea of dumping all of our excess clutter.&lt;/p>
&lt;h2 id="all-at-once">all at once &lt;a class="header-link" href="#all-at-once">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>We have excess clutter because of a fundamental problem with the way we deal with possessions. This is difficult to solve with simple tricks like better organization or getting rid of one thing every day. She argues that the best way to overcome the clutter is by doing a one-time hardcore purge of the entire house. She spends a long time detailing what she calls the KonMari Method (which you can read about on her &lt;a href="https://konmari.com/what-is-konmari-method/">website&lt;/a>). For us, doing this initial purge took us about two weeks. We spent several hours each evening, plus all day on both Saturdays. All told I think we got rid of like 15 garbage bags-worth of things from our two-bedroom apartment.&lt;/p></description></item><item><title>The smartest kids in the world</title><link>https://kylrth.com/book/smartest-kids/</link><pubDate>Tue, 30 Mar 2021 07:00:00 -0700</pubDate><guid>https://kylrth.com/book/smartest-kids/</guid><description>&lt;p>The PISA test tests common senses reasoning. The countries that did best on the test were a surprise to everyone. Finland, South Korea, and Poland were all standouts in their own ways, and Ripley compares the policies and learning environments in these countries with those of the US to determine why the US is falling behind, especially in math and science.&lt;/p>
&lt;p>We talk a lot about parent involvement in the US, but the US actually has above average parental involvement. It turns out it&amp;rsquo;s not very correlated with success. Parent involvement is most useful when it&amp;rsquo;s reading to your child every day or asking them deep questions about school and about their opinions of broader issues. In the US we have lots of superintendents over small school districts, so there&amp;rsquo;s a lot of overhead without much benefit. We&amp;rsquo;re also afraid of standardization, so each school district has to sort of reinvent the wheel when it comes to curricula and standards. Common Core is an attempt by state education departments to agree to standards together so that textbooks and schools can target their products and training toward a common, well-defined set of goals.&lt;/p></description></item><item><title>The infinite Atonement</title><link>https://kylrth.com/book/infinite-atonement/</link><pubDate>Tue, 30 Mar 2021 06:30:00 -0700</pubDate><guid>https://kylrth.com/book/infinite-atonement/</guid><description>&lt;p>&lt;em>These notes are made while reading this with a Mormon theological background, so I skip noting some of the basic Mormon doctrines about the Atonement that he teaches.&lt;/em>&lt;/p>
&lt;p>The Atonement is the central doctrine of Christianity. All scripture should be at least partially focused on it, and we&amp;rsquo;re invited to &amp;ldquo;speak of the atonement of Christ, and attain to a perfect knowledge of him&amp;rdquo; (Jacob 4:12).&lt;/p>
&lt;h2 id="what-is-the-significance-of-the-atonement">What is the significance of the Atonement? &lt;a class="header-link" href="#what-is-the-significance-of-the-atonement">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>Here are some of the ways that we come to understand the Atonement:&lt;/p></description></item><item><title>How not to diet: the groundbreaking science of healthy, permanent weight loss</title><link>https://kylrth.com/book/how-not-to-diet/</link><pubDate>Wed, 03 Mar 2021 07:00:37 -0700</pubDate><guid>https://kylrth.com/book/how-not-to-diet/</guid><description>&lt;p>&lt;em>I read this book with Irresistible and the Social Dilemma on my mind, so I have a lot of notes here about addiction and big business.&lt;/em>&lt;/p>
&lt;p>Just like everything else, capitalism has screwed over our diets by giving companies the incentive to put shareholders above customers. Food companies employ lobbyists to keep subsidies on sugar/corn syrup/meat, and keep a stranglehold on public organizations. They buy billions of dollars of ads to communicate the message that it&amp;rsquo;s laziness that has caused the obesity epidemic and to push their products that appeal to the unconscious desires of our brains to produce artificial hunger.&lt;/p></description></item><item><title>The gene: an intimate history</title><link>https://kylrth.com/book/the-gene/</link><pubDate>Fri, 22 Jan 2021 06:14:58 -0700</pubDate><guid>https://kylrth.com/book/the-gene/</guid><description>&lt;p>&lt;em>These are notes I made after finishing the book, so they&amp;rsquo;ll be more heavily weighted toward concepts discussed near the end. The first half of the book was primarily dedicated to a history of genetic research, which I think helped the reader understand the issues discussed in the latter half.&lt;/em>&lt;/p>
&lt;h2 id="playing-god">playing God &lt;a class="header-link" href="#playing-god">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>It seems like our identity derives from a complicated combination of genes and chance environmental effects. Part of our strength as a species has been our natural variation, and to begin editing the genome is to assume that we can do it better than evolution has done up until this point. To choose to remove variations is to decide that normal is best. (Some of our most beautiful productions have been created by people who under our normal social environment would be considered mentally ill.) To remove variation or introduce variation thus has a literally existential effect on our identity. What does it mean for a process to understand its own instructions?&lt;/p></description></item><item><title>Faith is not blind</title><link>https://kylrth.com/book/faith-is-not-blind/</link><pubDate>Mon, 18 Jan 2021 08:11:23 -0700</pubDate><guid>https://kylrth.com/book/faith-is-not-blind/</guid><description>&lt;p>Elder Hafen struggled as a missionary with the concept of knowing versus believing: he felt he believed it was true, but not that he knew it. On the mission he felt pressure to bear testimony with the word &amp;ldquo;know&amp;rdquo;, but he chafed at that. In this book, Elder Hafen hopes to discuss the complex boundaries between believing and knowing, Richard Bushman, a prominent LDS historian, found himself in a similar situation. He felt that he didn&amp;rsquo;t have the right words to express his belief in the nuanced way that he needed, even though looking back he thinks he &lt;em>did&lt;/em> believe. This makes me look forward to reading Bushman&amp;rsquo;s &amp;ldquo;Rough Stone Rolling&amp;rdquo;, to try to understand his language of faith.&lt;/p></description></item><item><title>Cross-lingual alignment of contextual word embeddings, with applications to zero-shot dependency parsing</title><link>https://kylrth.com/paper/cross-lingual-alignment-contextual/</link><pubDate>Fri, 11 Dec 2020 06:30:43 -0700</pubDate><guid>https://kylrth.com/paper/cross-lingual-alignment-contextual/</guid><description>&lt;p>Recent contextual word embeddings (e.g. &lt;a href="https://kylrth.com/paper/deep-contextualized-word-representations/">ELMo&lt;/a>) have shown to be much better than &amp;ldquo;static&amp;rdquo; embeddings (where there&amp;rsquo;s a one-to-one mapping from token to representation). This paper is exciting because they were able to create a multi-lingual embedding space that used &lt;em>contextual&lt;/em> word embeddings.&lt;/p>
&lt;p>Each token will have a &amp;ldquo;point cloud&amp;rdquo; of embedding values, one point for each context containing the token. They define the &lt;em>embedding anchor&lt;/em> as the average of all those points for a particular token. Here&amp;rsquo;s a figure from the paper that displays a two-dimensional PCA of the contextual representations for four Spanish words, along with their anchors:&lt;/p></description></item><item><title>Inductive biases for deep learning of higher-level cognition</title><link>https://kylrth.com/paper/inductive-biases-higher-cognition/</link><pubDate>Tue, 08 Dec 2020 06:40:48 -0700</pubDate><guid>https://kylrth.com/paper/inductive-biases-higher-cognition/</guid><description>&lt;p>&lt;em>This is a long paper, so a lot of my writing here is an attempt to condense the discussion. I&amp;rsquo;ve taken the liberty to pull exact phrases and structure from the paper without explicitly using quotes.&lt;/em>&lt;/p>
&lt;blockquote>
&lt;p>Our main hypothesis is that deep learning succeeded in part because of a set of inductive biases, but that additional ones should be added in order to go from good in-distribution generalization in highly supervised learning tasks (or where strong and dense rewards are available), such as object recognition in images, to strong out-of-distribution generalization and transfer learning to new tasks with low sample complexity.&lt;/p></description></item><item><title>SpanBERT: improving pre-training by representing and predicting spans</title><link>https://kylrth.com/paper/spanbert/</link><pubDate>Sat, 05 Dec 2020 16:08:03 -0700</pubDate><guid>https://kylrth.com/paper/spanbert/</guid><description>&lt;p>&lt;a href="https://kylrth.com/paper/bert/">BERT&lt;/a> optimizes the Masked Language Model (MLM) objective by masking word pieces &lt;em>uniformly at random&lt;/em> in its training data and attempting to predict the masked values. With SpanBERT, spans of tokens are masked and the model is expected to predict the text in the spans from the representations of the words on the boundary. Span lengths follow a geometric distribution, and span start points are uniformly random.&lt;/p>
&lt;p>To predict each individual masked token, a two-layer feedforward network was provided with the boundary token representations plus the position embedding of the target token, and the output vector representation was used to predict the masked token and compute cross-entropy loss exactly as in standard MLM.&lt;/p></description></item><item><title>Tools and weapons: the promise and peril of the digital age</title><link>https://kylrth.com/book/tools-and-weapons/</link><pubDate>Thu, 03 Dec 2020 20:58:02 -0700</pubDate><guid>https://kylrth.com/book/tools-and-weapons/</guid><description>&lt;p>&lt;em>I started taking notes later in the book. There were lots of good insights in the first half. Sorry!&lt;/em>&lt;/p>
&lt;h2 id="broadband-access">broadband access &lt;a class="header-link" href="#broadband-access">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>Getting the internet to rural communities is a big deal for the rural economy. Just like electricity, it&amp;rsquo;s something that needs government support because there isn&amp;rsquo;t the economic incentive for ISPs to reach some of these locations.&lt;/p>
&lt;h2 id="ethical-ai">ethical AI &lt;a class="header-link" href="#ethical-ai">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>The focus on AI now is not just a fad, but a convergence of several trends that have made AI the next logical step: the increased computational resources, flexible access to compute through the cloud, etc.&lt;/p></description></item><item><title>Deep contextualized word representations</title><link>https://kylrth.com/paper/deep-contextualized-word-representations/</link><pubDate>Thu, 03 Dec 2020 12:01:43 -0700</pubDate><guid>https://kylrth.com/paper/deep-contextualized-word-representations/</guid><description>&lt;p>&lt;em>This is the original paper introducing Embeddings from Language Models (ELMo).&lt;/em>&lt;/p>
&lt;blockquote>
&lt;p>Unlike most widely used word embeddings, ELMo word representations are functions of the entire input sentence.&lt;/p>&lt;/blockquote>
&lt;p>That&amp;rsquo;s what makes ELMo great: they&amp;rsquo;re &lt;em>contextualized&lt;/em> word representations, meaning that they can express multiple possible senses of the same word.&lt;/p>
&lt;p>Specifically, ELMo representations are a learned linear combination of all layers of an LSTM encoding. The LSTM undergoes general semi-supervised pretraining, but the linear combination is learned &lt;em>specific to the task&lt;/em>. It&amp;rsquo;s been shown that initial layers in LSTM encoders are more representative of syntax, while later layers tend to represent semantics, so this linear combination is a key advantage that allows ELMo to improve accuracy on tasks ranging from POS tagging to question answering.&lt;/p></description></item><item><title>Removing a keyword from git history</title><link>https://kylrth.com/post/removing-keyword-from-git-history/</link><pubDate>Wed, 02 Dec 2020 11:34:25 -0700</pubDate><guid>https://kylrth.com/post/removing-keyword-from-git-history/</guid><description>&lt;p>I recently had to remove a keyword from the git history of a project I was working on. This meant not just &lt;a href="https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/removing-sensitive-data-from-a-repository#using-filter-branch">removing a file&lt;/a> but modifying commits where the keyword was added, commits where the keyword was removed, and even commits with the keyword in the commit message. I eventually came to the right solution through a mix of blog posts and the documentation for &lt;a href="https://git-scm.com/docs/git-rebase/">&lt;code>git rebase&lt;/code>&lt;/a>.&lt;/p>
&lt;p>For this example, assume the keyword is &amp;ldquo;matrix&amp;rdquo;. The example output shown is from the git repo for this website.&lt;/p></description></item><item><title>Blink: the power of thinking without thinking</title><link>https://kylrth.com/book/blink/</link><pubDate>Tue, 17 Nov 2020 20:44:48 -0700</pubDate><guid>https://kylrth.com/book/blink/</guid><description>&lt;p>Our subconscious not only manages bodily systems but also performs processing of features in our experience that our conscious does not have time to process. This has been proven in lots of experiments where people have been given subconscious cues to help them solve problems, but the people are unaware of this and make up answers when asked to explain how they came to conclusions. It&amp;rsquo;s important to trust these judgments that seem to come out of nowhere, but if we try to explain them we&amp;rsquo;ll start trying to provide rational answers, which can be totally false or misleading. (Think about when one of the jurors in the OJ Simpson trial said that race had absolutely nothing to do with their judgment.)&lt;/p></description></item><item><title>A short history of nearly everything</title><link>https://kylrth.com/book/short-history-nearly-everything/</link><pubDate>Wed, 07 Oct 2020 11:19:03 -0600</pubDate><guid>https://kylrth.com/book/short-history-nearly-everything/</guid><description>&lt;p>We are &lt;em>extremely&lt;/em> lucky to be here, and even more lucky to be able to appreciate it. Let&amp;rsquo;s not waste it.&lt;/p></description></item><item><title>Overcoming catastrophic forgetting in neural networks</title><link>https://kylrth.com/paper/overcoming-catastrophic-forgetting/</link><pubDate>Thu, 01 Oct 2020 10:47:28 -0600</pubDate><guid>https://kylrth.com/paper/overcoming-catastrophic-forgetting/</guid><description>&lt;p>In the paper they use Bayes&amp;rsquo; rule to show that the contribution of the first of two tasks is contained in the posterior distribution of model parameters over the first dataset. This is important because it means we can estimate that posterior to try to get a sense for which model parameters were most important for that first task.&lt;/p>
&lt;p>In this paper, they perform that estimation using a multivariate Gaussian distribution. The means are the values of the model parameters after training on the first dataset, and the precision (inverse of variance) is the values of the diagonals along the Fisher information matrix.&lt;/p></description></item><item><title>Learning neural causal models from unknown interventions</title><link>https://kylrth.com/paper/neural-causal-models/</link><pubDate>Tue, 22 Sep 2020 10:39:54 -0600</pubDate><guid>https://kylrth.com/paper/neural-causal-models/</guid><description>&lt;p>&lt;em>This is a follow-on to &lt;a href="https://kylrth.com/paper/meta-transfer-objective-for-causal-mechanisms/">A meta-transfer objective for learning to disentangle causal mechanisms&lt;/a>&lt;/em>&lt;/p>
&lt;p>Here we describe an algorithm for predicting the causal graph structure of a set of visible random variables, each possibly causally dependent on any of the other variables.&lt;/p>
&lt;h2 id="the-algorithm">the algorithm &lt;a class="header-link" href="#the-algorithm">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>There are two sets of parameters, the &lt;em>structural parameters&lt;/em> and the &lt;em>functional parameters&lt;/em>. The structural parameters compose a matrix where &lt;code>\(\sigma(\gamma_{ij})\)&lt;/code> represents the belief that variable &lt;code>\(X_j\)&lt;/code> is a direct cause of &lt;code>\(X_i\)&lt;/code>. The functional parameters are the parameters of the neural networks that model the conditional probability distribution of each random variable given its parent set.&lt;/p></description></item><item><title>A meta-transfer objective for learning to disentangle causal mechanisms</title><link>https://kylrth.com/paper/meta-transfer-objective-for-causal-mechanisms/</link><pubDate>Mon, 21 Sep 2020 08:46:30 -0600</pubDate><guid>https://kylrth.com/paper/meta-transfer-objective-for-causal-mechanisms/</guid><description>&lt;p>Theoretically, models should be able to predict on out-of-distribution data if their understanding of causal relationships is correct. The toy problem they use in this paper is that of predicting temperature from altitude. If a model is trained on data from Switzerland, the model should ideally be able to correctly predict on data from the Netherlands, even though it hasn&amp;rsquo;t seen elevations that low before.&lt;/p>
&lt;p>The main contribution of this paper is that they&amp;rsquo;ve found that models tend to transfer &lt;em>faster&lt;/em> to a new distribution when they learn the correct causal relationships, and when those relationships are &lt;em>sparsely represented&lt;/em>, meaning they are represented by relatively few nodes in the network. This allowed them to create a meta-learning objective that trains the model to represent the correct causal dependencies, allowing for improved generalization.&lt;/p></description></item><item><title>Deep learning generalizes because the parameter-function map is biased towards simple functions</title><link>https://kylrth.com/paper/parameter-function-map-biased-to-simple/</link><pubDate>Tue, 08 Sep 2020 07:29:09 -0600</pubDate><guid>https://kylrth.com/paper/parameter-function-map-biased-to-simple/</guid><description>&lt;p>The theoretical value in talking about the parameter-function map is that this map lets us talk about sets of parameters that produce the same function. In this paper they used some recently proven stuff from algorithmic information theory (AIT) to show that for neural networks the parameter-function map is biased toward functions with low Komolgorov complexity, meaning that simple functions are more likely to appear given random choice of parameters. Since real world problems are also biased toward simple functions, this could explain the generalization/memorization results found by &lt;a href="https://kylrth.com/paper/understanding-requires-rethinking-generalization/">Zhang &lt;em>et al&lt;/em>&lt;/a>.&lt;/p></description></item><item><title>The moment of lift: how empowering women changes the world</title><link>https://kylrth.com/book/moment-of-lift/</link><pubDate>Tue, 01 Sep 2020 05:25:38 -0600</pubDate><guid>https://kylrth.com/book/moment-of-lift/</guid><description>&lt;p>This book is about empowering women by giving them the freedom to make their own choices and speak for themselves. She said some important things about stigma in society. She talked specifically about the stigma of not talking about birth control, but she made general statements too. It&amp;rsquo;s each person&amp;rsquo;s responsibility to work against stigma and stop the human tendency to cast out others. I need to spend more time thinking about my own stigmas and biases, so that I can help those who are marginalized.&lt;/p></description></item><item><title>A closer look at memorization in deep networks</title><link>https://kylrth.com/paper/closer-look-at-memorization/</link><pubDate>Mon, 31 Aug 2020 11:52:35 -0600</pubDate><guid>https://kylrth.com/paper/closer-look-at-memorization/</guid><description>&lt;p>This paper builds on what we learned in &lt;a href="https://kylrth.com/paper/understanding-requires-rethinking-generalization/">&amp;ldquo;Understanding deep learning requires rethinking generalization&amp;rdquo;&lt;/a>. In that paper they showed that DNNs are able to fit pure noise in the same amount of time as it can fit real data, which means that our optimization algorithm (SGD, Adam, etc.) is not what&amp;rsquo;s keeping DNNs from overfitting.&lt;/p>
&lt;h2 id="experiments-for-detecting-easyhard-samples">experiments for detecting easy/hard samples &lt;a class="header-link" href="#experiments-for-detecting-easyhard-samples">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>It looks like there are qualitative differences between a DNN that has memorized some data and a DNN that has seen real data. In experiments they found that real datasets contain &amp;ldquo;easy examples&amp;rdquo; that are more quickly learned than the hard examples. This is not the case for random data.&lt;/p></description></item><item><title>Naked economics: undressing the dismal science</title><link>https://kylrth.com/book/naked-economics/</link><pubDate>Sun, 30 Aug 2020 06:46:49 -0600</pubDate><guid>https://kylrth.com/book/naked-economics/</guid><description>&lt;ul>
&lt;li>An important question is how much we need to fight income inequality. Is it fair to have 35% growth in the upper class and 3% growth in the lower class? Where is a good balance?&lt;/li>
&lt;li>We have grown a lot richer since the Industrial Revolution, because we&amp;rsquo;ve become more productive.&lt;/li>
&lt;li>Wealth is not a zero-sum game. Globalization is good because it allows us to buy cheaper, better products.
We can offset short-run job loss by paying or giving human capital to those who lose their jobs to globalization&lt;/li>
&lt;li>Policies often don&amp;rsquo;t do what we intend them to do, because they change people&amp;rsquo;s decisions for the involved choice. Often, the best way to do policy is to incentivize the appropriate decision.
E.g. Tax the purchase of inefficient cars on a scale, so car makers want to produce cheaper efficient cars to compete.
Do that instead of simply banning cars with 18 mpg or less.&lt;/li>
&lt;li>The Fed&amp;rsquo;s job is to control the availability of credit through setting their interest rates, so banks who borrow from them can adjust their interest rates accordingly. It&amp;rsquo;s really hard with so many factors, but Bernanke et. al. did an outstanding job.&lt;/li>
&lt;li>Things that help an economy or nation develop:
effective government institutionsproperty rightsno excessive regulationhuman capitalgeography (countries between the tropics have more diseases and less productive crops)openness to traderesponsible fiscal and monetary policy(natural resources don&amp;rsquo;t matter)democracypeaceinclusion of women in economic production&lt;/li>
&lt;li>We can help underdeveloped countries, but the question of how is complicated.
Certainly, we should open our borders to their products.Jeffrey Sachs &amp;ndash; all these countries need is capital, so developed countries should invest in them (e.g. through comprehensively fighting AIDS)William Easterly &amp;ndash; We can&amp;rsquo;t measure effectiveness based on inputs. That&amp;rsquo;s like rating Hollywood movies by budget size. &amp;ldquo;Instead we should do small, context-sensitive projects with measurable benefits.&amp;ldquo;But most of the developing world&amp;rsquo;s problems come from bad government policy.&lt;/li>
&lt;/ul></description></item><item><title>The faith of a scientist</title><link>https://kylrth.com/book/faith-of-a-scientist/</link><pubDate>Sun, 30 Aug 2020 06:46:49 -0600</pubDate><guid>https://kylrth.com/book/faith-of-a-scientist/</guid><description>&lt;p>Scientific thinking and religion go hand in hand, and help refine and give purpose to each other.&lt;/p>
&lt;p>Descartes&amp;rsquo; approach wasn&amp;rsquo;t as good as Newton&amp;rsquo;s. Descartes relied on the soundness of his own reasoning.&lt;/p>
&lt;p>&amp;ldquo;The erroneous conception that revelation ended with the apostles promotes the misconception among sectarian religions that the Gospel is complete and that with a liberal admixture of human wisdom, all will be crystal clear.&amp;rdquo;&lt;/p>
&lt;p>God places messages in everything. We study the scriptures, but we also study each scientific field looking for the truth He offers us there.&lt;/p></description></item><item><title>Weapons of math destruction: how big data increases inequality and threatens democracy</title><link>https://kylrth.com/book/weapons-of-math-destruction/</link><pubDate>Sun, 30 Aug 2020 06:46:49 -0600</pubDate><guid>https://kylrth.com/book/weapons-of-math-destruction/</guid><description>&lt;blockquote>
&lt;p>In fact, I saw all kinds of parallels between finance and Big Data. Both industries gobble up the same pool of talent, much of it from elite universities like MIT, Princeton, or Stanford. These new hires are ravenous for success and have been focused on external metrics&amp;ndash;like SAT scores and college admissions&amp;ndash;their entire lives. Whether in finance or tech, the message they&amp;rsquo;ve received is that they will be rich, that they will run the world. Their productivity indicates that they&amp;rsquo;re on the right track, and it translates into dollars. This leads to the fallacious conclusion that whatever they&amp;rsquo;re doing to bring in more money is good. It &amp;ldquo;adds value.&amp;rdquo; Otherwise, why would the market reward it?&lt;/p></description></item><item><title>A disciplined approach to neural network hyperparameters: part 1</title><link>https://kylrth.com/paper/disciplined-approach-to-hyperparameters/</link><pubDate>Fri, 28 Aug 2020 14:16:29 -0600</pubDate><guid>https://kylrth.com/paper/disciplined-approach-to-hyperparameters/</guid><description>&lt;p>The goal of hyperparameter tuning is to reach the point where test loss is horizontal on the graph over model complexity.&lt;/p>
&lt;p>Underfitting can be observed with a small learning rate, simple architecture, or complex data distribution. You can observe underfitting decrease by seeing more drastic results at the outset, followed by a more horizontal line further into training. You can use the LR range test to find a good learning rate range, and then use a cyclical learning rate to move up and down within that range.&lt;/p></description></item><item><title>Forward and reverse gradient-based hyperparameter optimization</title><link>https://kylrth.com/paper/gradient-based-hyperparameter-optimization/</link><pubDate>Fri, 28 Aug 2020 14:16:29 -0600</pubDate><guid>https://kylrth.com/paper/gradient-based-hyperparameter-optimization/</guid><description>&lt;p>In the area of hyperparameter optimization (HO), the goal is to optimize a &lt;em>response function&lt;/em> of the hyperparameters. The response function is usually the average loss on a validation set. Gradient-based HO refers to iteratively finding the optimal hyperparameters using gradient updates, just as we do with neural network training itself. The gradient of the response function with respect to the hyperparameters is called the &lt;em>hypergradient&lt;/em>.&lt;/p>
&lt;p>One of the great things about this work is that their framework allows for all kinds of hyperparameters. The response function can be based on evaluation over the training set, the validation set, or both. The hyperparameters can be part of the loss function, part of regularization, or part of the model architecture.&lt;/p></description></item><item><title>Understanding deep learning requires rethinking generalization</title><link>https://kylrth.com/paper/understanding-requires-rethinking-generalization/</link><pubDate>Wed, 26 Aug 2020 08:42:58 -0600</pubDate><guid>https://kylrth.com/paper/understanding-requires-rethinking-generalization/</guid><description>&lt;p>It turns out that neural networks can reach training loss of 0 even on randomly labeled data, even when the data itself is random. It was previously thought that some implicit bias in the model architecture prevented (or &lt;em>regularized the model away from&lt;/em>) overfitting to specific training examples, but that&amp;rsquo;s obviously not true. They showed this empirically as just described, and also theoretically constructed a two-layer ReLU network with &lt;code>\(p=2n+d\)&lt;/code> parameters to express &lt;em>any labeling&lt;/em> of any sample of size &lt;code>\(n\)&lt;/code> in &lt;code>\(d\)&lt;/code> dimensions. The proof was actually relatively easy to follow.&lt;/p></description></item><item><title>Why does unsupervised pre-training help deep learning?</title><link>https://kylrth.com/paper/why-unsupervised-helps/</link><pubDate>Mon, 24 Aug 2020 11:40:00 -0600</pubDate><guid>https://kylrth.com/paper/why-unsupervised-helps/</guid><description>&lt;p>They&amp;rsquo;re pretty sure that it performs regularization by starting off the supervised training in a good spot, instead of by somehow improving the optimization path.&lt;/p></description></item><item><title>Essentialism</title><link>https://kylrth.com/book/essentialism/</link><pubDate>Sun, 23 Aug 2020 07:47:12 -0600</pubDate><guid>https://kylrth.com/book/essentialism/</guid><description>&lt;p>The main character of the first story slowly changed his attitude toward demands on his resources.&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;Can I actually fulfill this request, given the time and resources I have?&amp;rdquo;&lt;/li>
&lt;li>&amp;ldquo;Is this the very most important thing I should be doing with my time and resources right now?&amp;rdquo;&lt;/li>
&lt;li>&amp;ldquo;Just because I was invited didn&amp;rsquo;t seem a good enough reason to attend.&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;p>It&amp;rsquo;s important to pursue &amp;ldquo;less but better&amp;rdquo; in a disciplined way. Simply doing fewer things is not meaningful. Most things in the world are trivial noise, and my job is to sift through noise and find things that are truly valuable. This involves active consideration of responsibilities and trade-offs involved in making a choice. I think I often try to &amp;ldquo;force execution at the last moment&amp;rdquo;, which is a mark of a non-essentialist.&lt;/p></description></item><item><title>The consciousness prior</title><link>https://kylrth.com/paper/consciousness-prior/</link><pubDate>Fri, 14 Aug 2020 09:05:56 -0700</pubDate><guid>https://kylrth.com/paper/consciousness-prior/</guid><description>&lt;p>System 1 cognitive abilities are about low-level perception and intuitive knowledge. System 2 cognitive abilities can be described verbally, and include things like reasoning, planning, and imagination. In cognitive neuroscience, the &amp;ldquo;Global Workspace Theory&amp;rdquo; says that at each moment specific pieces of information become a part of working memory and become globally available to other unconscious computational processes. Relative to the unconscious state, the conscious state is low-dimensional, focusing on a few things. The paper proposes we use an attention mechanism (in the sense of the Bahdanau 2015 paper) to produce the conscious state, and then a VAE or conditional GAN to produce the output from the conscious state.&lt;/p></description></item><item><title>Troubling trends in machine learning scholarship</title><link>https://kylrth.com/paper/troubling-trends-in-ml/</link><pubDate>Thu, 13 Aug 2020 10:36:05 -0700</pubDate><guid>https://kylrth.com/paper/troubling-trends-in-ml/</guid><description>&lt;p>The authors discuss four trends in AI research that have negative consequences for the community.&lt;/p>
&lt;h2 id="problems">problems &lt;a class="header-link" href="#problems">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;h3 id="explanation-vs-speculation">explanation vs. speculation &lt;a class="header-link" href="#explanation-vs-speculation">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h3>
&lt;p>It&amp;rsquo;s important to allow researchers to include speculation, because speculation is what allows ideas to form. But the paper has to carefully couch speculation inside a &amp;ldquo;Motivations&amp;rdquo; section or other verbage to ensure the reader understands its place.&lt;/p>
&lt;p>It&amp;rsquo;s extremely important to define concepts before using them. Terms like &lt;em>internal covariate shift&lt;/em> or &lt;em>coverage&lt;/em> sound like definitions without actually being such.&lt;/p></description></item><item><title>Attention is all you need</title><link>https://kylrth.com/paper/attention-all-you-need/</link><pubDate>Wed, 05 Aug 2020 12:37:42 -0700</pubDate><guid>https://kylrth.com/paper/attention-all-you-need/</guid><description>&lt;p>&lt;em>I also referred to &lt;a href="https://github.com/lilianweng/transformer-tensorflow">this implementation&lt;/a> to understand some of the details.&lt;/em>&lt;/p>
&lt;p>This is the paper describing the Transformer, a sequence-to-sequence model based entirely on attention. I think it&amp;rsquo;s best described with pictures.&lt;/p>
&lt;h2 id="model-overview">model overview &lt;a class="header-link" href="#model-overview">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;img src="transformer.png" alt="Attention is all you need transformer.png" class="img-zoomable">

&lt;p>From this picture, I think the following things need explaining:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>embeddings&lt;/strong> these are learned embeddings that convert the input and output tokens to vectors of the model dimension. In this paper, they actually used the same weight matrix for input embedding, output embedding, and the final linear layer before the final softmax.&lt;/li>
&lt;li>&lt;strong>positional encoding&lt;/strong>: since there&amp;rsquo;s no concept of a hidden state or convolution that encodes the order of the inputs, we have to add some information about the position of the tokens. They used a sinusoidal positional encoding that was a function of the position and the dimension. The wavelength for each dimension forms a geometric progression from &lt;code>\(2\pi\)&lt;/code> to 10000 times that.&lt;/li>
&lt;li>&lt;strong>the outputs are &amp;ldquo;shifted right&amp;rdquo;&lt;/strong>&lt;/li>
&lt;li>&lt;strong>multi-head attention&lt;/strong>: see below for a description of multi-head attention. In the encoder-decoder attention layers, &lt;code>\(Q\)&lt;/code> comes from the previous masked attention layer and &lt;code>\(K\)&lt;/code> and &lt;code>\(V\)&lt;/code> come from the output of the encoder. Everywhere else uses self-attention, meaning that &lt;code>\(Q\)&lt;/code>, &lt;code>\(K\)&lt;/code>, and &lt;code>\(V\)&lt;/code> are all the same.&lt;/li>
&lt;li>&lt;strong>&lt;em>masked&lt;/em> multi-head attention&lt;/strong>: in the self-attention layers in the decoder, we can&amp;rsquo;t allow positions to attend to positions ahead of themselves, so we set all right-connecting values in the input of the softmax (right after scaling; see the image below) to negative infinity.&lt;/li>
&lt;li>&lt;strong>feed-forward blocks&lt;/strong> these are two linear transformation with ReLU in between. The transformations are the same across each position, but they are different transformations from layer to layer, as you might expect.&lt;/li>
&lt;li>&lt;strong>add &amp;amp; norm&lt;/strong>: these are residual connections followed by layer normalization.&lt;/li>
&lt;/ol>
&lt;h3 id="multi-head-attention">multi-head attention &lt;a class="header-link" href="#multi-head-attention">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h3>
&lt;img src="multi-head-attention.png" alt="Attention is all you need multi-head-attention.png" class="img-zoomable">

&lt;p>The &amp;ldquo;Mask (opt.)&amp;rdquo; can be ignored because that&amp;rsquo;s for masked attention, described above.&lt;/p></description></item><item><title>BERT: pre-training of deep bidirectional transformers for language understanding</title><link>https://kylrth.com/paper/bert/</link><pubDate>Tue, 04 Aug 2020 08:57:44 -0700</pubDate><guid>https://kylrth.com/paper/bert/</guid><description>&lt;p>The B is for bidirectional, and that&amp;rsquo;s a big deal. It makes it possible to do well on sentence-level (NLI, question answering) and token-level tasks (NER, POS tagging). In a unidirectional model, the word &amp;ldquo;bank&amp;rdquo; in a sentence like &amp;ldquo;I made a bank deposit.&amp;rdquo; has only &amp;ldquo;I made a&amp;rdquo; as its context, keeping useful information from the model.&lt;/p>
&lt;p>Another cool thing is masked language model training (MLM). They train the model by blanking certain words in the sentence and asking the model to guess the missing word.&lt;/p></description></item><item><title>Compositional generalization by factorizing alignment and translation</title><link>https://kylrth.com/paper/factorizing-alignment-and-translation/</link><pubDate>Mon, 27 Jul 2020 09:11:16 -0700</pubDate><guid>https://kylrth.com/paper/factorizing-alignment-and-translation/</guid><description>&lt;p>They had a biRNN with attention for alignment encoding, and then a single linear function of each one-hot encoded word for encoding that single word. Their reasoning was that by separating the alignment from the meaning of individual words the model could more easily generalize to unseen words.&lt;/p></description></item><item><title>Semi-supervised training for automatic speech recognition</title><link>https://kylrth.com/paper/semi-supervised-for-asr/</link><pubDate>Tue, 14 Jul 2020 08:06:00 -0600</pubDate><guid>https://kylrth.com/paper/semi-supervised-for-asr/</guid><description>&lt;p>&lt;em>This was Manohar&amp;rsquo;s PhD dissertation at JHU.&lt;/em>&lt;/p>
&lt;p>Chapter 2 provides a relatively clear overview of how chain and non-chain models work in Kaldi.&lt;/p>
&lt;p>In chapter 3 he tried using negative conditional entropy as the loss function for the unsupervised data, and it helped a bit.&lt;/p>
&lt;p>In chapter 4 Manohar uses [CTC loss]/paper/ctc/.&lt;/p>
&lt;p>In chapter 5, he discusses ways to do semi-supervised model training. It&amp;rsquo;s nice when you have parallel data in different domains, because then you can do a student-teacher model. When there&amp;rsquo;s no parallel data, the best you can do is decode the unsupervised data with the seed model and use that to train the LF-MMI model (see section 5.2.1).&lt;/p></description></item><item><title>Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title><link>https://kylrth.com/paper/ctc/</link><pubDate>Fri, 10 Jul 2020 09:14:59 -0600</pubDate><guid>https://kylrth.com/paper/ctc/</guid><description>&lt;p>RNNs generally require pre-segmented training data, but this avoids that need.&lt;/p>
&lt;p>Basically, you have the RNN output probabilities for each label (or a blank) for every frame, and then you find the most likely path across that lattice of probabilities.&lt;/p>
&lt;p>The section explaining the loss function was kind of complicated. They used their forward-backward algorithm (sort of like Viterbi) to get the probability of all paths corresponding to the output that go through each symbol at each time, and then they differentiated that to get the derivatives with respect to the outputs. Then it was backpropagation as normal from that point.&lt;/p></description></item><item><title>Google's neural machine translation system: bridging the gap between human and machine translation</title><link>https://kylrth.com/paper/google-nmt-2016/</link><pubDate>Tue, 30 Jun 2020 08:22:30 -0600</pubDate><guid>https://kylrth.com/paper/google-nmt-2016/</guid><description>&lt;p>&lt;em>This model was superseded by &lt;a href="https://kylrth.com/paper/google-zero-shot/">this one&lt;/a>.&lt;/em>&lt;/p>
&lt;p>They did some careful things with residual connections to make sure it was very parallelizable. They put each LSTM layer on a separate GPU. They quantized the models such that they could train using full floating-point computations with a couple restrictions and then convert the models to quantized versions.&lt;/p></description></item><item><title>Google's multilingual neural machine translation system</title><link>https://kylrth.com/paper/google-zero-shot/</link><pubDate>Fri, 26 Jun 2020 08:02:12 -0600</pubDate><guid>https://kylrth.com/paper/google-zero-shot/</guid><description>&lt;p>They use the word-piece model from &lt;a href="https://kylrth.com/paper/word-piece-model/">&amp;ldquo;Japanese and Korean Voice Search&amp;rdquo;&lt;/a>, with 32,000 word pieces. (This is a lot less than the 200,000 used in that paper.) They state in the paper that the shared word-piece model is very similar to Byte-Pair-Encoding, which was used for NMT in &lt;a href="https://www.aclweb.org/anthology/P16-1162.pdf">this paper&lt;/a> by researchers at U of Edinburgh.&lt;/p>
&lt;p>The model and training process are exactly as in &lt;a href="https://kylrth.com/paper/google-nmt-2016/">Google&amp;rsquo;s earlier paper&lt;/a>. It takes &lt;em>3 weeks&lt;/em> on &lt;em>100 GPUs&lt;/em> to train, even after increasing batch size and learning rate.&lt;/p></description></item><item><title>Japanese and Korean voice search</title><link>https://kylrth.com/paper/word-piece-model/</link><pubDate>Wed, 24 Jun 2020 14:44:02 -0600</pubDate><guid>https://kylrth.com/paper/word-piece-model/</guid><description>&lt;p>&lt;em>This was mentioned in the paper on &lt;a href="https://kylrth.com/paper/google-zero-shot/">Google&amp;rsquo;s Multilingual Neural Machine Translation System&lt;/a>. It&amp;rsquo;s regarded as the original paper to use the word-piece model, which is the focus of my notes here.&lt;/em>&lt;/p>
&lt;h2 id="the-wordpiecemodel">the WordPieceModel &lt;a class="header-link" href="#the-wordpiecemodel">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>Here&amp;rsquo;s the WordPieceModel algorithm:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-txt" data-lang="txt">&lt;span style="display:flex;">&lt;span>func WordPieceModel(D, chars, n, threshold) -&amp;gt; inventory:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> # D: training data
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> # n: user-specified number of word units (often 200k)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> # chars: unicode characters used in the language (e.g. Kanji, Hiragana, Katakana, ASCII for Japanese)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> # threshold: stopping criterion for likelihood increase
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> # inventory: the set of word units created by the model
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> inventory := chars
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> likelihood := +INF
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> while len(inventory) &amp;lt; n &amp;amp;&amp;amp; likelihood &amp;gt;= threshold:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lm := LM(inventory, D)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> inventory += argmax_{combined word unit}(lm.likelihood_{inventory + combined word unit}(D))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> likelihood = lm.likelihood_{inventory}(D)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> return inventory
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The algorithm can be optimized by&lt;/p></description></item><item><title>Towards a multi-view language representation</title><link>https://kylrth.com/paper/multi-view-language-representation/</link><pubDate>Tue, 23 Jun 2020 08:40:04 -0600</pubDate><guid>https://kylrth.com/paper/multi-view-language-representation/</guid><description>&lt;p>They used a technique called CCA to combine hand-made features with NN representations. It didn&amp;rsquo;t do great on typological feature prediction, but it did do well with predicting a phylogenetic tree for Indo-European languages.&lt;/p></description></item><item><title>Universal phone recognition with a multilingual allophone system</title><link>https://kylrth.com/paper/universal-phone-recognition/</link><pubDate>Tue, 23 Jun 2020 08:33:48 -0600</pubDate><guid>https://kylrth.com/paper/universal-phone-recognition/</guid><description>&lt;p>These guys made sure to model allophones. They had an encoder that produced a universal phone set, and then language-specific decoders. This meant they could use data from various languages to train the system. The decoder has an &lt;em>allophone layer&lt;/em>, followed by other dense trainable layers. The allophone layer is a single trainable dense layer, but was initialized by a bunch of linguists who sat down and described the phone sets belonging to each phoneme in each language present in the training set. They added an L2 penalty to divergence from the original linguist-created matrix.&lt;/p></description></item><item><title>using Matrix</title><link>https://kylrth.com/post/matrix-registration/</link><pubDate>Sun, 02 Feb 2020 12:34:56 -0700</pubDate><guid>https://kylrth.com/post/matrix-registration/</guid><description>&lt;p>Matrix is a &lt;em>federated&lt;/em>, open source chat system. Federation means that people can message each other across different servers, like in the image below. In that way, it works sort of like email: even though you may use &lt;code>you@gmail.com&lt;/code> and I might use &lt;code>me@kylrth.com&lt;/code>, we can still write each other emails.&lt;/p>
&lt;img src="federation.png" alt="using Matrix federation.png" class="img-zoomable">

&lt;p>In our case, I host the server at matrix.kylrth.com, and you and I can connect to it with various &lt;a href="https://matrix.org/ecosystem/clients/">clients&lt;/a>. We can write each other messages, but we can also communicate with people on other Matrix servers.&lt;span class="sidenote-number">&lt;small class="sidenote">There are a ton of cool features to Matrix, such as &lt;a href="https://matrix.org/bridges/">bridging&lt;/a>, voice and video calls, and encryption. You can read a more extensive beginner&amp;rsquo;s guide to Matrix &lt;a href="https://joinmatrix.org/">here&lt;/a>. Also check out the Matrix &lt;a href="https://matrix.org">website&lt;/a>.&lt;/small>&lt;/span>&lt;/p></description></item><item><title>Increase in learning: spiritual patterns for obtaining your own answers</title><link>https://kylrth.com/book/increase-in-learning/</link><pubDate>Wed, 20 Nov 2019 10:14:43 -0700</pubDate><guid>https://kylrth.com/book/increase-in-learning/</guid><description>&lt;h2 id="chapter-1">Chapter 1 &lt;a class="header-link" href="#chapter-1">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>We are give the opportunity to have the Spirit as a &lt;em>constant&lt;/em> companion! To take advantage, we need to sincerely desire it, invite it through action, and be worthy of it through obedience.&lt;/p>
&lt;h2 id="chapter-2">Chapter 2 &lt;a class="header-link" href="#chapter-2">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>&lt;strong>Knowledge&lt;/strong> is the accumulation of facts.&lt;/p>
&lt;p>&lt;strong>Understanding&lt;/strong> comes when we apply our hearts to knowledge, which lets the Holy Ghost testify to us of the truthfulness of it. Understanding comes by revelation.&lt;/p></description></item></channel></rss>