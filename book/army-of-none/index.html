<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>Army of none: autonomous weapons and the future of war</title><meta http-equiv=onion-location content="http://kylrthjj7mpvktolz7u6fnudt3hpdvjw4hzquanjpepgsf5vcq5divad.onion/book/army-of-none/"><meta property="og:title" content="Army of none: autonomous weapons and the future of war"><meta name=twitter:title content="Army of none: autonomous weapons and the future of war"><meta name=author content="Paul Scharre"><meta property="og:site_name" content="Kyle Roth"><meta property="og:url" content="https://kylrth.com/book/army-of-none/"><meta property="og:image" content="https://images-na.ssl-images-amazon.com/images/S/compressed.photo.goodreads.com/books/1529056546i/40180025.jpg"><meta name=twitter:image content="https://images-na.ssl-images-amazon.com/images/S/compressed.photo.goodreads.com/books/1529056546i/40180025.jpg"><meta name=twitter:card content="summary"><meta property="og:type" content="article"><meta name=generator content="Hugo 0.111.3"><link rel=stylesheet href=/css/style.min.css><script type=text/javascript src=/js/bundle.js></script></head><body><a href=#main class="skip-link p-screen-reader-text">Skip to content</a><svg xmlns="http://www.w3.org/2000/svg" style="display:none" aria-hidden="true"><symbol id="icon-permalink" viewBox="0 0 24 24"><g><path d="M0 0h24v24H0z" fill="none"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></g></symbol><symbol id="icon-feed" viewBox="0 0 16 16"><g><path d="M2.13 11.733c-1.175.0-2.13.958-2.13 2.126.0 1.174.955 2.122 2.13 2.122a2.126 2.126.0 002.133-2.122A2.133 2.133.0 002.13 11.733zM.002 5.436v3.067c1.997.0 3.874.781 5.288 2.196a7.45 7.45.0 012.192 5.302h3.08c0-5.825-4.739-10.564-10.56-10.564zM.006.0v3.068C7.128 3.068 12.924 8.87 12.924 16H16C16 7.18 8.824.0.006.0z"/></g></symbol><symbol id="icon-github" viewBox="0 0 16 16"><g><path d="M8 .198A8 8 0 005.471 15.789c.4.074.547-.174.547-.385.0-.191-.008-.821-.011-1.489-2.226.484-2.695-.944-2.695-.944-.364-.925-.888-1.171-.888-1.171-.726-.497.055-.486.055-.486.803.056 1.226.824 1.226.824.714 1.223 1.872.869 2.328.665.072-.517.279-.87.508-1.07-1.777-.202-3.645-.888-3.645-3.954.0-.873.313-1.587.824-2.147-.083-.202-.357-1.015.077-2.117.0.0.672-.215 2.201.82A7.672 7.672.0 018 4.066c.68.003 1.365.092 2.004.269 1.527-1.035 2.198-.82 2.198-.82.435 1.102.162 1.916.079 2.117.513.56.823 1.274.823 2.147.0 3.073-1.872 3.749-3.653 3.947.287.248.543.735.543 1.481.0 1.07-.009 1.932-.009 2.195.0.213.144.462.55.384A8 8 0 008.001.196z"/></g></symbol><symbol id="icon-gitlab" viewBox="0 0 28 28"><g><path d="M1.625 11.031 14 26.89.437 17.046a1.092 1.092.0 01-.391-1.203l1.578-4.813zm7.219.0h10.313L14.001 26.89zM5.75 1.469l3.094 9.562H1.625l3.094-9.562a.548.548.0 011.031.0zm20.625 9.562 1.578 4.813a1.09 1.09.0 01-.391 1.203l-13.563 9.844 12.375-15.859zm0 0h-7.219l3.094-9.562a.548.548.0 011.031.0z"/></g></symbol><symbol id="icon-instagram" viewBox="0 0 22 22"><g><path d="M15.445.0H6.554A6.559 6.559.0 000 6.554v8.891A6.559 6.559.0 006.554 22h8.891a6.56 6.56.0 006.554-6.555V6.554A6.557 6.557.0 0015.445.0zm4.342 15.445a4.343 4.343.0 01-4.342 4.342H6.554a4.341 4.341.0 01-4.341-4.342V6.554a4.34 4.34.0 014.341-4.341h8.891a4.342 4.342.0 014.341 4.341l.001 8.891z"/><path d="M11 5.312A5.693 5.693.0 005.312 11 5.694 5.694.0 0011 16.688 5.694 5.694.0 0016.688 11 5.693 5.693.0 0011 5.312zm0 9.163a3.475 3.475.0 11-.001-6.95 3.475 3.475.0 01.001 6.95zm5.7-10.484a1.363 1.363.0 11-1.364 1.364c0-.752.51-1.364 1.364-1.364z"/></g></symbol><symbol id="icon-linkedin" viewBox="0 0 16 16"><g><path d="M6 6h2.767v1.418h.04C9.192 6.727 10.134 6 11.539 6 14.46 6 15 7.818 15 10.183V15h-2.885v-4.27c0-1.018-.021-2.329-1.5-2.329-1.502.0-1.732 1.109-1.732 2.255V15H6V6zM1 6h3v9H1V6zM4 3.5A1.5 1.5.0 11.999 3.499 1.5 1.5.0 014 3.5z"/></g></symbol><symbol id="icon-medium" viewBox="0 0 24 24"><g><path d="M22.085 4.733 24 2.901V2.5h-6.634l-4.728 11.768L7.259 2.5H.303v.401L2.54 5.594c.218.199.332.49.303.783V16.96c.069.381-.055.773-.323 1.05L0 21.064v.396h7.145v-.401l-2.52-3.049a1.244 1.244.0 01-.347-1.05V7.806l6.272 13.659h.729l5.393-13.659v10.881c0 .287.0.346-.188.534l-1.94 1.877v.402h9.412v-.401l-1.87-1.831a.556.556.0 01-.214-.534V5.267a.554.554.0 01.213-.534z"/></g></symbol><symbol id="icon-npm" viewBox="0 0 16 16"><g><path d="M0 0v16h16V0H0zm13 13h-2V5H8v8H3V3h10v10z"/></g></symbol><symbol id="icon-twitter" viewBox="0 0 16 16"><g><path d="M16 3.538a6.461 6.461.0 01-1.884.516 3.301 3.301.0 001.444-1.816 6.607 6.607.0 01-2.084.797 3.28 3.28.0 00-2.397-1.034A3.28 3.28.0 007.882 6.029 9.321 9.321.0 011.116 2.598a3.284 3.284.0 001.015 4.381A3.301 3.301.0 01.643 6.57v.041A3.283 3.283.0 003.277 9.83a3.291 3.291.0 01-1.485.057 3.293 3.293.0 003.066 2.281 6.586 6.586.0 01-4.862 1.359 9.286 9.286.0 005.034 1.475c6.037.0 9.341-5.003 9.341-9.341.0-.144-.003-.284-.009-.425a6.59 6.59.0 001.637-1.697z"/></g></symbol><symbol id="icon-vimeo" viewBox="0 0 16 16"><g><path d="M15.994 4.281c-.072 1.556-1.159 3.691-3.263 6.397-2.175 2.825-4.016 4.241-5.522 4.241-.931.0-1.722-.859-2.366-2.581-.431-1.578-.859-3.156-1.291-4.734-.478-1.722-.991-2.581-1.541-2.581-.119.0-.538.253-1.256.753l-.753-.969c.791-.694 1.569-1.388 2.334-2.081 1.053-.909 1.844-1.387 2.372-1.438 1.244-.119 2.013.731 2.3 2.553.309 1.966.525 3.188.647 3.666.359 1.631.753 2.447 1.184 2.447.334.0.838-.528 1.509-1.588.669-1.056 1.028-1.862 1.078-2.416.097-.912-.262-1.372-1.078-1.372a2.98 2.98.0 00-1.184.263c.787-2.575 2.287-3.825 4.506-3.753 1.641.044 2.416 1.109 2.322 3.194z"/></g></symbol><symbol id="icon-wordpress" viewBox="0 0 16 16"><g><path d="M2 8c0 2.313 1.38 4.312 3.382 5.259L2.52 5.622A5.693 5.693.0 002 8zm10.05-.295c0-.722-.266-1.222-.495-1.612-.304-.482-.589-.889-.589-1.371.0-.537.418-1.037 1.008-1.037.027.0.052.003.078.005A6.064 6.064.0 008 2.156 6.036 6.036.0 002.987 4.79c.141.004.274.007.386.007.627.0 1.599-.074 1.599-.074.323-.018.361.444.038.482.0.0-.325.037-.687.055l2.185 6.33 1.313-3.835-.935-2.495a12.304 12.304.0 01-.629-.055c-.323-.019-.285-.5.038-.482.0.0.991.074 1.58.074.627.0 1.599-.074 1.599-.074.323-.018.362.444.038.482.0.0-.326.037-.687.055l2.168 6.282.599-1.947c.259-.809.457-1.389.457-1.889zm-3.945.806-1.8 5.095a6.148 6.148.0 003.687-.093.52.52.0 01-.043-.081L8.105 8.511zm5.16-3.315c.026.186.04.386.04.601.0.593-.114 1.259-.456 2.093l-1.833 5.16c1.784-1.013 2.983-2.895 2.983-5.051a5.697 5.697.0 00-.735-2.803zM8 0a8 8 0 100 16A8 8 0 008 0zm0 15A7 7 0 118 1a7 7 0 010 14z"/></g></symbol><symbol id="icon-youtube" viewBox="0 0 16 16"><g><path d="M15.841 4.8s-.156-1.103-.637-1.587c-.609-.637-1.291-.641-1.603-.678-2.237-.163-5.597-.163-5.597-.163h-.006s-3.359.0-5.597.163c-.313.038-.994.041-1.603.678C.317 3.697.164 4.8.164 4.8S.005 6.094.005 7.391v1.213c0 1.294.159 2.591.159 2.591s.156 1.103.634 1.588c.609.637 1.409.616 1.766.684 1.281.122 5.441.159 5.441.159s3.363-.006 5.6-.166c.313-.037.994-.041 1.603-.678.481-.484.637-1.588.637-1.588s.159-1.294.159-2.591V7.39c-.003-1.294-.162-2.591-.162-2.591zm-9.494 5.275V5.578l4.322 2.256-4.322 2.241z"/></g></symbol><symbol id="icon-matrix" viewBox="0 0 28 28"><g><path d="M.975097.640961V27.359H2.89517V28H.238281V0H2.89517V.640961H.975097z"/><path d="M8.37266 9.11071V10.4628H8.4111C8.7712 9.94812 9.20494 9.54849 9.71306 9.26518 10.2208 8.98235 10.8029 8.84036 11.4586 8.84036 12.0885 8.84036 12.664 8.96298 13.1846 9.2074c.5208.24483.9163.67596 1.1864 1.2941C14.6665 10.0638 15.0683 9.67744 15.5764 9.34266 16.0842 9.00804 16.6852 8.84036 17.3797 8.84036 17.9069 8.84036 18.3953 8.90487 18.8457 9.03365c.4498.128769999999999.8355.33478 1.157.61801C20.3239 9.93515 20.5746 10.3053 20.755 10.7621c.1799.4575.27 1.0077.27 1.6518v6.6827H18.2861V13.4373C18.2861 13.1027 18.2734 12.7872 18.2475 12.4908 18.2216 12.1949 18.1512 11.9375 18.0354 11.7183 17.9196 11.4996 17.7491 11.3256 17.5243 11.1967 17.2993 11.0684 16.9938 11.0037 16.6081 11.0037c-.3856.0-.6975.0745000000000005-.9354.222C15.4346 11.374 15.2483 11.5673 15.1134 11.8052c-.135.2386-.225.508800000000001-.2699.8116C14.7982 12.9192 14.7759 13.2252 14.7759 13.5342v5.5624H12.0372V13.4955C12.0372 13.1994 12.0305 12.9063 12.0181 12.6168 12.005 12.3269 11.9506 12.0598 11.8539 11.815 11.7575 11.5706 11.5967 11.374 11.3717 11.2257 11.1467 11.0782 10.8156 11.0037 10.3785 11.0037 10.2497 11.0037 10.0794 11.0327 9.86746 11.0908 9.65528 11.1487 9.44941 11.2584 9.25027 11.4191 9.05071 11.5802 8.88053 11.812 8.73908 12.1143 8.59754 12.4171 8.5269 12.8128 8.5269 13.3021v5.7945H5.78833V9.11071H8.37266z"/><path d="M26.0246 27.359V.640961h-1.92V0h2.657V28h-2.657V27.359h1.92z"/></g></symbol></svg><header class=l-header><p class="c-title p-title"><a href=/ class=p-title__link>Kyl<span style=color:gray>e</span> R<span style=color:gray>o</span>th</a></p></header><main id=main class=l-main><article class=p-article><header><h1><em><a href=https://www.goodreads.com/book/show/40180025-army-of-none>Army of none: autonomous weapons and the future of war</a></em></h1><div><p>by Paul Scharre</p><div class=c-time>Posted on
<time datetime=2023-02-14T13:33:19-05:00>2023-02-14 at 13:33:19 UTC-0500</time></div><a href=/tags/alignment/ class=c-tag>alignment</a>
<a href=/tags/politics/ class=c-tag>politics</a></div></header><img src=https://images-na.ssl-images-amazon.com/images/S/compressed.photo.goodreads.com/books/1529056546i/40180025.jpg alt=thumbnail class=p-article__thumbnail style=max-height:150px><section id=js-article class=p-article__body><p>The examples in this book make it clear that there is no easy line we can draw between autonomous and non-autonomous weapons (and by extension, autonomous AI agents). There is a smooth gradient of autonomy, which makes the question of allowing autonomous weapons much more nuanced. It&rsquo;s probably the case that higher-level alignment becomes important proportionally to the level of autonomy and intelligence.</p><p>He analyzes the Patriot fratricides,<span class=sidenote-number><small class=sidenote>In a military context, the word <em>fratricide</em> means the killing of someone on the same side of a conflict.</small></span>
and ends up blaming the individuals involved for automation bias. I would say that these humans in the system were set up to fail by the training and the functioning of the system. They&rsquo;re expected to decide whether the computer is right, with only seconds to decide. He acknowledges this later when he talks about Aegis.</p><h2 id=robustness>robustness <a class=header-link href=#robustness><svg class="c-links__icon"><title>permalink</title><use xlink:href="#icon-permalink"/></svg></a></h2><p>It is simply too difficult to account for all possible outcomes of a complex system: robustness is impossible. Charles Perrow has useful things to say about normal accident theory. 3 mile island had 2000 alarms, and usually more than one went off at once. Air France had a plane hit the ocean due to a simple climbing stall, because the pilots didn&rsquo;t understand what all the automatic systems were doing and what all the indicators were telling them.</p><p>There&rsquo;s a tension between normal accident theory and high-reliability organization theory: will accidents always happen eventually, or can we mitigate all risks with enough careful planning and regulation?</p><p><a href=https://en.wikipedia.org/wiki/SUBSAFE>SUBSAFE</a> is another example of an HRO, along with aviation and nuclear. It influenced <a href=https://en.wikipedia.org/wiki/Aegis_Combat_System>Aegis</a>, which seems to be a very well-managed automated weapons system. Aegis operators are kind of like programmers: they intimately understand the &ldquo;doctrines&rdquo; they&rsquo;ve written and tested, which outline the kinds of automation the system will perform. And yet despite all this control they still don&rsquo;t trust the automation except when necessary. This is also helpful in a moral context because the responsibility for mistakes still basically lies with human operators.</p><p>In <a href=https://www.jstor.org/stable/j.ctvzsmf8r><em>The limits of safety: organizations, accidents, and nuclear weapons</em></a>, Scott Sagan claims that HRO ideas can only stave off inevitable mistakes.</p><p><strong>Always/never dilemma</strong>: we have to be always ready to fire, but never accidentally fire. There&rsquo;s no way to drive the risk of accidental firing to zero, according to normal accident theory. What&rsquo;s worse is that war time is inherently not normal.</p><p><strong>The question is not whether the automated system&rsquo;s error rate is lower than the human error rate, because (as implemented currently) when an automated system makes an error, it does not realize it has made a mistake. The damage that it can cause is much, much higher. That has to be taken into account when making the risk calculation, but it&rsquo;s difficult to calculate precisely because the system is complex and we don&rsquo;t know all the ways it can fail.</strong><span class=sidenote-number><small class=sidenote>This idea about relative error rates is something that has entered the public discourse on autonomous driving, and this is an extremely important point that I hope enters the conversation as well.</small></span></p><p>Something like a flash crash could happen in war: a flash war. Drones crossing lines might lead to escalation. Autonomy will lead to unintended escalation, but accidents can&rsquo;t happen nearly as quickly in the physical world as they can in trading. The author says cyber war can happen more at the speed of trading, but I&rsquo;d like to add that this ignores the large human time investment that current hacks require.</p><p>Stuxnet was the first cyber weapon, in that it affected the physical world in a concrete way.</p><h2 id=agi>AGI <a class=header-link href=#agi><svg class="c-links__icon"><title>permalink</title><use xlink:href="#icon-permalink"/></svg></a></h2><p>When people say something is 50 years out, they don&rsquo;t think it&rsquo;s possible. When they say 20 years, they don&rsquo;t know how it will happen but they think it&rsquo;s possible. AGI is coming into reach as we speak.</p><p>The author does a good job covering ideas like the <a href=https://en.wikipedia.org/wiki/Technological_singularity#Intelligence_explosion>AI foom</a>, hard takeoff, and soft takeoff. This book was published in 2018, so it&rsquo;s relatively up to date, but it&rsquo;s missing the meteoric sucess of language models in the last few years.</p><p><strong>&ldquo;AGI is about personhood.&rdquo;</strong> The fact is that while the military isn&rsquo;t looking at AI technology as a way to create real agents in the real world, it&rsquo;s actually the dream behind a lot of the research.<span class=sidenote-number><small class=sidenote>As I&rsquo;m listening to this, I&rsquo;m thinking about the ethics around complex artificial agents. Do we create ethical patients simply by understanding and believing the mechanism that results in the behavior we associate with ethical patients?</small></span>
The Turing test is a sign of our anthropocentric bias. Not a great test.</p><p>An AI trying to optimize human happiness might bury us in coffins with heroine drips. What&rsquo;s worse, sufficiently intelligent systems will develop desires for resources, continued existence, etc., even if we don&rsquo;t explicitly design it with those desires, simply because it wants to accomplish its own goals. Some of the things we&rsquo;re afraid of AGI doing are things we are afraid of powerful, rational, sociopathic people doing.</p><p>&ldquo;Tool AI&rdquo; and AGI may be a false dichotomy. Plus, we keep moving the goalposts for what superintelligence is. What humans do might be totally irrelevant to what superintelligence looks like.</p><p>Automating systems exposes us to greater danger from hacking.</p><p>Can we align AI weapons with IHL guidelines for war? There&rsquo;s already too much ambiguity in modern warfare. Who is an insurgent, who is a plainclothes policeman? Fake surrender is another difficult case.</p><p>With autonomous systems, where does the accountability land? When someone starts an autonomous weapon and it does something that constitutes a war crime, do we blame the operator or the developer?</p><p>The problem with land mines is that their autonomy continues beyond the original bounds of the engagement in time and space.</p><h2 id=ethics>ethics <a class=header-link href=#ethics><svg class="c-links__icon"><title>permalink</title><use xlink:href="#icon-permalink"/></svg></a></h2><p>Consequentialism would say that autonomous weapons are unethical because they will probably increase the harm in the world. That is, unless they make wars more efficient and protect civilians. Autonomous weapons will remove moments of mercy in war. They also will let soldiers become more detached from their killing, which means they will kill more readily. On the other hand, human fighters aren&rsquo;t very ethical. If we create a system that can&rsquo;t break the laws of war, it will do better than we do now.</p><p>From a deontological perspective, one could argue that we ought not to relegate the immoral act of killing to automated systems. The author is not convinced by the argument that those killed deserve the dignity of the action being taken by a human. War is already a horror and there is no solace for loved ones in the fact that it was another person who did the killing. What&rsquo;s more convincing is the wrongness of outsourcing the moral dilemma to automated systems. As nations we already hand off a lot of that moral burden to our soldiers, but imagine if even our soldiers handed off the moral burden of their killings to the systems they turned on. From an ethical perspective, it&rsquo;s good that the Aegis operators use their keys so carefully because then the responsibility gets assigned to the humans who are making the decisions.</p><p>The problem is that war is already inherently a moral dilemma. It&rsquo;s a rational stance to believe that war is always unethical.</p><p>Autonomous weapons might make starting a war easier and ending a war harder. (Starting one might be easier because autonomous systems could inadvertently make acts of escalation, and ending one might be harder because systems we&rsquo;ve initiated may not be able to be recalled.) Autonomous systems are rigid, which is good when we can get them to follow the rules, but bad for reevaluating judgements in new contexts.</p><h2 id=strategic-stability>strategic stability <a class=header-link href=#strategic-stability><svg class="c-links__icon"><title>permalink</title><use xlink:href="#icon-permalink"/></svg></a></h2><p>Different technologies can be stabilizing or destabilizing. Submarines with nuclear weapons are seen as stabilizing because they eliminate nuclear first strike advantage, while missile defense systems are seen as destabilizing because they enhance first strike advantage. Are autonomous weapons a stabilizing or destabilizing technology? It&rsquo;s hard to say, because autonomy takes so many forms.</p><p>In automation, there&rsquo;s a haste that&rsquo;s added that is problematic when humans are out of the loop. Humans ought to be in the loop to override autonomy in cases where the context implies a different action is best. Humans have a projection bias when predicting their own future preferences for novel situations. What happens in a novel situation if we have encoded our decisions beforehand in the code that automates our weapons?</p><p>There&rsquo;s an argument that too much stability is bad, because it will allow for other lesser forms of violence. The <a href=https://en.wikipedia.org/wiki/Madman_theory>madman theory</a> says we might want to imply that there is some instability, in order to convince the enemy that the weapons could really be used.</p><p>Combined human-machine systems can use the benefits of both. Humans can be the moral agency, failsafes, etc. The only problem is the limitations of speed when humans are in the loop or on the loop.</p><h2 id=bans>bans <a class=header-link href=#bans><svg class="c-links__icon"><title>permalink</title><use xlink:href="#icon-permalink"/></svg></a></h2><p>Bans of certain kinds of weapons are difficult to come by, and depend on factors like the weapon&rsquo;s stability effects, usefulness, and perceived horribleness. This movement toward an automated weapons ban is different because it&rsquo;s led by NGOs, not states. All the states that have signed on so far are small ones without the ability to win an automation arms race. But among those participating in the conversation, there appears to be an early consensus that humans ought to always be involved. But this particular area is hard because it&rsquo;s so difficult to define the set of things that are &ldquo;automated systems&rdquo;.</p><p>Things that make a ban successful:</p><ol><li>An identifiable focal point</li><li>Perceived horribleness</li><li>Transparency: the ability to verify</li></ol><p>Types of possible automation bans:</p><ul><li>regulate the type of autonomy: hard because it&rsquo;s all software, hard to judge<ul><li>Maybe just non-recoverable machines can be automated? That way there&rsquo;s a cost to deployment.</li></ul></li><li>anti-personnel autonomous weapons: low military usefulness, high horror</li><li>non-legally binding code of conduct to establish rules of the road</li><li>establishing a general principle about the role of human judgement in war<ul><li>E.g. stating that a human must be responsible for the IHL rules, meaning that autonomous systems would have to remain limited in scope.</li></ul></li></ul><p>The good news is that human societies already show natural restraint in war. Maybe we&rsquo;ll figure it out.</p></section><footer><nav class="p-pagination c-pagination"><div class=c-pagination__ctrl><div class=c-pagination__newer><a href=/book/think-again/>Newer</a></div><div class=c-pagination__older><a href=/book/conscious/>Older</a></div></div></nav><section class=p-related><h3>See Also</h3><ul id=slider class=p-related__list><li class="p-related__item js-related__item"><a href=/post/nyt-ai-explainer/ style=background-image:url(/post/nyt-ai-explainer/surveillance.png)><span>the NYT AI explainer misses the point</span></a></li><li class="p-related__item js-related__item"><a href=/post/edward-frenkel/ style=background-image:url(/post/edward-frenkel/billiards.png)><span>the inherent subjectivity of reality</span></a></li><li class="p-related__item js-related__item"><a href=/book/winners-take-all/ style=background-image:url(https://images-na.ssl-images-amazon.com/images/S/compressed.photo.goodreads.com/books/1528750369i/37506348.jpg)><span>Winners take all: the elite charade of changing the world</span></a></li><li class="p-related__item js-related__item"><a href=/post/bengio-crawford/ style=background-image:url(/post/bengio-crawford/poster.jpg)><span>for a socially beneficial and responsible development of AI</span></a></li><li class="p-related__item js-related__item"><a href=/paper/ai-values-alignment/ style=background-image:url(/paper/ai-values-alignment/trolley_abstract.png)><span>Artificial intelligence, values, and alignment</span></a></li><li class="p-related__item js-related__item"><a href=/paper/unsolved-problems-ml-safety/ style=background-image:url(/paper/unsolved-problems-ml-safety/icons.png)><span>Unsolved problems in ML safety</span></a></li></ul></section></footer></article></main><nav class="l-nav p-menu"><ul class=p-menu__lists><li class=p-menu__listitem><a href=/>homepage</a></li><li class=p-menu__listitem><a href=/post/>posts</a></li><li class=p-menu__listitem><a href=/paper/>paper notes</a></li><li class=p-menu__listitem><a href=/book/>book notes</a></li></ul></nav><footer class=l-footer><ul class=c-links><li class=c-links__item><a href=https://matrix.to/#/%40kyle%3akylrth.com target=_blank><svg viewBox="0 0 30 28" class="c-links__icon"><title>matrix</title><use xlink:href="#icon-matrix"/></svg></a></li><li class=c-links__item><a href=https://github.com/kylrth target=_blank><svg viewBox="0 0 64 64" class="c-links__icon"><title>github</title><use xlink:href="#icon-github"/></svg></a></li><li class=c-links__item><a href=https://www.linkedin.com/in/kyle-roth/ target=_blank><svg viewBox="0 0 64 64" class="c-links__icon"><title>linkedin</title><use xlink:href="#icon-linkedin"/></svg></a></li></ul><p class=p-copyright><a href=https://github.com/kylrth/kylrth.github.io/commits/f0fc5d68a9295f1e0368782ed40bd989cb10db00/content/book/army-of-none.md>page
history</a></p></footer></body></html>