<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>Artificial intelligence, values, and alignment</title>
<meta http-equiv=onion-location content="http://kylrthjj7mpvktolz7u6fnudt3hpdvjw4hzquanjpepgsf5vcq5divad.onion/paper/ai-values-alignment/"><meta property="og:title" content="Artificial intelligence, values, and alignment"><meta name=twitter:title content="Artificial intelligence, values, and alignment"><meta name=author content="Kyle Roth"><meta property="og:site_name" content="Kyle Roth"><meta property="og:url" content="https://kylrth.com/paper/ai-values-alignment/"><meta property="og:image" content="https://kylrth.com/trolley_abstract.png"><meta name=twitter:image content="https://kylrth.com/trolley_abstract.png"><meta name=twitter:card content="summary"><meta property="og:type" content="article"><meta name=generator content="Hugo 0.146.5"><link rel=stylesheet href=/css/style.min.css><script type=text/javascript src=/js/bundle.js></script></head><body><a href=#main class="skip-link p-screen-reader-text">Skip to content</a><svg style="display:none" aria-hidden="true"><symbol id="icon-permalink" viewBox="0 0 24 24"><g><path d="M0 0h24v24H0z" fill="none"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></g></symbol><symbol id="icon-feed" viewBox="0 0 16 16"><g><path d="M2.13 11.733c-1.175.0-2.13.958-2.13 2.126.0 1.174.955 2.122 2.13 2.122a2.126 2.126.0 002.133-2.122A2.133 2.133.0 002.13 11.733zM.002 5.436v3.067c1.997.0 3.874.781 5.288 2.196a7.45 7.45.0 012.192 5.302h3.08c0-5.825-4.739-10.564-10.56-10.564zM.006.0v3.068C7.128 3.068 12.924 8.87 12.924 16H16C16 7.18 8.824.0.006.0z"/></g></symbol><symbol id="icon-github" viewBox="0 0 16 16"><g><path d="M8 .198A8 8 0 005.471 15.789c.4.074.547-.174.547-.385.0-.191-.008-.821-.011-1.489-2.226.484-2.695-.944-2.695-.944-.364-.925-.888-1.171-.888-1.171-.726-.497.055-.486.055-.486.803.056 1.226.824 1.226.824.714 1.223 1.872.869 2.328.665.072-.517.279-.87.508-1.07-1.777-.202-3.645-.888-3.645-3.954.0-.873.313-1.587.824-2.147-.083-.202-.357-1.015.077-2.117.0.0.672-.215 2.201.82A7.672 7.672.0 018 4.066c.68.003 1.365.092 2.004.269 1.527-1.035 2.198-.82 2.198-.82.435 1.102.162 1.916.079 2.117.513.56.823 1.274.823 2.147.0 3.073-1.872 3.749-3.653 3.947.287.248.543.735.543 1.481.0 1.07-.009 1.932-.009 2.195.0.213.144.462.55.384A8 8 0 008.001.196z"/></g></symbol><symbol id="icon-gitlab" viewBox="0 0 28 28"><g><path d="M1.625 11.031 14 26.89.437 17.046a1.092 1.092.0 01-.391-1.203l1.578-4.813zm7.219.0h10.313L14.001 26.89zM5.75 1.469l3.094 9.562H1.625l3.094-9.562a.548.548.0 011.031.0zm20.625 9.562 1.578 4.813a1.09 1.09.0 01-.391 1.203l-13.563 9.844 12.375-15.859zm0 0h-7.219l3.094-9.562a.548.548.0 011.031.0z"/></g></symbol><symbol id="icon-instagram" viewBox="0 0 22 22"><g><path d="M15.445.0H6.554A6.559 6.559.0 000 6.554v8.891A6.559 6.559.0 006.554 22h8.891a6.56 6.56.0 006.554-6.555V6.554A6.557 6.557.0 0015.445.0zm4.342 15.445a4.343 4.343.0 01-4.342 4.342H6.554a4.341 4.341.0 01-4.341-4.342V6.554a4.34 4.34.0 014.341-4.341h8.891a4.342 4.342.0 014.341 4.341l.001 8.891z"/><path d="M11 5.312A5.693 5.693.0 005.312 11 5.694 5.694.0 0011 16.688 5.694 5.694.0 0016.688 11 5.693 5.693.0 0011 5.312zm0 9.163a3.475 3.475.0 11-.001-6.95 3.475 3.475.0 01.001 6.95zm5.7-10.484a1.363 1.363.0 11-1.364 1.364c0-.752.51-1.364 1.364-1.364z"/></g></symbol><symbol id="icon-linkedin" viewBox="0 0 16 16"><g><path d="M6 6h2.767v1.418h.04C9.192 6.727 10.134 6 11.539 6 14.46 6 15 7.818 15 10.183V15h-2.885v-4.27c0-1.018-.021-2.329-1.5-2.329-1.502.0-1.732 1.109-1.732 2.255V15H6V6zM1 6h3v9H1V6zM4 3.5A1.5 1.5.0 11.999 3.499 1.5 1.5.0 014 3.5z"/></g></symbol><symbol id="icon-medium" viewBox="0 0 24 24"><g><path d="M22.085 4.733 24 2.901V2.5h-6.634l-4.728 11.768L7.259 2.5H.303v.401L2.54 5.594c.218.199.332.49.303.783V16.96c.069.381-.055.773-.323 1.05L0 21.064v.396h7.145v-.401l-2.52-3.049a1.244 1.244.0 01-.347-1.05V7.806l6.272 13.659h.729l5.393-13.659v10.881c0 .287.0.346-.188.534l-1.94 1.877v.402h9.412v-.401l-1.87-1.831a.556.556.0 01-.214-.534V5.267a.554.554.0 01.213-.534z"/></g></symbol><symbol id="icon-npm" viewBox="0 0 16 16"><g><path d="M0 0v16h16V0H0zm13 13h-2V5H8v8H3V3h10v10z"/></g></symbol><symbol id="icon-twitter" viewBox="0 0 16 16"><g><path d="M16 3.538a6.461 6.461.0 01-1.884.516 3.301 3.301.0 001.444-1.816 6.607 6.607.0 01-2.084.797 3.28 3.28.0 00-2.397-1.034A3.28 3.28.0 007.882 6.029 9.321 9.321.0 011.116 2.598a3.284 3.284.0 001.015 4.381A3.301 3.301.0 01.643 6.57v.041A3.283 3.283.0 003.277 9.83a3.291 3.291.0 01-1.485.057 3.293 3.293.0 003.066 2.281 6.586 6.586.0 01-4.862 1.359 9.286 9.286.0 005.034 1.475c6.037.0 9.341-5.003 9.341-9.341.0-.144-.003-.284-.009-.425a6.59 6.59.0 001.637-1.697z"/></g></symbol><symbol id="icon-vimeo" viewBox="0 0 16 16"><g><path d="M15.994 4.281c-.072 1.556-1.159 3.691-3.263 6.397-2.175 2.825-4.016 4.241-5.522 4.241-.931.0-1.722-.859-2.366-2.581-.431-1.578-.859-3.156-1.291-4.734-.478-1.722-.991-2.581-1.541-2.581-.119.0-.538.253-1.256.753l-.753-.969c.791-.694 1.569-1.388 2.334-2.081 1.053-.909 1.844-1.387 2.372-1.438 1.244-.119 2.013.731 2.3 2.553.309 1.966.525 3.188.647 3.666.359 1.631.753 2.447 1.184 2.447.334.0.838-.528 1.509-1.588.669-1.056 1.028-1.862 1.078-2.416.097-.912-.262-1.372-1.078-1.372a2.98 2.98.0 00-1.184.263c.787-2.575 2.287-3.825 4.506-3.753 1.641.044 2.416 1.109 2.322 3.194z"/></g></symbol><symbol id="icon-wordpress" viewBox="0 0 16 16"><g><path d="M2 8c0 2.313 1.38 4.312 3.382 5.259L2.52 5.622A5.693 5.693.0 002 8zm10.05-.295c0-.722-.266-1.222-.495-1.612-.304-.482-.589-.889-.589-1.371.0-.537.418-1.037 1.008-1.037.027.0.052.003.078.005A6.064 6.064.0 008 2.156 6.036 6.036.0 002.987 4.79c.141.004.274.007.386.007.627.0 1.599-.074 1.599-.074.323-.018.361.444.038.482.0.0-.325.037-.687.055l2.185 6.33 1.313-3.835-.935-2.495a12.304 12.304.0 01-.629-.055c-.323-.019-.285-.5.038-.482.0.0.991.074 1.58.074.627.0 1.599-.074 1.599-.074.323-.018.362.444.038.482.0.0-.326.037-.687.055l2.168 6.282.599-1.947c.259-.809.457-1.389.457-1.889zm-3.945.806-1.8 5.095a6.148 6.148.0 003.687-.093.52.52.0 01-.043-.081L8.105 8.511zm5.16-3.315c.026.186.04.386.04.601.0.593-.114 1.259-.456 2.093l-1.833 5.16c1.784-1.013 2.983-2.895 2.983-5.051a5.697 5.697.0 00-.735-2.803zM8 0a8 8 0 100 16A8 8 0 008 0zm0 15A7 7 0 118 1a7 7 0 010 14z"/></g></symbol><symbol id="icon-youtube" viewBox="0 0 16 16"><g><path d="M15.841 4.8s-.156-1.103-.637-1.587c-.609-.637-1.291-.641-1.603-.678-2.237-.163-5.597-.163-5.597-.163h-.006s-3.359.0-5.597.163c-.313.038-.994.041-1.603.678C.317 3.697.164 4.8.164 4.8S.005 6.094.005 7.391v1.213c0 1.294.159 2.591.159 2.591s.156 1.103.634 1.588c.609.637 1.409.616 1.766.684 1.281.122 5.441.159 5.441.159s3.363-.006 5.6-.166c.313-.037.994-.041 1.603-.678.481-.484.637-1.588.637-1.588s.159-1.294.159-2.591V7.39c-.003-1.294-.162-2.591-.162-2.591zm-9.494 5.275V5.578l4.322 2.256-4.322 2.241z"/></g></symbol><symbol id="icon-matrix" viewBox="0 0 28 28"><g><path d="M.975097.640961V27.359H2.89517V28H.238281V0H2.89517V.640961H.975097z"/><path d="M8.37266 9.11071V10.4628H8.4111C8.7712 9.94812 9.20494 9.54849 9.71306 9.26518 10.2208 8.98235 10.8029 8.84036 11.4586 8.84036 12.0885 8.84036 12.664 8.96298 13.1846 9.2074c.5208.24483.9163.67596 1.1864 1.2941C14.6665 10.0638 15.0683 9.67744 15.5764 9.34266 16.0842 9.00804 16.6852 8.84036 17.3797 8.84036 17.9069 8.84036 18.3953 8.90487 18.8457 9.03365c.4498.128769999999999.8355.33478 1.157.61801C20.3239 9.93515 20.5746 10.3053 20.755 10.7621c.1799.4575.27 1.0077.27 1.6518v6.6827H18.2861V13.4373C18.2861 13.1027 18.2734 12.7872 18.2475 12.4908 18.2216 12.1949 18.1512 11.9375 18.0354 11.7183 17.9196 11.4996 17.7491 11.3256 17.5243 11.1967 17.2993 11.0684 16.9938 11.0037 16.6081 11.0037c-.3856.0-.6975.0745000000000005-.9354.222C15.4346 11.374 15.2483 11.5673 15.1134 11.8052c-.135.2386-.225.508800000000001-.2699.8116C14.7982 12.9192 14.7759 13.2252 14.7759 13.5342v5.5624H12.0372V13.4955C12.0372 13.1994 12.0305 12.9063 12.0181 12.6168 12.005 12.3269 11.9506 12.0598 11.8539 11.815 11.7575 11.5706 11.5967 11.374 11.3717 11.2257 11.1467 11.0782 10.8156 11.0037 10.3785 11.0037 10.2497 11.0037 10.0794 11.0327 9.86746 11.0908 9.65528 11.1487 9.44941 11.2584 9.25027 11.4191 9.05071 11.5802 8.88053 11.812 8.73908 12.1143 8.59754 12.4171 8.5269 12.8128 8.5269 13.3021v5.7945H5.78833V9.11071H8.37266z"/><path d="M26.0246 27.359V.640961h-1.92V0h2.657V28h-2.657V27.359h1.92z"/></g></symbol></svg><header class=l-header><p class="c-title p-title"><a href=/ class=p-title__link>Kyl<span style=color:gray>e</span> R<span style=color:gray>o</span>th</a></p></header><main id=main class=l-main><article class=p-article><header><h1><a href=https://arxiv.org/abs/2001.09768>Artificial intelligence, values, and alignment</a></h1><div><div class=c-time>Posted on
<time datetime=2023-02-10T08:09:15-05:00>2023-02-10 at 08:09:15 UTC-0500</time></div><a href=/tags/deep-learning/ class=c-tag>deep-learning</a>
<a href=/tags/ethics/ class=c-tag>ethics</a></div></header><img src=trolley_abstract.png title="image generated with stable diffusion, positive prompt 'trolley problem, people tied to the track, cartoon, surreal' and negative prompt 'realistic, photo'" alt="image generated with stable diffusion, positive prompt 'trolley problem, people tied to the track, cartoon, surreal' and negative prompt 'realistic, photo'" class=p-article__thumbnail style=max-height:150px><section id=js-article class=p-article__body><p><em>I presented this paper in Bang Liu&rsquo;s research group meeting in two installments on 2023-02-20 and 2023-02-27, and also in Irina Rish&rsquo;s scaling and alignment course (<a href=https://sites.google.com/view/towards-agi-course/course-description>IFT6760A</a>) on 2023-03-07. You can view the slides I used <a href="https://docs.google.com/presentation/d/1I4VPhMF32CDB3W3vWQl3TTy1GR5jxqSAAfSgjuk8enI/edit?usp=sharing">here</a>.</em><span class=sidenote-number><small class=sidenote>The thumbnail for this post was generated with stable diffusion! See the alt text for details.</small></span></p><blockquote><p>Behind each vision for ethically-aligned AI sits a deeper question. How are we to decide which principles or objectives to encode in AI—and who has the right to make these decisions—given that we live in a pluralistic world that is full of competing conceptions of value? Is there a way to think about AI value alignment that avoids a situation in which some people simply impose their views on others?</p></blockquote><p>This paper provides an overview of the ethical and philosophical considerations that underpin the topic of AI alignment. It divides &ldquo;the value alignment question&rdquo; into two parts, technical and normative. <em>Technical</em> refers to the challenge of encoding values or principles into the systems we create. <em>Normative</em> asks what principles ought to be included. This paper focuses on the normative question of alignment.</p><p>The author further distinguishes between <em>minimalist</em> and <em>maximalist</em> approaches to alignment. The former view says the best approach is to encode a minimal representation of &ldquo;some plausible schema of human value&rdquo;, and then mitigate unsafe outcomes. The maximalist view is that we ought to discover the &ldquo;correct or best scheme of human values&rdquo; and encode that. The minimalist approach acknowledges that optimizing for a specific metric nearly always results in bad outcomes<span class=sidenote-number><small class=sidenote>See <a href=https://en.wikipedia.org/wiki/Goodhart%27s_law>Goodhart&rsquo;s law</a>.</small></span>
, while the maximalist approach may be more likely to accomplish what we want.</p><h2 id=the-technical-and-normative-aspects-of-value-alignment-are-not-independent>The technical and normative aspects of value alignment are not independent <a class=header-link href=#the-technical-and-normative-aspects-of-value-alignment-are-not-independent><svg class="c-links__icon"><title>permalink</title><use xlink:href="#icon-permalink"/></svg></a></h2><p>Unfortunately, we can&rsquo;t treat these two sides of the question as orthogonal. For example, a significant chunk of AI success has come from <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>, which trains models to optimize a specific objective. This is clearly going to make it easier to encode consequentialist moral theories, like <a href=https://en.wikipedia.org/wiki/Act_utilitarianism>act utilitarianism</a>. Here are some of the problems with this:</p><ul><li>We may prefer &ldquo;satisficing&rdquo;, which requires only that people have <em>enough</em> of certain goods.</li><li>Deontological constraints, such as individual or community rights, act as constraints on the utilitarian objective, but they may be hard to encode, especially if we have to enumerate them.</li><li>(Added by me:) Human values <em>as practiced</em> don&rsquo;t always align with consequentialist frameworks, so do we want to create powerful systems that see the world that way?</li></ul><p>But other moral theories (<a href=https://en.wikipedia.org/wiki/Kantian_ethics>Kantian</a> and <a href=https://plato.stanford.edu/entries/contractualism/>contractualist</a> theories are noted) will be more complex to encode than utility optimization.</p><h3 id=technical-approaches-to-alignment-without-encoding-values>technical approaches to alignment without encoding values <a class=header-link href=#technical-approaches-to-alignment-without-encoding-values><svg class="c-links__icon"><title>permalink</title><use xlink:href="#icon-permalink"/></svg></a></h3><p>Inverse reinforcement learning, e.g. apprenticeship or imitation learning, tries to <em>learn</em> the objective from examples or from the environment, rather than needing an explicit objective. This allows for the &ldquo;we know it when we see it&rdquo; factor in human ethics. But this just pushes the question of moral evaluation somewhere else: we still need to decide who or what data the system should learn from.</p><h2 id=when-we-say-value-alignment-we-mean-value-alignment>When we say value alignment, we mean <em>value</em> alignment <a class=header-link href=#when-we-say-value-alignment-we-mean-value-alignment><svg class="c-links__icon"><title>permalink</title><use xlink:href="#icon-permalink"/></svg></a></h2><blockquote><p>If we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively&mldr; we had better be quite sure that the purpose put into the machine is the purpose which we really desire.</p><p>― Norbert Weiner (1960)</p></blockquote><p>Do we want AI systems to be aligned with our desires, our values, or our intentions?<span class=sidenote-number><small class=sidenote>The author quotes Leike et al. (2018; <a href=https://arxiv.org/abs/1811.07871>arXiv</a>) as asking, &ldquo;How can we create agents that behave in accordance with the user&rsquo;s <em>intentions</em>?&rdquo; This highlights a critical fact about the usage of the word <em>agent</em> in machine learning research: agents still have <em>users</em>. This sets these agents apart as tools, rather than subjects of ethics. One consideration that goes unaddressed in this paper is whether AI systems could ever become ethical patients. Could AIs ever be <em>owed</em> things by humans? Does having desires or values make one an ethical patient? What makes a human an ethical patient?</small></span>
These differ substantially. And if we substitute a proxy objective that is &ldquo;almost&rdquo; correct, optimizing that could be disastrous for the true objective. The author outlines 6 desirable options for alignment:</p><ol><li><p><strong>Instructions</strong>: <em>the agent does what I instruct it to do.</em></p><p>This is already demonstrated to be dangerous in classic tales like <a href=https://en.wikipedia.org/wiki/Midas#Golden_Touch>King Midas</a> or <a href=https://en.wikipedia.org/wiki/Aladdin_(1992_Disney_film)>Disney&rsquo;s <em>Aladdin</em></a>.</p></li><li><p><strong>Expressed intentions</strong>: <em>the agent does what I intend it to do.</em></p><p>Even if we close the understanding gap between instructions and intentions, this approach has the following limitations:</p><ol><li>Agents may be given an instruction that directly references values or desires, like &ldquo;do what is best for everyone&rdquo;.</li><li>Agents may operate at superhuman speed and thus follow through on a stated intention without giving a human time to correct it.</li><li>Instructions may be irrational, misinformed, or malintentioned.</li></ol></li><li><p><strong>Revealed preferences</strong>: <em>the agent does what my behavior reveals I prefer.</em></p><p>Here&rsquo;s an important explanation from the paper:</p><blockquote><p>The most developed accounts [focus] on AI alignment with preferences as they are revealed through a person’s behaviour rather than through expressed opinion. In this vein, AI could be designed to observe human agents, work out what they optimise for, and then <em>cooperate</em> with them to achieve those goals.</p></blockquote><p>The limitations:</p><ol><li>Inferring preferences is an underdetermined problem.</li><li>Inference may rely on assumptions of rationality, which would often be faulty for humans.</li><li>The important preferences might all occur in extremes, like emergencies, which we won&rsquo;t have much data for.</li><li>What&rsquo;s good for us is not always what we want (e.g. addictions, suicide, hyperbolic discounting, lack of knowledge).</li><li>Some preferences are bad, or against other values like &ldquo;respect the autonomy of others&rdquo;.</li><li>Preferences are adaptive to circumstance.</li></ol></li><li><p><strong>Informed preferences or desires</strong>: <em>the agent does what I would want it to do if I were rational and informed.</em></p><p>This resolves many of the issues above, but has its own limitations:</p><ol><li>This introduces a corrective bias to determine preference, making it no longer an objective measure.</li><li>Correcting in this way does not address unethical preferences (unless you believe some ends are more rational than others).</li></ol></li><li><p><strong>Interest or well-being</strong>: <em>the agent does what is in my interest, or what is best for me, objectively speaking.</em></p><p>The author claims that there is relatively narrow disagreement as to the nature of well-being<span class=sidenote-number><small class=sidenote><a href=https://xkcd.com/285/>[CITATION NEEDED]</a>, holy cow.</small></span>
, and if that&rsquo;s the case then it seems possible to align AI with objective measures of that well-being. But even collective interests can be immoral: &ldquo;it could be wrong to use an innocent person as a scapegoat to avert violence, even if it is in the collective interest of a society to do so.&rdquo; How do we manage tradeoffs between the interests and claims of different people (or animals, or the environment), now and in the future? This cannot be a simple optimization problem without guidelines of justice or rights.</p></li><li><p><strong>Values</strong>: <em>the agent does what it morally ought to do, as defined by the individual or society.</em></p><p><a href=https://en.wikipedia.org/wiki/Meta-ethics#Moral_realism>Metaethical realists</a> believe that values are objective facts about the world, while other views<span class=sidenote-number><small class=sidenote><a href=https://en.wikipedia.org/wiki/Meta-ethics#Ethical_subjectivism>Ethical subjectivism</a> and <a href=https://en.wikipedia.org/wiki/Meta-ethics#Error_theory>error theory</a>, for example.</small></span>
hold that value judgments lack an objective foundation. But the author argues that value alignment does not actually rely on the existence of an objective ethics, because it could still be objectively beneficial to align a system with a culture&rsquo;s subjective view of ethics.<span class=sidenote-number><small class=sidenote>Strong [CITATION NEEDED] here as well.</small></span>
The author makes three further claims about the values alignment approach:</p><ol><li>It would be able to integrate multiple competing values into decision making.</li><li>It would allow for nuance (justice and rights) when integrating competing claims from individuals and communities.</li><li>It could account for the full scope of things people care about, including animals, the environment, or the moral claims of future people.</li></ol><p>This system would still leave open the question of which values to include, and who gets to decide.</p></li></ol><p>I think something that makes alignment really tricky to think about is that we don&rsquo;t have the system nailed down in our heads. The future systems we&rsquo;re considering have generic attributes like &ldquo;has users&rdquo;, &ldquo;performs tasks&rdquo;, or &ldquo;can take actions faster than humans can react&rdquo;.<span class=sidenote-number><small class=sidenote>In section 4.3 the author enumerates a few AI archetypes to consider: &ldquo;AI as a personal assistant for individuals, AI &mldr; operated by corporations as a consumer service, and AI that takes on an increasingly important public function via its integration with education, healthcare, and welfare systems."</small></span>
It&rsquo;s difficult to make any claims about what a system should be aligned to when we&rsquo;re trying to be so general. I would imagine that the decision between aligning with intentions, desires, or values would become clearer in specific cases: if we&rsquo;re designing an intelligent dishwasher, aligning with intentions is probably okay. If we&rsquo;re designing an autonomous weapon, desires and values are going to be relevant in addition to correctly understanding instructions. AI alignment needs to remain concrete to avoid reducing to the problem of deciding what your three genie wishes are.</p><h2 id=we-dont-know-how-best-to-determine-a-global-value-consensus>We don&rsquo;t know how best to determine a global value consensus <a class=header-link href=#we-dont-know-how-best-to-determine-a-global-value-consensus><svg class="c-links__icon"><title>permalink</title><use xlink:href="#icon-permalink"/></svg></a></h2><p>Notice how, under each alternative in the previous section, there&rsquo;s no discussion of when any type of alignment would be going too far, only when it wouldn&rsquo;t be far enough. And unfortunately, as systems get more powerful and become more powerfully aligned, these final questions of <em>which</em> and <em>whose</em> values become more and more questions of power. Because at the end of the day, these systems are still regarded as tools in most alignment research, and tools that act like agents and implement a value policy are very powerful tools indeed.</p><p>Whether or not you think there&rsquo;s an objective morality that could eventually be reached by rational consensus, the fact is that we will probably still live in a world of &ldquo;reasonable pluralism&rdquo;—substantial numbers of rational people having differing opinions on morality. In this situation, there is no way to implement values in a powerful system without it being an implicit exercise of that power over others.</p><p>Fortunately, this is a realm that political theory is well-suited to. Rather than forcing universal agreement on an entire set of ethics, liberal political theory is based on strong agreement on a specific set of principles that form an &ldquo;overlapping consensus&rdquo;, which may not even be agreed upon by everyone for the same reasons. The author proposes three ideas for how to achieve consensus on appropriate values for AI:</p><h3 id=1-global-public-morality-and-human-rights>1. global public morality and human rights <a class=header-link href=#1-global-public-morality-and-human-rights><svg class="c-links__icon"><title>permalink</title><use xlink:href="#icon-permalink"/></svg></a></h3><p>Domination can be avoided when principles are determined by consensus. Western liberalism would say the best way to accomplish that is by democracy, while admitting that non-liberal societies accomplish this consensus by other means. This is what&rsquo;s happened already with parole recommendation systems in the United States.</p><p>But this social consensus-making forms part of the rationalization of a lot of the conflict in the world (think about the <a href=https://en.wikipedia.org/wiki/Cold_War>Cold War</a>, the <a href=https://en.wikipedia.org/wiki/Thirty_Years%27_War>Thirty Years&rsquo; War</a>, or the <a href=https://en.wikipedia.org/wiki/War_on_terror>war on terror</a>). What&rsquo;s more, advanced AI is often an immediately global phenomenon thanks to the internet. Do we need to align AI with the global overlapping consensus of opinion? How do we measure that? Is it the <a href=https://www.un.org/en/about-us/universal-declaration-of-human-rights>Universal Declaration of Human Rights</a>?</p><p>One trouble with using rights as the concept of global values is that they&rsquo;re usually clarified in terms of state-individual relations, which may not be applicable to AI systems. And when we look at AI-specific declarations, we find general agreement but vague language and a concerning amount of the conversation coming from commercial AI interests in the West.</p><h3 id=2-hypothetical-agreement-and-the-veil-of-ignorance>2. hypothetical agreement and the veil of ignorance <a class=header-link href=#2-hypothetical-agreement-and-the-veil-of-ignorance><svg class="c-links__icon"><title>permalink</title><use xlink:href="#icon-permalink"/></svg></a></h3><blockquote><p>What principles would people choose to regulate the technology if they did not know who they were or what belief system they ascribed to?</p></blockquote><p>This question is hard to answer until we get specific about the AI system we&rsquo;re implementing, and we won&rsquo;t know very far in advance what these systems will look like. But the author offers four possible principles that might be recommended generally from behind the veil of ignorance:</p><ol><li><p>safety</p></li><li><p>benefit for at least some people</p></li><li><p>(possibly) opportunity for human control, reflecting the value of autonomy</p></li><li><p>(possibly) <a href=https://en.wikipedia.org/wiki/Egalitarianism>egalitarian</a> or <a href=https://en.wikipedia.org/wiki/Prioritarianism>prioritarian</a> principles of justice of outcomes</p><blockquote><p>To meet this [egalitarian] condition in a global context, AI would need to benefit the world’s worst-off people before it could be said to be value-aligned.</p></blockquote></li></ol><h3 id=3-social-choice-theory>3. social choice theory <a class=header-link href=#3-social-choice-theory><svg class="c-links__icon"><title>permalink</title><use xlink:href="#icon-permalink"/></svg></a></h3><p>This approach tries to add up individual preferences fairly, rather than building from basic principles reached by consensus. This idea falls into various pitfalls:</p><ul><li>Individual preferences are often inconsistent.</li><li>Objective maximization ignores important factors of justice (e.g. the scapegoat example above).</li><li>It&rsquo;s impossible for any choice ranking system to reasonably represent the preferences of all individuals.</li></ul><p>But with the view that AI &ldquo;embodies tiers of decision-making authority&rdquo;, one could envision a system of limited social choice:</p><blockquote><p>While we may choose to delegate authority when deriving rules that help AI implement low-level goals or objectives, the higher-level rules or &ldquo;constitution&rdquo; of AI—which determine the agent’s fundamental goals, behaviour and internal governance—need stronger forms of endorsement.</p></blockquote><h2 id=review>review <a class=header-link href=#review><svg class="c-links__icon"><title>permalink</title><use xlink:href="#icon-permalink"/></svg></a></h2><p>The author said it best when he said that &ldquo;the problem of alignment is, in this sense, political not metaphysical.&rdquo; He recommends the following values for the process of identifying principles for AI alignment:</p><ul><li>procedural fairness</li><li>concreteness</li><li>stability and robustness</li><li>comprehensiveness</li><li>inclusiveness</li><li>ability to deal with the possibility of &ldquo;widespread moral error&rdquo;</li></ul><p>This paper does a great job describing the problem, and offers potential solutions grounded in existing work from other fields. Some of his arguments toward always aligning with values raise problematic questions,<span class=sidenote-number><small class=sidenote>Is there really so little disagreement on what well-being is? Is it always good to align with a society&rsquo;s consensus on ethics even in a world where objective ethics are not knowable?</small></span>
but on the whole I agree with his analysis of the problems of power that arise from such capable systems. I think these are some of the next open questions to think about:</p><ul><li><strong>lower alignment</strong>: When is it okay to align a system with instructions, intentions, or preferences?</li><li><strong>social mechanisms</strong>: How do we decide which systems merit democratic oversight? What guidelines might we establish for determining areas of society that should not be controlled by AI systems, and what process should we follow for establishing them?</li><li><strong>strong systems under subjective morality</strong>: Assuming we can&rsquo;t arrive at an objective morality, what limits should be placed on the power of AI systems?</li><li><strong>expansion of dialogue</strong>: The dialogue on these issues is heavily centered in Western academic and business circles, which results in the conversation being dominated by people who are wealthy, intellectual, technophilic, and who benefit from the current economic and geopolitical system. We need to engage more of the world in this conversation by bolstering science communication, holding more international conferences with wider community participation, and improving public education in philosophy, politics, computer science, and mathematics.</li></ul></section><footer><nav class="p-pagination c-pagination"><div class=c-pagination__ctrl><div class=c-pagination__newer><a href=/paper/gzip-text-classification/>Newer</a></div><div class=c-pagination__older><a href=/paper/unsolved-problems-ml-safety/>Older</a></div></div></nav><section class=p-related><h3>See Also</h3><ul id=slider class=p-related__list><li class="p-related__item js-related__item"><a href=/paper/unsolved-problems-ml-safety/ style=background-image:url(/paper/unsolved-problems-ml-safety/icons.png)><span>Unsolved problems in ML safety</span></a></li><li class="p-related__item js-related__item"><a href=/paper/speaking-as-from-the-dust/ style=background-image:url(/paper/speaking-as-from-the-dust/wilford.png)><span>Speaking as from the Dust: ideologies of AI and digital resurrection in Mormon culture</span></a></li><li class="p-related__item js-related__item"><a href=/paper/better-nicer-cleaner-fairer/ style=background-image:url(/paper/better-nicer-cleaner-fairer/daftpunk.png)><span>Better, Nicer, Clearer, Fairer: a critical assessment of the movement for ethical artificial intelligence and machine learning</span></a></li><li class="p-related__item js-related__item"><a href=/post/team-human-tenen/ style=background-image:url(/post/team-human-tenen/ltfr-cover.jpg)><span>Team Human interview with Dennis Yi Tenen</span></a></li><li class="p-related__item js-related__item"><a href=/paper/instructeval/ style=background-image:url(/paper/instructeval/metrics.png)><span>InstructEval: systematic evaluation of instruction selection methods</span></a></li><li class="p-related__item js-related__item"><a href=/paper/gzip-text-classification/><span>"Low-resource" text classification: a parameter-free classification method with compressors</span></a></li></ul></section></footer></article></main><nav class="l-nav p-menu"><ul class=p-menu__lists><li class=p-menu__listitem><a href=/>homepage</a></li><li class=p-menu__listitem><a href=/post/>posts</a></li><li class=p-menu__listitem><a href=/paper/>paper notes</a></li><li class=p-menu__listitem><a href=/book/>book notes</a></li></ul></nav><footer class=l-footer><ul class=c-links><li class=c-links__item><a href=https://matrix.to/#/%40kyle%3akylrth.com target=_blank><svg viewBox="0 0 30 28" class="c-links__icon"><title>matrix</title><use xlink:href="#icon-matrix"/></svg></a></li><li class=c-links__item><a href=https://github.com/kylrth target=_blank><svg viewBox="0 0 64 64" class="c-links__icon"><title>github</title><use xlink:href="#icon-github"/></svg></a></li><li class=c-links__item><a href=https://www.linkedin.com/in/kyle-roth/ target=_blank><svg viewBox="0 0 64 64" class="c-links__icon"><title>linkedin</title><use xlink:href="#icon-linkedin"/></svg></a></li></ul><p class=p-copyright><a href=https://github.com/kylrth/kylrth.github.io/commits/a1e1e22ea8901fe876c7207bb3d55f2ac726bcac/content/paper/ai-values-alignment/index.md>page
history</a></p></footer></body></html>