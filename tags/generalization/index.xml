<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Generalization on Kyle Roth</title><link>https://kylrth.com/tags/generalization/</link><description>Recent content in Generalization on Kyle Roth</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Tue, 17 Sep 2024 20:52:39 -0400</lastBuildDate><atom:link href="https://kylrth.com/tags/generalization/index.xml" rel="self" type="application/rss+xml"/><item><title>"Low-resource" text classification: a parameter-free classification method with compressors</title><link>https://kylrth.com/paper/gzip-text-classification/</link><pubDate>Mon, 24 Jul 2023 11:35:13 -0400</pubDate><guid>https://kylrth.com/paper/gzip-text-classification/</guid><description>&lt;p>&lt;em>I presented this paper in Bang Liu&amp;rsquo;s research group meeting on 2023-07-24. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/1w4n4UCWegJlDTCjKqWyTHOb3QHpnz1G2kTV6j_MODqY/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p>
&lt;p>It seems like the authors made a mistake that inflated the scores for the multilingual experiments, &lt;a href="https://kenschutte.com/gzip-knn-paper/">according to Ken Schutte&lt;/a>.&lt;/p></description></item><item><title>the effects of scale on worst-group performance</title><link>https://kylrth.com/post/worst-group-scale/</link><pubDate>Mon, 18 Jul 2022 15:31:12 -0400</pubDate><guid>https://kylrth.com/post/worst-group-scale/</guid><description>&lt;p>I think it&amp;rsquo;s valuable to be working in the open whenever possible, so I&amp;rsquo;m going to keep my research notes here. These notes will hopefully be full of good (and bad) ideas, so if someone borrows a good idea and publishes on it, that&amp;rsquo;s great!&lt;/p>
&lt;p>This post contains my research notes as I try to understand how model scaling affects worst-group performance. This started as a group project in the neural scaling laws course at Mila in winter 2022. We presented about an existing &lt;a href="https://kylrth.com/paper/effect-of-model-size-on-worst-group-generalization/">paper&lt;/a> and presented our preliminary results &lt;a href="https://sites.google.com/view/nsl-course/schedule#h.o7ntdr3dzoiv">in class&lt;/a>. The repository for this project is &lt;a href="https://github.com/kylrth/worst_group_scale/">here&lt;/a>.&lt;/p></description></item><item><title>The effect of model size on worst-group generalization</title><link>https://kylrth.com/paper/effect-of-model-size-on-worst-group-generalization/</link><pubDate>Thu, 17 Mar 2022 14:34:33 -0400</pubDate><guid>https://kylrth.com/paper/effect-of-model-size-on-worst-group-generalization/</guid><description>&lt;p>&lt;em>This was a paper we presented about in Irina Rish&amp;rsquo;s neural scaling laws course (&lt;a href="https://sites.google.com/view/nsl-course">IFT6167&lt;/a>) in winter 2022. You can view the slides we used &lt;a href="https://docs.google.com/presentation/d/1Fxs60aXvANsBj_k-m1h_KtjDfUrqPKY8f1UIh3j4XY0/edit?usp=sharing">here&lt;/a>, and the recording &lt;a href="https://sites.google.com/view/nsl-course/schedule#h.h6n8wkndidtb">here&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>Learning explanations that are hard to vary</title><link>https://kylrth.com/paper/learning-explanations-hard-to-vary/</link><pubDate>Tue, 22 Feb 2022 12:29:17 -0500</pubDate><guid>https://kylrth.com/paper/learning-explanations-hard-to-vary/</guid><description>&lt;p>The big idea here is to use the geometric mean instead of the arithmetic mean across samples in the batch when computing the gradient for SGD. This overcomes the situation where averaging produces optima that are not actually optimal for any individual samples, as demonstrated in their toy example below:&lt;/p>
&lt;img src="example.png" alt="Learning explanations that are hard to vary example.png" class="img-zoomable">

&lt;p>In practice, the method the authors test is not exactly the geometric mean for numerical and performance reasons, but effectively accomplishes the same thing by avoiding optima that are &amp;ldquo;inconsistent&amp;rdquo; (meaning that gradients from relatively few samples actually point in that direction).&lt;/p></description></item><item><title>In search of robust measures of generalization</title><link>https://kylrth.com/paper/robust-measures-of-generalization/</link><pubDate>Mon, 21 Feb 2022 15:33:22 -0500</pubDate><guid>https://kylrth.com/paper/robust-measures-of-generalization/</guid><description>&lt;p>These authors define &lt;em>robust error&lt;/em> as the least upper bound on the expected loss over a family of environmental settings (including dataset, model architecture, learning algorithm, etc.):&lt;/p>
&lt;p>&lt;code>\[\sup_{e\in\mathcal F}\mathbb E_{\omega\in P^e}\left[\ell(\phi,\omega)\right]\]&lt;/code>&lt;/p>
&lt;p>The fact that this is an &lt;strong>upper bound&lt;/strong> and not an average is very important and is what makes this work unique from previous work in this direction. Indeed, what we should be concerned about is not how poorly a model performs on the &lt;em>average&lt;/em> sample but on the &lt;em>worst-case&lt;/em> sample.&lt;/p></description></item><item><title>Inductive biases for deep learning of higher-level cognition</title><link>https://kylrth.com/paper/inductive-biases-higher-cognition/</link><pubDate>Tue, 08 Dec 2020 06:40:48 -0700</pubDate><guid>https://kylrth.com/paper/inductive-biases-higher-cognition/</guid><description>&lt;p>&lt;em>This is a long paper, so a lot of my writing here is an attempt to condense the discussion. I&amp;rsquo;ve taken the liberty to pull exact phrases and structure from the paper without explicitly using quotes.&lt;/em>&lt;/p>
&lt;blockquote>
&lt;p>Our main hypothesis is that deep learning succeeded in part because of a set of inductive biases, but that additional ones should be added in order to go from good in-distribution generalization in highly supervised learning tasks (or where strong and dense rewards are available), such as object recognition in images, to strong out-of-distribution generalization and transfer learning to new tasks with low sample complexity.&lt;/p></description></item><item><title>Overcoming catastrophic forgetting in neural networks</title><link>https://kylrth.com/paper/overcoming-catastrophic-forgetting/</link><pubDate>Thu, 01 Oct 2020 10:47:28 -0600</pubDate><guid>https://kylrth.com/paper/overcoming-catastrophic-forgetting/</guid><description>&lt;p>In the paper they use Bayes&amp;rsquo; rule to show that the contribution of the first of two tasks is contained in the posterior distribution of model parameters over the first dataset. This is important because it means we can estimate that posterior to try to get a sense for which model parameters were most important for that first task.&lt;/p>
&lt;p>In this paper, they perform that estimation using a multivariate Gaussian distribution. The means are the values of the model parameters after training on the first dataset, and the precision (inverse of variance) is the values of the diagonals along the Fisher information matrix.&lt;/p></description></item><item><title>Learning neural causal models from unknown interventions</title><link>https://kylrth.com/paper/neural-causal-models/</link><pubDate>Tue, 22 Sep 2020 10:39:54 -0600</pubDate><guid>https://kylrth.com/paper/neural-causal-models/</guid><description>&lt;p>&lt;em>This is a follow-on to &lt;a href="https://kylrth.com/paper/meta-transfer-objective-for-causal-mechanisms/">A meta-transfer objective for learning to disentangle causal mechanisms&lt;/a>&lt;/em>&lt;/p>
&lt;p>Here we describe an algorithm for predicting the causal graph structure of a set of visible random variables, each possibly causally dependent on any of the other variables.&lt;/p>
&lt;h2 id="the-algorithm">the algorithm &lt;a class="header-link" href="#the-algorithm">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>There are two sets of parameters, the &lt;em>structural parameters&lt;/em> and the &lt;em>functional parameters&lt;/em>. The structural parameters compose a matrix where &lt;code>\(\sigma(\gamma_{ij})\)&lt;/code> represents the belief that variable &lt;code>\(X_j\)&lt;/code> is a direct cause of &lt;code>\(X_i\)&lt;/code>. The functional parameters are the parameters of the neural networks that model the conditional probability distribution of each random variable given its parent set.&lt;/p></description></item><item><title>A meta-transfer objective for learning to disentangle causal mechanisms</title><link>https://kylrth.com/paper/meta-transfer-objective-for-causal-mechanisms/</link><pubDate>Mon, 21 Sep 2020 08:46:30 -0600</pubDate><guid>https://kylrth.com/paper/meta-transfer-objective-for-causal-mechanisms/</guid><description>&lt;p>Theoretically, models should be able to predict on out-of-distribution data if their understanding of causal relationships is correct. The toy problem they use in this paper is that of predicting temperature from altitude. If a model is trained on data from Switzerland, the model should ideally be able to correctly predict on data from the Netherlands, even though it hasn&amp;rsquo;t seen elevations that low before.&lt;/p>
&lt;p>The main contribution of this paper is that they&amp;rsquo;ve found that models tend to transfer &lt;em>faster&lt;/em> to a new distribution when they learn the correct causal relationships, and when those relationships are &lt;em>sparsely represented&lt;/em>, meaning they are represented by relatively few nodes in the network. This allowed them to create a meta-learning objective that trains the model to represent the correct causal dependencies, allowing for improved generalization.&lt;/p></description></item><item><title>Deep learning generalizes because the parameter-function map is biased towards simple functions</title><link>https://kylrth.com/paper/parameter-function-map-biased-to-simple/</link><pubDate>Tue, 08 Sep 2020 07:29:09 -0600</pubDate><guid>https://kylrth.com/paper/parameter-function-map-biased-to-simple/</guid><description>&lt;p>The theoretical value in talking about the parameter-function map is that this map lets us talk about sets of parameters that produce the same function. In this paper they used some recently proven stuff from algorithmic information theory (AIT) to show that for neural networks the parameter-function map is biased toward functions with low Komolgorov complexity, meaning that simple functions are more likely to appear given random choice of parameters. Since real world problems are also biased toward simple functions, this could explain the generalization/memorization results found by &lt;a href="https://kylrth.com/paper/understanding-requires-rethinking-generalization/">Zhang &lt;em>et al&lt;/em>&lt;/a>.&lt;/p></description></item><item><title>A closer look at memorization in deep networks</title><link>https://kylrth.com/paper/closer-look-at-memorization/</link><pubDate>Mon, 31 Aug 2020 11:52:35 -0600</pubDate><guid>https://kylrth.com/paper/closer-look-at-memorization/</guid><description>&lt;p>This paper builds on what we learned in &lt;a href="https://kylrth.com/paper/understanding-requires-rethinking-generalization/">&amp;ldquo;Understanding deep learning requires rethinking generalization&amp;rdquo;&lt;/a>. In that paper they showed that DNNs are able to fit pure noise in the same amount of time as it can fit real data, which means that our optimization algorithm (SGD, Adam, etc.) is not what&amp;rsquo;s keeping DNNs from overfitting.&lt;/p>
&lt;h2 id="experiments-for-detecting-easyhard-samples">experiments for detecting easy/hard samples &lt;a class="header-link" href="#experiments-for-detecting-easyhard-samples">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>It looks like there are qualitative differences between a DNN that has memorized some data and a DNN that has seen real data. In experiments they found that real datasets contain &amp;ldquo;easy examples&amp;rdquo; that are more quickly learned than the hard examples. This is not the case for random data.&lt;/p></description></item><item><title>Why does unsupervised pre-training help deep learning?</title><link>https://kylrth.com/paper/why-unsupervised-helps/</link><pubDate>Mon, 24 Aug 2020 11:40:00 -0600</pubDate><guid>https://kylrth.com/paper/why-unsupervised-helps/</guid><description>&lt;p>They&amp;rsquo;re pretty sure that it performs regularization by starting off the supervised training in a good spot, instead of by somehow improving the optimization path.&lt;/p></description></item><item><title>The consciousness prior</title><link>https://kylrth.com/paper/consciousness-prior/</link><pubDate>Fri, 14 Aug 2020 09:05:56 -0700</pubDate><guid>https://kylrth.com/paper/consciousness-prior/</guid><description>&lt;p>System 1 cognitive abilities are about low-level perception and intuitive knowledge. System 2 cognitive abilities can be described verbally, and include things like reasoning, planning, and imagination. In cognitive neuroscience, the &amp;ldquo;Global Workspace Theory&amp;rdquo; says that at each moment specific pieces of information become a part of working memory and become globally available to other unconscious computational processes. Relative to the unconscious state, the conscious state is low-dimensional, focusing on a few things. The paper proposes we use an attention mechanism (in the sense of the Bahdanau 2015 paper) to produce the conscious state, and then a VAE or conditional GAN to produce the output from the conscious state.&lt;/p></description></item><item><title>Compositional generalization by factorizing alignment and translation</title><link>https://kylrth.com/paper/factorizing-alignment-and-translation/</link><pubDate>Mon, 27 Jul 2020 09:11:16 -0700</pubDate><guid>https://kylrth.com/paper/factorizing-alignment-and-translation/</guid><description>&lt;p>They had a biRNN with attention for alignment encoding, and then a single linear function of each one-hot encoded word for encoding that single word. Their reasoning was that by separating the alignment from the meaning of individual words the model could more easily generalize to unseen words.&lt;/p></description></item></channel></rss>