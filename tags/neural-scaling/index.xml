<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Neural-Scaling on Kyle Roth</title><link>https://kylrth.com/tags/neural-scaling/</link><description>Recent content in Neural-Scaling on Kyle Roth</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Fri, 25 Oct 2024 15:54:58 -0400</lastBuildDate><atom:link href="https://kylrth.com/tags/neural-scaling/index.xml" rel="self" type="application/rss+xml"/><item><title>Beyond neural scaling laws: beating power law scaling via data pruning</title><link>https://kylrth.com/paper/beyond_nsl/</link><pubDate>Thu, 11 Aug 2022 14:09:08 -0400</pubDate><guid>https://kylrth.com/paper/beyond_nsl/</guid><description>&lt;p>In this paper they show that we can achieve exponential performance scaling over dataset size, when the samples added are pruned to be only the best examples. This beats power law scaling in a big way. There is still no free lunch, in some sense, because in most cases it will become progressively harder to add new useful samples as the dataset gets bigger. But this is a big deal for computation, because it means that the number of samples in the dataset is not nearly as important as the coverage and quality that the dataset provides.&lt;span class="sidenote-number">&lt;small class="sidenote">This also means that scaling laws for &lt;em>compute&lt;/em> (usually expressed as a function of dataset and model size) are dataset-specific and not generalizable, because of how much sample quality affects data scaling.&lt;/small>&lt;/span>&lt;/p></description></item><item><title>the effects of scale on worst-group performance</title><link>https://kylrth.com/post/worst-group-scale/</link><pubDate>Mon, 18 Jul 2022 15:31:12 -0400</pubDate><guid>https://kylrth.com/post/worst-group-scale/</guid><description>&lt;p>I think it&amp;rsquo;s valuable to be working in the open whenever possible, so I&amp;rsquo;m going to keep my research notes here. These notes will hopefully be full of good (and bad) ideas, so if someone borrows a good idea and publishes on it, that&amp;rsquo;s great!&lt;/p>
&lt;p>This post contains my research notes as I try to understand how model scaling affects worst-group performance. This started as a group project in the neural scaling laws course at Mila in winter 2022. We presented about an existing &lt;a href="https://kylrth.com/paper/effect-of-model-size-on-worst-group-generalization/">paper&lt;/a> and presented our preliminary results &lt;a href="https://sites.google.com/view/nsl-course/schedule#h.o7ntdr3dzoiv">in class&lt;/a>. The repository for this project is &lt;a href="https://github.com/kylrth/worst_group_scale/">here&lt;/a>.&lt;/p></description></item><item><title>PaLM</title><link>https://kylrth.com/paper/palm/</link><pubDate>Mon, 11 Apr 2022 12:17:25 -0400</pubDate><guid>https://kylrth.com/paper/palm/</guid><description>&lt;p>&lt;em>This was a paper I presented about in Bang Liu&amp;rsquo;s research group meeting on 2022-04-11. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/19TWSV9rACztA2Umw6VU2Bo61EwWycjyx-AoNlFdsk64/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>Scaling laws for the few-shot adaptation of pre-trained image classifiers</title><link>https://kylrth.com/paper/scaling-laws-few-shot-image-classifiers/</link><pubDate>Tue, 22 Feb 2022 13:19:12 -0500</pubDate><guid>https://kylrth.com/paper/scaling-laws-few-shot-image-classifiers/</guid><description>&lt;p>The unsurprising result here is that few-shot performance scales predictably with pre-training dataset size under traditional fine-tuning, matching network, and prototypical network approaches.&lt;/p>
&lt;p>The interesting result is that the exponents of these three approaches were substantially different (see Table 1 in the paper), which says to me that the few-shot inference approach matters a lot.&lt;/p>
&lt;p>The surprising result was that while more training on the &amp;ldquo;non-natural&amp;rdquo; &lt;a href="https://github.com/brendenlake/omniglot">Omniglot&lt;/a> dataset did not improve few-shot accuracy on other datasets, training on &amp;ldquo;natural&amp;rdquo; datasets &lt;em>did&lt;/em> improve accuracy on few-shot Omniglot.&lt;/p></description></item><item><title>In search of robust measures of generalization</title><link>https://kylrth.com/paper/robust-measures-of-generalization/</link><pubDate>Mon, 21 Feb 2022 15:33:22 -0500</pubDate><guid>https://kylrth.com/paper/robust-measures-of-generalization/</guid><description>&lt;p>These authors define &lt;em>robust error&lt;/em> as the least upper bound on the expected loss over a family of environmental settings (including dataset, model architecture, learning algorithm, etc.):&lt;/p>
&lt;p>&lt;code>\[\sup_{e\in\mathcal F}\mathbb E_{\omega\in P^e}\left[\ell(\phi,\omega)\right]\]&lt;/code>&lt;/p>
&lt;p>The fact that this is an &lt;strong>upper bound&lt;/strong> and not an average is very important and is what makes this work unique from previous work in this direction. Indeed, what we should be concerned about is not how poorly a model performs on the &lt;em>average&lt;/em> sample but on the &lt;em>worst-case&lt;/em> sample.&lt;/p></description></item><item><title>It's not just size that matters: small language models are also few-shot learners</title><link>https://kylrth.com/paper/not-just-size-that-matters/</link><pubDate>Fri, 18 Feb 2022 13:13:54 -0500</pubDate><guid>https://kylrth.com/paper/not-just-size-that-matters/</guid><description>&lt;p>&lt;em>We presented this paper as a mini-lecture in Bang Liu&amp;rsquo;s &lt;a href="https://www-labs.iro.umontreal.ca/~liubang/IFT%206289%20-%20Winter%202022.htm">IFT6289&lt;/a> course in winter 2022. You can view the slides we used &lt;a href="https://docs.google.com/presentation/d/1XPRSLC24AQK0MeZY5zww4gZ0t0B_gqDl8eztKZpt_v8/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>Scaling laws for transfer</title><link>https://kylrth.com/paper/scaling-laws-for-transfer/</link><pubDate>Wed, 16 Feb 2022 14:12:26 -0500</pubDate><guid>https://kylrth.com/paper/scaling-laws-for-transfer/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Irina Rish&amp;rsquo;s neural scaling laws course (&lt;a href="https://sites.google.com/view/nsl-course">IFT6167&lt;/a>) in winter 2022. The post contains no summarization, only questions and thoughts.&lt;/em>&lt;/p>
&lt;p>Sometimes these scaling laws can feel like pseudoscience because they&amp;rsquo;re a post hoc attempt to place a trend line on data. How can we be confident that the trends we observe actually reflect the scaling laws that we&amp;rsquo;re after? In the limitations section they mention that they didn&amp;rsquo;t tune hyperparameters for fine-tuning or for the code data distribution. How can we know that a confounding hyperparameter is not responsible for the trend we see? I wonder if we aren&amp;rsquo;t really being statistically rigorous until we can predict generalization error on an unseen &lt;em>training setup&lt;/em>, rather than just an unseen model size/dataset size.&lt;/p></description></item><item><title>Deep learning scaling is predictable, empirically</title><link>https://kylrth.com/paper/scaling-predictable-empirically/</link><pubDate>Mon, 14 Feb 2022 10:38:11 -0500</pubDate><guid>https://kylrth.com/paper/scaling-predictable-empirically/</guid><description>&lt;p>&lt;em>This was a paper we presented about in Irina Rish&amp;rsquo;s neural scaling laws course (&lt;a href="https://sites.google.com/view/nsl-course">IFT6167&lt;/a>) in winter 2022. You can view the slides we used &lt;a href="https://docs.google.com/presentation/d/1e0SXonZiW6o8VyqXTnjyYlMs97YCcntBznaoiBwlWFE/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p>
&lt;p>It&amp;rsquo;s important to note that in the results for NMT (Figure 1) we would expect the lines in the graph on the left to curve as the capacity of the individual models is exhausted. That&amp;rsquo;s why the authors fit the curves with an extra constant added. Meanwhile, the results in the graph on the right are curved because as the data size grows, the optimal model size also grows and it becomes increasingly difficult to find the right hyperparameters to train the model down to the optimal generalization error. (See the last paragraph in Section 4.1.)&lt;/p></description></item><item><title>Data scaling laws in NMT: the effect of noise and architecture</title><link>https://kylrth.com/paper/data-scaling-laws-nmt/</link><pubDate>Wed, 09 Feb 2022 20:47:59 -0500</pubDate><guid>https://kylrth.com/paper/data-scaling-laws-nmt/</guid><description>&lt;p>This paper is all about trying a bunch of different changes to the training setup to see what affects the power law exponent &lt;strong>over dataset size&lt;/strong>. Here are some of the answers:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>encoder-decoder size asymmetry&lt;/strong>: exponent not affected, but effective model capacity affected&lt;/li>
&lt;li>&lt;strong>architecture (LSTM vs. Transformer)&lt;/strong>: exponent not affected, but effective model capacity affected&lt;/li>
&lt;li>&lt;strong>dataset quality (filtered vs. not)&lt;/strong>: exponent and effective model capacity not effected, losses on smaller datasets affected&lt;/li>
&lt;li>&lt;strong>dataset source (ParaCrawl vs. in-house dataset)&lt;/strong>: exponent not affected&lt;/li>
&lt;li>&lt;strong>adding independent noise&lt;/strong>: exponent not affected, but effective model capacity affected&lt;/li>
&lt;/ul>
&lt;p>Here are some other things to test that I thought of while I read this:&lt;/p></description></item><item><title>Parallel training of deep networks with local updates</title><link>https://kylrth.com/paper/parallel-training-with-local-updates/</link><pubDate>Wed, 09 Feb 2022 10:50:21 -0500</pubDate><guid>https://kylrth.com/paper/parallel-training-with-local-updates/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Irina Rish&amp;rsquo;s neural scaling laws course (&lt;a href="https://sites.google.com/view/nsl-course">IFT6167&lt;/a>) in winter 2022. The post contains no summarization, only questions and thoughts.&lt;/em>&lt;/p>
&lt;p>Once I learned how the loss functions worked for each chunk, my first question was whether the earlier chunks were going to be able to learn the low-level features that later chunks would need. Figure 7 seems to show that they do, although their quality apparently decreases with increasingly local updates.&lt;/p></description></item><item><title>Learning transferable visual models from natural language supervision (CLIP)</title><link>https://kylrth.com/paper/clip/</link><pubDate>Wed, 02 Feb 2022 12:35:03 -0500</pubDate><guid>https://kylrth.com/paper/clip/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Irina Rish&amp;rsquo;s neural scaling laws course (&lt;a href="https://sites.google.com/view/nsl-course">IFT6167&lt;/a>) in winter 2022. The post contains no summarization, only questions and thoughts.&lt;/em>&lt;/p>
&lt;p>This concept of wide vs. narrow supervision (rather than binary &amp;ldquo;supervised&amp;rdquo; and &amp;ldquo;unsupervised&amp;rdquo;) is an interesting and flexible way to think about the way these training schemes leverage data.&lt;/p>
&lt;p>The zero-shot CLIP matches the performance of 4-shot CLIP, which is a surprising result. What do the authors mean when they make this guess about zero-shot&amp;rsquo;s advantage:&lt;/p></description></item></channel></rss>