<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Vision on Kyle Roth</title><link>https://kylrth.com/tags/vision/</link><description>Recent content in Vision on Kyle Roth</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Fri, 25 Oct 2024 15:54:58 -0400</lastBuildDate><atom:link href="https://kylrth.com/tags/vision/index.xml" rel="self" type="application/rss+xml"/><item><title>Masked autoencoders are scalable vision learners</title><link>https://kylrth.com/paper/masked-autoencoders-are-scalable-vision-learners/</link><pubDate>Fri, 11 Feb 2022 14:18:30 -0500</pubDate><guid>https://kylrth.com/paper/masked-autoencoders-are-scalable-vision-learners/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Irina Rish&amp;rsquo;s neural scaling laws course (&lt;a href="https://sites.google.com/view/nsl-course">IFT6167&lt;/a>) in winter 2022. The post contains no summarization, only questions and thoughts.&lt;/em>&lt;/p>
&lt;p>In this paper they mention that the mask vector is learned, and it sounds like the positional embeddings are also learned. I remember in &lt;a href="https://kylrth.com/paper/attention-all-you-need/">&lt;em>Attention is all you need&lt;/em>&lt;/a> they found that cosine positional embeddings worked better than learned ones, especially for sequences of longer length. But now it seems like most papers are doing learned embeddings. If anyone knows why, send me an email.&lt;/p></description></item></channel></rss>