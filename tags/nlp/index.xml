<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Nlp on Kyle Roth</title><link>https://kylrth.com/tags/nlp/</link><description>Recent content in Nlp on Kyle Roth</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Fri, 25 Oct 2024 15:54:58 -0400</lastBuildDate><atom:link href="https://kylrth.com/tags/nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>InstructEval: systematic evaluation of instruction selection methods</title><link>https://kylrth.com/paper/instructeval/</link><pubDate>Mon, 25 Sep 2023 11:38:24 -0400</pubDate><guid>https://kylrth.com/paper/instructeval/</guid><description>&lt;p>&lt;em>This was a paper I presented about in Bang Liu&amp;rsquo;s research group meeting on 2023-09-25. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/1Qo_KPNnkj2jQzYDKG1wSEisHAr18Fs_eHq5Iuzj0MNs/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>"Low-resource" text classification: a parameter-free classification method with compressors</title><link>https://kylrth.com/paper/gzip-text-classification/</link><pubDate>Mon, 24 Jul 2023 11:35:13 -0400</pubDate><guid>https://kylrth.com/paper/gzip-text-classification/</guid><description>&lt;p>&lt;em>I presented this paper in Bang Liu&amp;rsquo;s research group meeting on 2023-07-24. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/1w4n4UCWegJlDTCjKqWyTHOb3QHpnz1G2kTV6j_MODqY/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p>
&lt;p>It seems like the authors made a mistake that inflated the scores for the multilingual experiments, &lt;a href="https://kenschutte.com/gzip-knn-paper/">according to Ken Schutte&lt;/a>.&lt;/p></description></item><item><title>Whisper: robust speech recognition via large-scale weak supervision</title><link>https://kylrth.com/paper/whisper/</link><pubDate>Fri, 30 Sep 2022 09:46:29 -0400</pubDate><guid>https://kylrth.com/paper/whisper/</guid><description>&lt;p>&lt;em>This was a paper I presented about in Bang Liu&amp;rsquo;s research group meeting on 2022-09-30. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/1bGjzq0f2KEh49F9eyaNYz_VNlEQVJW0HKR61d48Y9fs/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>Selective annotation makes language models better few-shot learners</title><link>https://kylrth.com/paper/selective-annotation/</link><pubDate>Tue, 13 Sep 2022 11:26:21 -0400</pubDate><guid>https://kylrth.com/paper/selective-annotation/</guid><description>&lt;p>Selective annotation chooses a pool of samples to annotate from a large set of unlabeled data. The main result of the paper is that when this is combined with item-specific prompt retrieval the performance drastically improves (&amp;gt;10% relative gain and lower performance variance). Interestingly, selective annotation does &lt;em>not&lt;/em> help for finetuning, or when the prompts are randomly selected. They call their selective annotation method &amp;ldquo;vote-&lt;code>\(k\)&lt;/code>&amp;rdquo;.&lt;/p>
&lt;h2 id="selective-annotation-method">selective annotation method &lt;a class="header-link" href="#selective-annotation-method">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>Vote-&lt;code>\(k\)&lt;/code> essentially creates a network of similar&lt;span class="sidenote-number">&lt;small class="sidenote">according to Sentence-BERT&lt;/small>&lt;/span>
unlabeled instances, and then selects from them with a network importance score that is discounted to promote diversity&lt;span class="sidenote-number">&lt;small class="sidenote">The discounting is performed by iteratively adding to the selection set, each time penalizing new nodes for being close to nodes that are already in the selection set.&lt;/small>&lt;/span>
.&lt;/p></description></item><item><title>Continual-T0: progressively instructing 50+ tasks to language models without forgetting</title><link>https://kylrth.com/paper/continual-t0/</link><pubDate>Thu, 02 Jun 2022 15:28:55 -0400</pubDate><guid>https://kylrth.com/paper/continual-t0/</guid><description>&lt;p>&lt;em>This was a paper I presented about in Bang Liu&amp;rsquo;s research group meeting on 2022-06-06. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/1-L5TnQvh-4WQHRSlIU-gcCyzudFxzZC0ur7vtLS28gs/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p>
&lt;p>Continual-T0 (CT0) extends &lt;a href="https://kylrth.com/paper/t0/">T0&lt;/a> by progressively training it on 8 unseen language generation tasks, while retaining a replay buffer of 1% of the original training data to preserve performance. The result is a model that maintains nearly all of its performance on previous tasks while learning the new tasks. In addition, CT0 maintains the original T0&amp;rsquo;s performance on unseen tasks (which is a big deal because those tasks could not appear in the replay buffer) and it extends the compositionality of T0 to even more unseen tasks.&lt;/p></description></item><item><title>Multitask prompted training enables zero-shot task generalization (T0)</title><link>https://kylrth.com/paper/t0/</link><pubDate>Fri, 27 May 2022 17:05:02 -0400</pubDate><guid>https://kylrth.com/paper/t0/</guid><description>&lt;p>T0 builds on T5 by fine-tuning on more natural prompts and testing the model&amp;rsquo;s generalization to held-out tasks.&lt;/p>
&lt;p>Compare the training format diagrams for T5 (top) and T0 (bottom):&lt;/p>
&lt;p>&lt;img src="t5.png" alt="Multitask prompted training enables zero-shot task generalization (T0) t5.png" class="img-zoomable">

&lt;br />&lt;br />&lt;br />
&lt;img src="prompt_format.png" alt="Multitask prompted training enables zero-shot task generalization (T0) prompt_format.png" class="img-zoomable">
&lt;/p>
&lt;p>Intuitively, the T0 prompts are more likely to be similar to implicit/explicit prompting that&amp;rsquo;s present in the pretraining data. The authors created several prompts for each dataset.&lt;/p></description></item><item><title>WordBurner beta</title><link>https://kylrth.com/post/wordburner/</link><pubDate>Mon, 18 Apr 2022 23:08:52 -0400</pubDate><guid>https://kylrth.com/post/wordburner/</guid><description>&lt;p>&lt;strong>Update 2022-04-27&lt;/strong>: The beta is over, but the apk is still installable with the instructions below and any feedback sent from inside the app will be received by me. I&amp;rsquo;m going to be working on this more over the summer, and eventually publishing it on the app store. :)&lt;/p>
&lt;p>Ever since learning Spanish, it has been a dream of mine to create a vocabulary study app that meets my needs. Duolingo won&amp;rsquo;t cover advanced vocabulary, Anki requires manually-generated decks, and other apps have expensive subscription plans.&lt;/p></description></item><item><title>PaLM</title><link>https://kylrth.com/paper/palm/</link><pubDate>Mon, 11 Apr 2022 12:17:25 -0400</pubDate><guid>https://kylrth.com/paper/palm/</guid><description>&lt;p>&lt;em>This was a paper I presented about in Bang Liu&amp;rsquo;s research group meeting on 2022-04-11. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/19TWSV9rACztA2Umw6VU2Bo61EwWycjyx-AoNlFdsk64/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>It's not just size that matters: small language models are also few-shot learners</title><link>https://kylrth.com/paper/not-just-size-that-matters/</link><pubDate>Fri, 18 Feb 2022 13:13:54 -0500</pubDate><guid>https://kylrth.com/paper/not-just-size-that-matters/</guid><description>&lt;p>&lt;em>We presented this paper as a mini-lecture in Bang Liu&amp;rsquo;s &lt;a href="https://www-labs.iro.umontreal.ca/~liubang/IFT%206289%20-%20Winter%202022.htm">IFT6289&lt;/a> course in winter 2022. You can view the slides we used &lt;a href="https://docs.google.com/presentation/d/1XPRSLC24AQK0MeZY5zww4gZ0t0B_gqDl8eztKZpt_v8/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>A sensitivity analysis of (and practitionersâ€™ guide to) convolutional neural networks for sentence classification</title><link>https://kylrth.com/paper/cnn-sentence/</link><pubDate>Wed, 02 Feb 2022 15:35:00 -0500</pubDate><guid>https://kylrth.com/paper/cnn-sentence/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Bang Liu&amp;rsquo;s &lt;a href="https://www-labs.iro.umontreal.ca/~liubang/IFT%206289%20-%20Winter%202022.htm">IFT6289&lt;/a> course in winter 2022. The structure of the post follows the structure of the assignment: summarization followed by my own comments.&lt;/em>&lt;/p>
&lt;h2 id="paper-summarization">paper summarization &lt;a class="header-link" href="#paper-summarization">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>Word embeddings have gotten so good that state-of-the-art sentence classification can often be achieved with just a one-layer convolutional network on top of those embeddings. This paper dials in on the specifics of training that convolutional layer for this downstream sentence classification task.&lt;/p></description></item><item><title>Learning transferable visual models from natural language supervision (CLIP)</title><link>https://kylrth.com/paper/clip/</link><pubDate>Wed, 02 Feb 2022 12:35:03 -0500</pubDate><guid>https://kylrth.com/paper/clip/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Irina Rish&amp;rsquo;s neural scaling laws course (&lt;a href="https://sites.google.com/view/nsl-course">IFT6167&lt;/a>) in winter 2022. The post contains no summarization, only questions and thoughts.&lt;/em>&lt;/p>
&lt;p>This concept of wide vs. narrow supervision (rather than binary &amp;ldquo;supervised&amp;rdquo; and &amp;ldquo;unsupervised&amp;rdquo;) is an interesting and flexible way to think about the way these training schemes leverage data.&lt;/p>
&lt;p>The zero-shot CLIP matches the performance of 4-shot CLIP, which is a surprising result. What do the authors mean when they make this guess about zero-shot&amp;rsquo;s advantage:&lt;/p></description></item><item><title>Distributed representations of words and phrases and their compositionality</title><link>https://kylrth.com/paper/distributed-representations/</link><pubDate>Tue, 01 Feb 2022 16:09:19 -0500</pubDate><guid>https://kylrth.com/paper/distributed-representations/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Bang Liu&amp;rsquo;s &lt;a href="https://www-labs.iro.umontreal.ca/~liubang/IFT%206289%20-%20Winter%202022.htm">IFT6289&lt;/a> course in winter 2022. The structure of the post follows the structure of the assignment: summarization followed by my own comments.&lt;/em>&lt;/p>
&lt;h2 id="paper-summarization">paper summarization &lt;a class="header-link" href="#paper-summarization">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>This paper describes multiple improvements that are made to the original &lt;a href="https://arxiv.org/abs/1301.3781">Skip-gram&lt;/a> model:&lt;/p>
&lt;ol>
&lt;li>Decreasing the rate of exposure to common words improves the training speed and increases the model&amp;rsquo;s accuracy on infrequent words.&lt;/li>
&lt;li>A new training target they call &amp;ldquo;negative sampling&amp;rdquo; improves the training speed and the model&amp;rsquo;s accuracy on frequent words.&lt;/li>
&lt;li>Allowing the model to use phrase vectors improves the expressivity of the model.&lt;/li>
&lt;/ol>
&lt;h3 id="negative-sampling">negative sampling &lt;a class="header-link" href="#negative-sampling">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h3>
&lt;p>The original Skip-gram model computed probabilities using a hierarchical softmax, which allowed the model to compute only &lt;code>\(O(\log_2(|V|))\)&lt;/code> probabilities when estimating the probability of a particular word, rather than &lt;code>\(O(|V|)\)&lt;/code>. Negative sampling, on the other hand, deals directly with the generated vector representations. The negative sampling loss function basically tries to maximize cosine similarity between the input representation of the input word with the output representation of the neighboring word, while decreasing cosine similarity between the input word and a few random vectors. They find that the required number of negative examples decreases as the dataset size increases.&lt;/p></description></item><item><title>Cross-lingual alignment of contextual word embeddings, with applications to zero-shot dependency parsing</title><link>https://kylrth.com/paper/cross-lingual-alignment-contextual/</link><pubDate>Fri, 11 Dec 2020 06:30:43 -0700</pubDate><guid>https://kylrth.com/paper/cross-lingual-alignment-contextual/</guid><description>&lt;p>Recent contextual word embeddings (e.g. &lt;a href="https://kylrth.com/paper/deep-contextualized-word-representations/">ELMo&lt;/a>) have shown to be much better than &amp;ldquo;static&amp;rdquo; embeddings (where there&amp;rsquo;s a one-to-one mapping from token to representation). This paper is exciting because they were able to create a multi-lingual embedding space that used &lt;em>contextual&lt;/em> word embeddings.&lt;/p>
&lt;p>Each token will have a &amp;ldquo;point cloud&amp;rdquo; of embedding values, one point for each context containing the token. They define the &lt;em>embedding anchor&lt;/em> as the average of all those points for a particular token. Here&amp;rsquo;s a figure from the paper that displays a two-dimensional PCA of the contextual representations for four Spanish words, along with their anchors:&lt;/p></description></item><item><title>SpanBERT: improving pre-training by representing and predicting spans</title><link>https://kylrth.com/paper/spanbert/</link><pubDate>Sat, 05 Dec 2020 16:08:03 -0700</pubDate><guid>https://kylrth.com/paper/spanbert/</guid><description>&lt;p>&lt;a href="https://kylrth.com/paper/bert/">BERT&lt;/a> optimizes the Masked Language Model (MLM) objective by masking word pieces &lt;em>uniformly at random&lt;/em> in its training data and attempting to predict the masked values. With SpanBERT, spans of tokens are masked and the model is expected to predict the text in the spans from the representations of the words on the boundary. Span lengths follow a geometric distribution, and span start points are uniformly random.&lt;/p>
&lt;p>To predict each individual masked token, a two-layer feedforward network was provided with the boundary token representations plus the position embedding of the target token, and the output vector representation was used to predict the masked token and compute cross-entropy loss exactly as in standard MLM.&lt;/p></description></item><item><title>Deep contextualized word representations</title><link>https://kylrth.com/paper/deep-contextualized-word-representations/</link><pubDate>Thu, 03 Dec 2020 12:01:43 -0700</pubDate><guid>https://kylrth.com/paper/deep-contextualized-word-representations/</guid><description>&lt;p>&lt;em>This is the original paper introducing Embeddings from Language Models (ELMo).&lt;/em>&lt;/p>
&lt;blockquote>
&lt;p>Unlike most widely used word embeddings, ELMo word representations are functions of the entire input sentence.&lt;/p>
&lt;/blockquote>
&lt;p>That&amp;rsquo;s what makes ELMo great: they&amp;rsquo;re &lt;em>contextualized&lt;/em> word representations, meaning that they can express multiple possible senses of the same word.&lt;/p>
&lt;p>Specifically, ELMo representations are a learned linear combination of all layers of an LSTM encoding. The LSTM undergoes general semi-supervised pretraining, but the linear combination is learned &lt;em>specific to the task&lt;/em>. It&amp;rsquo;s been shown that initial layers in LSTM encoders are more representative of syntax, while later layers tend to represent semantics, so this linear combination is a key advantage that allows ELMo to improve accuracy on tasks ranging from POS tagging to question answering.&lt;/p></description></item><item><title>Attention is all you need</title><link>https://kylrth.com/paper/attention-all-you-need/</link><pubDate>Wed, 05 Aug 2020 12:37:42 -0700</pubDate><guid>https://kylrth.com/paper/attention-all-you-need/</guid><description>&lt;p>&lt;em>I also referred to &lt;a href="https://github.com/lilianweng/transformer-tensorflow">this implementation&lt;/a> to understand some of the details.&lt;/em>&lt;/p>
&lt;p>This is the paper describing the Transformer, a sequence-to-sequence model based entirely on attention. I think it&amp;rsquo;s best described with pictures.&lt;/p>
&lt;h2 id="model-overview">model overview &lt;a class="header-link" href="#model-overview">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;img src="transformer.png" alt="Attention is all you need transformer.png" class="img-zoomable">

&lt;p>From this picture, I think the following things need explaining:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>embeddings&lt;/strong> these are learned embeddings that convert the input and output tokens to vectors of the model dimension. In this paper, they actually used the same weight matrix for input embedding, output embedding, and the final linear layer before the final softmax.&lt;/li>
&lt;li>&lt;strong>positional encoding&lt;/strong>: since there&amp;rsquo;s no concept of a hidden state or convolution that encodes the order of the inputs, we have to add some information about the position of the tokens. They used a sinusoidal positional encoding that was a function of the position and the dimension. The wavelength for each dimension forms a geometric progression from &lt;code>\(2\pi\)&lt;/code> to 10000 times that.&lt;/li>
&lt;li>&lt;strong>the outputs are &amp;ldquo;shifted right&amp;rdquo;&lt;/strong>&lt;/li>
&lt;li>&lt;strong>multi-head attention&lt;/strong>: see below for a description of multi-head attention. In the encoder-decoder attention layers, &lt;code>\(Q\)&lt;/code> comes from the previous masked attention layer and &lt;code>\(K\)&lt;/code> and &lt;code>\(V\)&lt;/code> come from the output of the encoder. Everywhere else uses self-attention, meaning that &lt;code>\(Q\)&lt;/code>, &lt;code>\(K\)&lt;/code>, and &lt;code>\(V\)&lt;/code> are all the same.&lt;/li>
&lt;li>&lt;strong>&lt;em>masked&lt;/em> multi-head attention&lt;/strong>: in the self-attention layers in the decoder, we can&amp;rsquo;t allow positions to attend to positions ahead of themselves, so we set all right-connecting values in the input of the softmax (right after scaling; see the image below) to negative infinity.&lt;/li>
&lt;li>&lt;strong>feed-forward blocks&lt;/strong> these are two linear transformation with ReLU in between. The transformations are the same across each position, but they are different transformations from layer to layer, as you might expect.&lt;/li>
&lt;li>&lt;strong>add &amp;amp; norm&lt;/strong>: these are residual connections followed by layer normalization.&lt;/li>
&lt;/ol>
&lt;h3 id="multi-head-attention">multi-head attention &lt;a class="header-link" href="#multi-head-attention">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h3>
&lt;img src="multi-head-attention.png" alt="Attention is all you need multi-head-attention.png" class="img-zoomable">

&lt;p>The &amp;ldquo;Mask (opt.)&amp;rdquo; can be ignored because that&amp;rsquo;s for masked attention, described above.&lt;/p></description></item><item><title>BERT: pre-training of deep bidirectional transformers for language understanding</title><link>https://kylrth.com/paper/bert/</link><pubDate>Tue, 04 Aug 2020 08:57:44 -0700</pubDate><guid>https://kylrth.com/paper/bert/</guid><description>&lt;p>The B is for bidirectional, and that&amp;rsquo;s a big deal. It makes it possible to do well on sentence-level (NLI, question answering) and token-level tasks (NER, POS tagging). In a unidirectional model, the word &amp;ldquo;bank&amp;rdquo; in a sentence like &amp;ldquo;I made a bank deposit.&amp;rdquo; has only &amp;ldquo;I made a&amp;rdquo; as its context, keeping useful information from the model.&lt;/p>
&lt;p>Another cool thing is masked language model training (MLM). They train the model by blanking certain words in the sentence and asking the model to guess the missing word.&lt;/p></description></item><item><title>Google's neural machine translation system: bridging the gap between human and machine translation</title><link>https://kylrth.com/paper/google-nmt-2016/</link><pubDate>Tue, 30 Jun 2020 08:22:30 -0600</pubDate><guid>https://kylrth.com/paper/google-nmt-2016/</guid><description>&lt;p>&lt;em>This model was superseded by &lt;a href="https://kylrth.com/paper/google-zero-shot/">this one&lt;/a>.&lt;/em>&lt;/p>
&lt;p>They did some careful things with residual connections to make sure it was very parallelizable. They put each LSTM layer on a separate GPU. They quantized the models such that they could train using full floating-point computations with a couple restrictions and then convert the models to quantized versions.&lt;/p></description></item><item><title>Google's multilingual neural machine translation system</title><link>https://kylrth.com/paper/google-zero-shot/</link><pubDate>Fri, 26 Jun 2020 08:02:12 -0600</pubDate><guid>https://kylrth.com/paper/google-zero-shot/</guid><description>&lt;p>They use the word-piece model from &lt;a href="https://kylrth.com/paper/word-piece-model/">&amp;ldquo;Japanese and Korean Voice Search&amp;rdquo;&lt;/a>, with 32,000 word pieces. (This is a lot less than the 200,000 used in that paper.) They state in the paper that the shared word-piece model is very similar to Byte-Pair-Encoding, which was used for NMT in &lt;a href="https://www.aclweb.org/anthology/P16-1162.pdf">this paper&lt;/a> by researchers at U of Edinburgh.&lt;/p>
&lt;p>The model and training process are exactly as in &lt;a href="https://kylrth.com/paper/google-nmt-2016/">Google&amp;rsquo;s earlier paper&lt;/a>. It takes &lt;em>3 weeks&lt;/em> on &lt;em>100 GPUs&lt;/em> to train, even after increasing batch size and learning rate.&lt;/p></description></item><item><title>Japanese and Korean voice search</title><link>https://kylrth.com/paper/word-piece-model/</link><pubDate>Wed, 24 Jun 2020 14:44:02 -0600</pubDate><guid>https://kylrth.com/paper/word-piece-model/</guid><description>&lt;p>&lt;em>This was mentioned in the paper on &lt;a href="https://kylrth.com/paper/google-zero-shot/">Google&amp;rsquo;s Multilingual Neural Machine Translation System&lt;/a>. It&amp;rsquo;s regarded as the original paper to use the word-piece model, which is the focus of my notes here.&lt;/em>&lt;/p>
&lt;h2 id="the-wordpiecemodel">the WordPieceModel &lt;a class="header-link" href="#the-wordpiecemodel">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>Here&amp;rsquo;s the WordPieceModel algorithm:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-txt" data-lang="txt">&lt;span style="display:flex;">&lt;span>func WordPieceModel(D, chars, n, threshold) -&amp;gt; inventory:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> # D: training data
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> # n: user-specified number of word units (often 200k)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> # chars: unicode characters used in the language (e.g. Kanji, Hiragana, Katakana, ASCII for Japanese)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> # threshold: stopping criterion for likelihood increase
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> # inventory: the set of word units created by the model
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> inventory := chars
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> likelihood := +INF
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> while len(inventory) &amp;lt; n &amp;amp;&amp;amp; likelihood &amp;gt;= threshold:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lm := LM(inventory, D)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> inventory += argmax_{combined word unit}(lm.likelihood_{inventory + combined word unit}(D))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> likelihood = lm.likelihood_{inventory}(D)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> return inventory
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The algorithm can be optimized by&lt;/p></description></item><item><title>Towards a multi-view language representation</title><link>https://kylrth.com/paper/multi-view-language-representation/</link><pubDate>Tue, 23 Jun 2020 08:40:04 -0600</pubDate><guid>https://kylrth.com/paper/multi-view-language-representation/</guid><description>&lt;p>They used a technique called CCA to combine hand-made features with NN representations. It didn&amp;rsquo;t do great on typological feature prediction, but it did do well with predicting a phylogenetic tree for Indo-European languages.&lt;/p></description></item></channel></rss>