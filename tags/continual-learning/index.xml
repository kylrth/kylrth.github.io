<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Continual-Learning on Kyle Roth</title><link>https://kylrth.com/tags/continual-learning/</link><description>Recent content in Continual-Learning on Kyle Roth</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Tue, 17 Sep 2024 20:52:39 -0400</lastBuildDate><atom:link href="https://kylrth.com/tags/continual-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Continual-T0: progressively instructing 50+ tasks to language models without forgetting</title><link>https://kylrth.com/paper/continual-t0/</link><pubDate>Thu, 02 Jun 2022 15:28:55 -0400</pubDate><guid>https://kylrth.com/paper/continual-t0/</guid><description>&lt;p>&lt;em>This was a paper I presented about in Bang Liu&amp;rsquo;s research group meeting on 2022-06-06. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/1-L5TnQvh-4WQHRSlIU-gcCyzudFxzZC0ur7vtLS28gs/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p>
&lt;p>Continual-T0 (CT0) extends &lt;a href="https://kylrth.com/paper/t0/">T0&lt;/a> by progressively training it on 8 unseen language generation tasks, while retaining a replay buffer of 1% of the original training data to preserve performance. The result is a model that maintains nearly all of its performance on previous tasks while learning the new tasks. In addition, CT0 maintains the original T0&amp;rsquo;s performance on unseen tasks (which is a big deal because those tasks could not appear in the replay buffer) and it extends the compositionality of T0 to even more unseen tasks.&lt;/p></description></item></channel></rss>