<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ethics on Kyle Roth</title><link>https://kylrth.com/tags/ethics/</link><description>Recent content in Ethics on Kyle Roth</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 11 Nov 2024 16:26:59 -0500</lastBuildDate><atom:link href="https://kylrth.com/tags/ethics/index.xml" rel="self" type="application/rss+xml"/><item><title>Speaking as from the Dust: ideologies of AI and digital resurrection in Mormon culture</title><link>https://kylrth.com/paper/speaking-as-from-the-dust/</link><pubDate>Wed, 08 May 2024 15:03:18 -0400</pubDate><guid>https://kylrth.com/paper/speaking-as-from-the-dust/</guid><description>&lt;p>I wrote this paper with Stephen Betts, a friend of mine doing Mormon Studies at the University of Virginia. (Click the title to download the preprint.) Our interest in the topic was initially piqued when we stumbled across the &lt;a href="https://wilfordwoodruffpapers.org/wilford-woodruff-ai-learning-experience">Wilford Woodruff AI Learning Experience&lt;/a>, and this paper explores the unique intersection of Mormonism, transhumanism, and machine learning that makes such a thing possible.&lt;/p>
&lt;p>We&amp;rsquo;re currently looking for a venue to publish this work. &lt;strong>Any feedback on the preprint is appreciated!&lt;/strong>&lt;/p></description></item><item><title>Better, Nicer, Clearer, Fairer: a critical assessment of the movement for ethical artificial intelligence and machine learning</title><link>https://kylrth.com/paper/better-nicer-cleaner-fairer/</link><pubDate>Tue, 24 Oct 2023 12:53:31 -0400</pubDate><guid>https://kylrth.com/paper/better-nicer-cleaner-fairer/</guid><description>&lt;p>&lt;em>I will present this paper in the FATE (fairness, accountability, transparency, ethics) reading group tomorrow (2023-10-25). You can view the slides I&amp;rsquo;ll use &lt;a href="https://docs.google.com/presentation/d/1Bl4Bfh-ryFfmsEVrmdnYBe69hKjZP4mYZppvZHU5-yM/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p>
&lt;p>There are unresolved tensions in the algorithmic ethics world. Here are two examples:&lt;/p>
&lt;ul>
&lt;li>Is inclusion always good?
&lt;ul>
&lt;li>Gebru: &amp;ldquo;you can&amp;rsquo;t have ethical A.I. that&amp;rsquo;s not inclusive&amp;hellip; [a]nd whoever is creating the technology is setting the standards&amp;rdquo;&lt;/li>
&lt;li>Nelson: &amp;ldquo;&amp;hellip; I struggle to understand why we want to make black communities more cognizant in facial recognition systems that are disproportionately used for surveillance.&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>academic activism
&lt;ul>
&lt;li>O&amp;rsquo;Neil: why is there a lack of academic efforts to inform policymakers and regulators?&lt;/li>
&lt;li>PERVADE: Academics have been doing this work for a while but it is underfunded, marginalized, and at odds with a US political apparatus generally favorable towards Silicon Valley.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Ethics manifestos or value statements mask these tensions behind a business ethics lens.&lt;/p></description></item><item><title>Team Human interview with Dennis Yi Tenen</title><link>https://kylrth.com/post/team-human-tenen/</link><pubDate>Mon, 09 Oct 2023 10:34:12 -0400</pubDate><guid>https://kylrth.com/post/team-human-tenen/</guid><description>&lt;p>&lt;em>You can view this episode &lt;a href="https://www.teamhuman.fm/episodes/265-dennis-yi-tenen">here&lt;/a>, and you can download a transcript I made with whisper-medium &lt;a href="https://dl.kylrth.com/teamhuman_tenen.txt">here&lt;/a>. I accept responsibility for errors in the transcript, alongside OpenAI, all people whose voices exist on the web, and the rest of humanity. :)&lt;/em>&lt;/p>
&lt;h2 id="writing-technology">writing technology &lt;a class="header-link" href="#writing-technology">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;blockquote>
&lt;p>&lt;strong>DYT&lt;/strong>: You know, it took humans like centuries to perfect the technology of a dictionary and it took hundreds, thousands of people, probably like millions of hours to actually get to the point where you can easily look up a word.&lt;/p></description></item><item><title>the inherent subjectivity of reality</title><link>https://kylrth.com/post/edward-frenkel/</link><pubDate>Fri, 14 Apr 2023 14:45:44 -0400</pubDate><guid>https://kylrth.com/post/edward-frenkel/</guid><description>&lt;p>&lt;em>These are some thoughts I&amp;rsquo;ve had while listening to a Lex Fridman &lt;a href="https://lexfridman.com/edward-frenkel/">interview&lt;/a> with Edward Frenkel, a mathematician at UC Berkeley working on mathematical quantum physics.&lt;/em>&lt;/p>
&lt;p>In the information age, we like to see everything as computation. But what do we mean when we say that something is computation? We mean that a physical system with predictable interactions has a meaningful result. If we somehow learned that the universe was computational in nature, the only thing that adds is that the universe&amp;rsquo;s state is meaningful somehow.&lt;/p></description></item><item><title>for a socially beneficial and responsible development of AI</title><link>https://kylrth.com/post/bengio-crawford/</link><pubDate>Mon, 20 Mar 2023 14:04:51 -0400</pubDate><guid>https://kylrth.com/post/bengio-crawford/</guid><description>&lt;p>&lt;em>These are my notes from a conversation between &lt;a href="https://en.wikipedia.org/wiki/Yoshua_Bengio">Yoshua Bengio&lt;/a> and &lt;a href="https://en.wikipedia.org/wiki/Kate_Crawford">Kate Crawford&lt;/a> held at &lt;a href="https://mila.quebec/">Mila&lt;/a> on 2023-03-20, announcing the release of a new book created as a joint report between Mila and &lt;a href="https://en.wikipedia.org/wiki/UNESCO">UNESCO&lt;/a> called&lt;/em> Missing links in AI governance &lt;em>(&lt;a href="https://unesdoc.unesco.org/ark:/48223/pf0000384787">link&lt;/a>). There were news articles in French (&lt;a href="https://www.ledevoir.com/societe/785960/intelligence-artificielle-l-onu-et-le-mila-inquiets-des-derapages-potentiels-d-ia-comme-chatgpt">Le Devoir&lt;/a>), but not as many in English unfortunately (&lt;a href="https://www.datanami.com/this-just-in/mila-and-unesco-join-forces-to-emphasize-urgent-need-for-better-ai-governance/">Datanami&lt;/a>).&lt;/em>&lt;/p>
&lt;p>&lt;strong>Bengio&lt;/strong>: What motivates you to do what you do? This topic has turned from an academic move to a societal one really quickly, and it&amp;rsquo;s scary. We need to figure out what to do.&lt;/p></description></item><item><title>Army of none: autonomous weapons and the future of war</title><link>https://kylrth.com/book/army-of-none/</link><pubDate>Tue, 14 Feb 2023 13:33:19 -0500</pubDate><guid>https://kylrth.com/book/army-of-none/</guid><description>&lt;p>The examples in this book make it clear that there is no easy line we can draw between autonomous and non-autonomous weapons (and by extension, autonomous AI agents). There is a smooth gradient of autonomy, which makes the question of allowing autonomous weapons much more nuanced. It&amp;rsquo;s probably the case that higher-level alignment becomes important proportionally to the level of autonomy and intelligence.&lt;/p>
&lt;p>He analyzes the Patriot fratricides,&lt;span class="sidenote-number">&lt;small class="sidenote">In a military context, the word &lt;em>fratricide&lt;/em> means the killing of someone on the same side of a conflict.&lt;/small>&lt;/span>
and ends up blaming the individuals involved for automation bias. I would say that these humans in the system were set up to fail by the training and the functioning of the system. They&amp;rsquo;re expected to decide whether the computer is right, with only seconds to decide. He acknowledges this later when he talks about Aegis.&lt;/p></description></item><item><title>Artificial intelligence, values, and alignment</title><link>https://kylrth.com/paper/ai-values-alignment/</link><pubDate>Fri, 10 Feb 2023 08:09:15 -0500</pubDate><guid>https://kylrth.com/paper/ai-values-alignment/</guid><description>&lt;p>&lt;em>I presented this paper in Bang Liu&amp;rsquo;s research group meeting in two installments on 2023-02-20 and 2023-02-27, and also in Irina Rish&amp;rsquo;s scaling and alignment course (&lt;a href="https://sites.google.com/view/towards-agi-course/course-description">IFT6760A&lt;/a>) on 2023-03-07. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/1I4VPhMF32CDB3W3vWQl3TTy1GR5jxqSAAfSgjuk8enI/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;span class="sidenote-number">&lt;small class="sidenote">The thumbnail for this post was generated with stable diffusion! See the alt text for details.&lt;/small>&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>Behind each vision for ethically-aligned AI sits a deeper question. How are we to decide which principles or objectives to encode in AI—and who has the right to make these decisions—given that we live in a pluralistic world that is full of competing conceptions of value? Is there a way to think about AI value alignment that avoids a situation in which some people simply impose their views on others?&lt;/p></description></item><item><title>Unsolved problems in ML safety</title><link>https://kylrth.com/paper/unsolved-problems-ml-safety/</link><pubDate>Mon, 06 Feb 2023 11:39:33 -0500</pubDate><guid>https://kylrth.com/paper/unsolved-problems-ml-safety/</guid><description>&lt;p>&lt;em>This was a paper we presented about in Irina Rish&amp;rsquo;s neural scaling laws course (&lt;a href="https://sites.google.com/view/towards-agi-course">IFT6760A&lt;/a>) in winter 2023. You can view the slides we used &lt;a href="https://docs.google.com/presentation/d/11VtXg-sfLkjtIQWXEEEm875esK-8QrNFr09woMYzrV8/edit?usp=sharing">here&lt;/a>, and the recording &lt;a href="https://sites.google.com/view/towards-agi-course/schedule#h.lmlkbq72t3iz">here&lt;/a> (or my backup &lt;a href="https://dl.kylrth.com/videos/2023-02-02-ml-safety.mp4">here&lt;/a>).&lt;/em>&lt;/p></description></item><item><title>ethics drift within bubbles</title><link>https://kylrth.com/post/ethics-drift/</link><pubDate>Fri, 01 Apr 2022 08:35:54 -0400</pubDate><guid>https://kylrth.com/post/ethics-drift/</guid><description>&lt;p>Here are some snippets from a Lex Fridman &lt;a href="https://web.archive.org/web/20221222002108/https://lexfridman.com/john-abramson/">interview&lt;/a> with John Abramson, outspoken critic of Big Pharma.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Lex&lt;/strong>: Are people corrupt? Are people malevolent? Are people ignorant that work at the low level and at the high level, at Pfizer for example? How is this possible? I believe that most people are good, and I actually believe if you join Big Pharma your life trajectory often involves dreaming, wanting, and enjoying helping people. And then we look at the outcomes that you&amp;rsquo;re describing and that&amp;rsquo;s why the narrative takes hold that Pfizer CEO Albert Bourla is malevolent. The sense is that these companies are evil. So if the different parts are people that are good and they want to do good, how are we getting these outcomes?&lt;/p></description></item><item><title>Tools and weapons: the promise and peril of the digital age</title><link>https://kylrth.com/book/tools-and-weapons/</link><pubDate>Thu, 03 Dec 2020 20:58:02 -0700</pubDate><guid>https://kylrth.com/book/tools-and-weapons/</guid><description>&lt;p>&lt;em>I started taking notes later in the book. There were lots of good insights in the first half. Sorry!&lt;/em>&lt;/p>
&lt;h2 id="broadband-access">broadband access &lt;a class="header-link" href="#broadband-access">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>Getting the internet to rural communities is a big deal for the rural economy. Just like electricity, it&amp;rsquo;s something that needs government support because there isn&amp;rsquo;t the economic incentive for ISPs to reach some of these locations.&lt;/p>
&lt;h2 id="ethical-ai">ethical AI &lt;a class="header-link" href="#ethical-ai">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>The focus on AI now is not just a fad, but a convergence of several trends that have made AI the next logical step: the increased computational resources, flexible access to compute through the cloud, etc.&lt;/p></description></item><item><title>Weapons of math destruction: how big data increases inequality and threatens democracy</title><link>https://kylrth.com/book/weapons-of-math-destruction/</link><pubDate>Sun, 30 Aug 2020 06:46:49 -0600</pubDate><guid>https://kylrth.com/book/weapons-of-math-destruction/</guid><description>&lt;blockquote>
&lt;p>In fact, I saw all kinds of parallels between finance and Big Data. Both industries gobble up the same pool of talent, much of it from elite universities like MIT, Princeton, or Stanford. These new hires are ravenous for success and have been focused on external metrics&amp;ndash;like SAT scores and college admissions&amp;ndash;their entire lives. Whether in finance or tech, the message they&amp;rsquo;ve received is that they will be rich, that they will run the world. Their productivity indicates that they&amp;rsquo;re on the right track, and it translates into dollars. This leads to the fallacious conclusion that whatever they&amp;rsquo;re doing to bring in more money is good. It &amp;ldquo;adds value.&amp;rdquo; Otherwise, why would the market reward it?&lt;/p></description></item></channel></rss>