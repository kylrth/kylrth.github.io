<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Attention on Kyle Roth</title><link>https://kylrth.com/tags/attention/</link><description>Recent content in Attention on Kyle Roth</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 17 Apr 2025 13:19:47 -0400</lastBuildDate><atom:link href="https://kylrth.com/tags/attention/index.xml" rel="self" type="application/rss+xml"/><item><title>Attention is all you need</title><link>https://kylrth.com/paper/attention-all-you-need/</link><pubDate>Wed, 05 Aug 2020 12:37:42 -0700</pubDate><guid>https://kylrth.com/paper/attention-all-you-need/</guid><description>&lt;p>&lt;em>I also referred to &lt;a href="https://github.com/lilianweng/transformer-tensorflow">this implementation&lt;/a> to understand some of the details.&lt;/em>&lt;/p>
&lt;p>This is the paper describing the Transformer, a sequence-to-sequence model based entirely on attention. I think it&amp;rsquo;s best described with pictures.&lt;/p>
&lt;h2 id="model-overview">model overview &lt;a class="header-link" href="#model-overview">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;img src="transformer.png" alt="Attention is all you need transformer.png" class="img-zoomable">

&lt;p>From this picture, I think the following things need explaining:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>embeddings&lt;/strong> these are learned embeddings that convert the input and output tokens to vectors of the model dimension. In this paper, they actually used the same weight matrix for input embedding, output embedding, and the final linear layer before the final softmax.&lt;/li>
&lt;li>&lt;strong>positional encoding&lt;/strong>: since there&amp;rsquo;s no concept of a hidden state or convolution that encodes the order of the inputs, we have to add some information about the position of the tokens. They used a sinusoidal positional encoding that was a function of the position and the dimension. The wavelength for each dimension forms a geometric progression from &lt;code>\(2\pi\)&lt;/code> to 10000 times that.&lt;/li>
&lt;li>&lt;strong>the outputs are &amp;ldquo;shifted right&amp;rdquo;&lt;/strong>&lt;/li>
&lt;li>&lt;strong>multi-head attention&lt;/strong>: see below for a description of multi-head attention. In the encoder-decoder attention layers, &lt;code>\(Q\)&lt;/code> comes from the previous masked attention layer and &lt;code>\(K\)&lt;/code> and &lt;code>\(V\)&lt;/code> come from the output of the encoder. Everywhere else uses self-attention, meaning that &lt;code>\(Q\)&lt;/code>, &lt;code>\(K\)&lt;/code>, and &lt;code>\(V\)&lt;/code> are all the same.&lt;/li>
&lt;li>&lt;strong>&lt;em>masked&lt;/em> multi-head attention&lt;/strong>: in the self-attention layers in the decoder, we can&amp;rsquo;t allow positions to attend to positions ahead of themselves, so we set all right-connecting values in the input of the softmax (right after scaling; see the image below) to negative infinity.&lt;/li>
&lt;li>&lt;strong>feed-forward blocks&lt;/strong> these are two linear transformation with ReLU in between. The transformations are the same across each position, but they are different transformations from layer to layer, as you might expect.&lt;/li>
&lt;li>&lt;strong>add &amp;amp; norm&lt;/strong>: these are residual connections followed by layer normalization.&lt;/li>
&lt;/ol>
&lt;h3 id="multi-head-attention">multi-head attention &lt;a class="header-link" href="#multi-head-attention">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h3>
&lt;img src="multi-head-attention.png" alt="Attention is all you need multi-head-attention.png" class="img-zoomable">

&lt;p>The &amp;ldquo;Mask (opt.)&amp;rdquo; can be ignored because that&amp;rsquo;s for masked attention, described above.&lt;/p></description></item><item><title>BERT: pre-training of deep bidirectional transformers for language understanding</title><link>https://kylrth.com/paper/bert/</link><pubDate>Tue, 04 Aug 2020 08:57:44 -0700</pubDate><guid>https://kylrth.com/paper/bert/</guid><description>&lt;p>The B is for bidirectional, and that&amp;rsquo;s a big deal. It makes it possible to do well on sentence-level (NLI, question answering) and token-level tasks (NER, POS tagging). In a unidirectional model, the word &amp;ldquo;bank&amp;rdquo; in a sentence like &amp;ldquo;I made a bank deposit.&amp;rdquo; has only &amp;ldquo;I made a&amp;rdquo; as its context, keeping useful information from the model.&lt;/p>
&lt;p>Another cool thing is masked language model training (MLM). They train the model by blanking certain words in the sentence and asking the model to guess the missing word.&lt;/p></description></item><item><title>Google's neural machine translation system: bridging the gap between human and machine translation</title><link>https://kylrth.com/paper/google-nmt-2016/</link><pubDate>Tue, 30 Jun 2020 08:22:30 -0600</pubDate><guid>https://kylrth.com/paper/google-nmt-2016/</guid><description>&lt;p>&lt;em>This model was superseded by &lt;a href="https://kylrth.com/paper/google-zero-shot/">this one&lt;/a>.&lt;/em>&lt;/p>
&lt;p>They did some careful things with residual connections to make sure it was very parallelizable. They put each LSTM layer on a separate GPU. They quantized the models such that they could train using full floating-point computations with a couple restrictions and then convert the models to quantized versions.&lt;/p></description></item><item><title>Google's multilingual neural machine translation system</title><link>https://kylrth.com/paper/google-zero-shot/</link><pubDate>Fri, 26 Jun 2020 08:02:12 -0600</pubDate><guid>https://kylrth.com/paper/google-zero-shot/</guid><description>&lt;p>They use the word-piece model from &lt;a href="https://kylrth.com/paper/word-piece-model/">&amp;ldquo;Japanese and Korean Voice Search&amp;rdquo;&lt;/a>, with 32,000 word pieces. (This is a lot less than the 200,000 used in that paper.) They state in the paper that the shared word-piece model is very similar to Byte-Pair-Encoding, which was used for NMT in &lt;a href="https://www.aclweb.org/anthology/P16-1162.pdf">this paper&lt;/a> by researchers at U of Edinburgh.&lt;/p>
&lt;p>The model and training process are exactly as in &lt;a href="https://kylrth.com/paper/google-nmt-2016/">Google&amp;rsquo;s earlier paper&lt;/a>. It takes &lt;em>3 weeks&lt;/em> on &lt;em>100 GPUs&lt;/em> to train, even after increasing batch size and learning rate.&lt;/p></description></item></channel></rss>