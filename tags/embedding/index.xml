<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Embedding on Kyle Roth</title><link>https://kylrth.com/tags/embedding/</link><description>Recent content in Embedding on Kyle Roth</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 17 Apr 2025 13:19:47 -0400</lastBuildDate><atom:link href="https://kylrth.com/tags/embedding/index.xml" rel="self" type="application/rss+xml"/><item><title>Cross-lingual alignment of contextual word embeddings, with applications to zero-shot dependency parsing</title><link>https://kylrth.com/paper/cross-lingual-alignment-contextual/</link><pubDate>Fri, 11 Dec 2020 06:30:43 -0700</pubDate><guid>https://kylrth.com/paper/cross-lingual-alignment-contextual/</guid><description>&lt;p>Recent contextual word embeddings (e.g. &lt;a href="https://kylrth.com/paper/deep-contextualized-word-representations/">ELMo&lt;/a>) have shown to be much better than &amp;ldquo;static&amp;rdquo; embeddings (where there&amp;rsquo;s a one-to-one mapping from token to representation). This paper is exciting because they were able to create a multi-lingual embedding space that used &lt;em>contextual&lt;/em> word embeddings.&lt;/p>
&lt;p>Each token will have a &amp;ldquo;point cloud&amp;rdquo; of embedding values, one point for each context containing the token. They define the &lt;em>embedding anchor&lt;/em> as the average of all those points for a particular token. Here&amp;rsquo;s a figure from the paper that displays a two-dimensional PCA of the contextual representations for four Spanish words, along with their anchors:&lt;/p></description></item><item><title>Deep contextualized word representations</title><link>https://kylrth.com/paper/deep-contextualized-word-representations/</link><pubDate>Thu, 03 Dec 2020 12:01:43 -0700</pubDate><guid>https://kylrth.com/paper/deep-contextualized-word-representations/</guid><description>&lt;p>&lt;em>This is the original paper introducing Embeddings from Language Models (ELMo).&lt;/em>&lt;/p>
&lt;blockquote>
&lt;p>Unlike most widely used word embeddings, ELMo word representations are functions of the entire input sentence.&lt;/p>&lt;/blockquote>
&lt;p>That&amp;rsquo;s what makes ELMo great: they&amp;rsquo;re &lt;em>contextualized&lt;/em> word representations, meaning that they can express multiple possible senses of the same word.&lt;/p>
&lt;p>Specifically, ELMo representations are a learned linear combination of all layers of an LSTM encoding. The LSTM undergoes general semi-supervised pretraining, but the linear combination is learned &lt;em>specific to the task&lt;/em>. It&amp;rsquo;s been shown that initial layers in LSTM encoders are more representative of syntax, while later layers tend to represent semantics, so this linear combination is a key advantage that allows ELMo to improve accuracy on tasks ranging from POS tagging to question answering.&lt;/p></description></item></channel></rss>