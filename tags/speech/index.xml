<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Speech on Kyle Roth</title><link>https://kylrth.com/tags/speech/</link><description>Recent content in Speech on Kyle Roth</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 17 Apr 2025 13:19:47 -0400</lastBuildDate><atom:link href="https://kylrth.com/tags/speech/index.xml" rel="self" type="application/rss+xml"/><item><title>Whisper: robust speech recognition via large-scale weak supervision</title><link>https://kylrth.com/paper/whisper/</link><pubDate>Fri, 30 Sep 2022 09:46:29 -0400</pubDate><guid>https://kylrth.com/paper/whisper/</guid><description>&lt;p>&lt;em>This was a paper I presented about in Bang Liu&amp;rsquo;s research group meeting on 2022-09-30. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/1bGjzq0f2KEh49F9eyaNYz_VNlEQVJW0HKR61d48Y9fs/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>Semi-supervised training for automatic speech recognition</title><link>https://kylrth.com/paper/semi-supervised-for-asr/</link><pubDate>Tue, 14 Jul 2020 08:06:00 -0600</pubDate><guid>https://kylrth.com/paper/semi-supervised-for-asr/</guid><description>&lt;p>&lt;em>This was Manohar&amp;rsquo;s PhD dissertation at JHU.&lt;/em>&lt;/p>
&lt;p>Chapter 2 provides a relatively clear overview of how chain and non-chain models work in Kaldi.&lt;/p>
&lt;p>In chapter 3 he tried using negative conditional entropy as the loss function for the unsupervised data, and it helped a bit.&lt;/p>
&lt;p>In chapter 4 Manohar uses [CTC loss]/paper/ctc/.&lt;/p>
&lt;p>In chapter 5, he discusses ways to do semi-supervised model training. It&amp;rsquo;s nice when you have parallel data in different domains, because then you can do a student-teacher model. When there&amp;rsquo;s no parallel data, the best you can do is decode the unsupervised data with the seed model and use that to train the LF-MMI model (see section 5.2.1).&lt;/p></description></item><item><title>Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title><link>https://kylrth.com/paper/ctc/</link><pubDate>Fri, 10 Jul 2020 09:14:59 -0600</pubDate><guid>https://kylrth.com/paper/ctc/</guid><description>&lt;p>RNNs generally require pre-segmented training data, but this avoids that need.&lt;/p>
&lt;p>Basically, you have the RNN output probabilities for each label (or a blank) for every frame, and then you find the most likely path across that lattice of probabilities.&lt;/p>
&lt;p>The section explaining the loss function was kind of complicated. They used their forward-backward algorithm (sort of like Viterbi) to get the probability of all paths corresponding to the output that go through each symbol at each time, and then they differentiated that to get the derivatives with respect to the outputs. Then it was backpropagation as normal from that point.&lt;/p></description></item><item><title>Universal phone recognition with a multilingual allophone system</title><link>https://kylrth.com/paper/universal-phone-recognition/</link><pubDate>Tue, 23 Jun 2020 08:33:48 -0600</pubDate><guid>https://kylrth.com/paper/universal-phone-recognition/</guid><description>&lt;p>These guys made sure to model allophones. They had an encoder that produced a universal phone set, and then language-specific decoders. This meant they could use data from various languages to train the system. The decoder has an &lt;em>allophone layer&lt;/em>, followed by other dense trainable layers. The allophone layer is a single trainable dense layer, but was initialized by a bunch of linguists who sat down and described the phone sets belonging to each phoneme in each language present in the training set. They added an L2 penalty to divergence from the original linguist-created matrix.&lt;/p></description></item></channel></rss>