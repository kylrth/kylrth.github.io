<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>question-answering on Kyle Roth</title><link>https://kylrth.com/tags/question-answering/</link><description>Recent content in question-answering on Kyle Roth</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 05 Apr 2022 22:54:43 -0400</lastBuildDate><atom:link href="https://kylrth.com/tags/question-answering/index.xml" rel="self" type="application/rss+xml"/><item><title>QA-GNN: reasoning with language models and knowledge graphs for question answering</title><link>https://kylrth.com/paper/qa-gnn/</link><pubDate>Tue, 05 Apr 2022 22:54:43 -0400</pubDate><guid>https://kylrth.com/paper/qa-gnn/</guid><description>This post was created as an assignment in Bang Liu&amp;rsquo;s IFT6289 course in winter 2022. The structure of the post follows the structure of the assignment: summarization followed by my own comments.
The authors create a novel system for combining an LM and a knowledge graph by performing reasoning over a joint graph produced by the LM and the KG, thus solving the problem of irrelevant entities appearing in the knowledge graph and unifying the representations across the LM and KG.</description></item></channel></rss>