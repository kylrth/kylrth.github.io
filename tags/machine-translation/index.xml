<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine-Translation on Kyle Roth</title><link>https://kylrth.com/tags/machine-translation/</link><description>Recent content in Machine-Translation on Kyle Roth</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 17 Apr 2025 13:19:47 -0400</lastBuildDate><atom:link href="https://kylrth.com/tags/machine-translation/index.xml" rel="self" type="application/rss+xml"/><item><title>BERT: pre-training of deep bidirectional transformers for language understanding</title><link>https://kylrth.com/paper/bert/</link><pubDate>Tue, 04 Aug 2020 08:57:44 -0700</pubDate><guid>https://kylrth.com/paper/bert/</guid><description>&lt;p>The B is for bidirectional, and that&amp;rsquo;s a big deal. It makes it possible to do well on sentence-level (NLI, question answering) and token-level tasks (NER, POS tagging). In a unidirectional model, the word &amp;ldquo;bank&amp;rdquo; in a sentence like &amp;ldquo;I made a bank deposit.&amp;rdquo; has only &amp;ldquo;I made a&amp;rdquo; as its context, keeping useful information from the model.&lt;/p>
&lt;p>Another cool thing is masked language model training (MLM). They train the model by blanking certain words in the sentence and asking the model to guess the missing word.&lt;/p></description></item><item><title>Google's neural machine translation system: bridging the gap between human and machine translation</title><link>https://kylrth.com/paper/google-nmt-2016/</link><pubDate>Tue, 30 Jun 2020 08:22:30 -0600</pubDate><guid>https://kylrth.com/paper/google-nmt-2016/</guid><description>&lt;p>&lt;em>This model was superseded by &lt;a href="https://kylrth.com/paper/google-zero-shot/">this one&lt;/a>.&lt;/em>&lt;/p>
&lt;p>They did some careful things with residual connections to make sure it was very parallelizable. They put each LSTM layer on a separate GPU. They quantized the models such that they could train using full floating-point computations with a couple restrictions and then convert the models to quantized versions.&lt;/p></description></item><item><title>Google's multilingual neural machine translation system</title><link>https://kylrth.com/paper/google-zero-shot/</link><pubDate>Fri, 26 Jun 2020 08:02:12 -0600</pubDate><guid>https://kylrth.com/paper/google-zero-shot/</guid><description>&lt;p>They use the word-piece model from &lt;a href="https://kylrth.com/paper/word-piece-model/">&amp;ldquo;Japanese and Korean Voice Search&amp;rdquo;&lt;/a>, with 32,000 word pieces. (This is a lot less than the 200,000 used in that paper.) They state in the paper that the shared word-piece model is very similar to Byte-Pair-Encoding, which was used for NMT in &lt;a href="https://www.aclweb.org/anthology/P16-1162.pdf">this paper&lt;/a> by researchers at U of Edinburgh.&lt;/p>
&lt;p>The model and training process are exactly as in &lt;a href="https://kylrth.com/paper/google-nmt-2016/">Google&amp;rsquo;s earlier paper&lt;/a>. It takes &lt;em>3 weeks&lt;/em> on &lt;em>100 GPUs&lt;/em> to train, even after increasing batch size and learning rate.&lt;/p></description></item></channel></rss>