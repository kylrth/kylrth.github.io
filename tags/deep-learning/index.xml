<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep-Learning on Kyle Roth</title><link>https://kylrth.com/tags/deep-learning/</link><description>Recent content in Deep-Learning on Kyle Roth</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 17 Apr 2025 13:19:47 -0400</lastBuildDate><atom:link href="https://kylrth.com/tags/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>InstructEval: systematic evaluation of instruction selection methods</title><link>https://kylrth.com/paper/instructeval/</link><pubDate>Mon, 25 Sep 2023 11:38:24 -0400</pubDate><guid>https://kylrth.com/paper/instructeval/</guid><description>&lt;p>&lt;em>This was a paper I presented about in Bang Liu&amp;rsquo;s research group meeting on 2023-09-25. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/1Qo_KPNnkj2jQzYDKG1wSEisHAr18Fs_eHq5Iuzj0MNs/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>"Low-resource" text classification: a parameter-free classification method with compressors</title><link>https://kylrth.com/paper/gzip-text-classification/</link><pubDate>Mon, 24 Jul 2023 11:35:13 -0400</pubDate><guid>https://kylrth.com/paper/gzip-text-classification/</guid><description>&lt;p>&lt;em>I presented this paper in Bang Liu&amp;rsquo;s research group meeting on 2023-07-24. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/1w4n4UCWegJlDTCjKqWyTHOb3QHpnz1G2kTV6j_MODqY/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p>
&lt;p>It seems like the authors made a mistake that inflated the scores for the multilingual experiments, &lt;a href="https://kenschutte.com/gzip-knn-paper/">according to Ken Schutte&lt;/a>.&lt;/p></description></item><item><title>the AI art debate</title><link>https://kylrth.com/post/ai-art/</link><pubDate>Wed, 08 Mar 2023 12:09:25 -0500</pubDate><guid>https://kylrth.com/post/ai-art/</guid><description>&lt;p>On 2022-11-03 a &lt;a href="https://githubcopilotlitigation.com/">class action lawsuit&lt;/a> was announced against GitHub Copilot on the basis of copyright infringement, and now (2023-01-13) there&amp;rsquo;s &lt;a href="https://web.archive.org/web/20230114170709/https://stablediffusionlitigation.com/">one&lt;/a> for stable diffusion (against StabilityAI and friends). Browsing through r/StableDiffusion, I&amp;rsquo;m seeing lots of posts like &lt;a href="https://www.reddit.com/r/StableDiffusion/comments/10e13r3/the_lawyers_suing_stable_diffusion_when_you_tell/">this&lt;/a> making the very memeable point that 5 billion images can&amp;rsquo;t be stored in a 4 GB model. &lt;a href="https://www.reddit.com/r/StableDiffusion/comments/10e13r3/the_lawyers_suing_stable_diffusion_when_you_tell/j4oeny6/?context=3">From the original poster&lt;/a>:&lt;span class="sidenote-number">&lt;small class="sidenote">The thumbnail for this post was generated with stable diffusion. See the alt text for details. Yes, I&amp;rsquo;m not great at this.&lt;/small>&lt;/span>&lt;/p></description></item><item><title>Artificial intelligence, values, and alignment</title><link>https://kylrth.com/paper/ai-values-alignment/</link><pubDate>Fri, 10 Feb 2023 08:09:15 -0500</pubDate><guid>https://kylrth.com/paper/ai-values-alignment/</guid><description>&lt;p>&lt;em>I presented this paper in Bang Liu&amp;rsquo;s research group meeting in two installments on 2023-02-20 and 2023-02-27, and also in Irina Rish&amp;rsquo;s scaling and alignment course (&lt;a href="https://sites.google.com/view/towards-agi-course/course-description">IFT6760A&lt;/a>) on 2023-03-07. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/1I4VPhMF32CDB3W3vWQl3TTy1GR5jxqSAAfSgjuk8enI/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;span class="sidenote-number">&lt;small class="sidenote">The thumbnail for this post was generated with stable diffusion! See the alt text for details.&lt;/small>&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>Behind each vision for ethically-aligned AI sits a deeper question. How are we to decide which principles or objectives to encode in AI—and who has the right to make these decisions—given that we live in a pluralistic world that is full of competing conceptions of value? Is there a way to think about AI value alignment that avoids a situation in which some people simply impose their views on others?&lt;/p></description></item><item><title>Unsolved problems in ML safety</title><link>https://kylrth.com/paper/unsolved-problems-ml-safety/</link><pubDate>Mon, 06 Feb 2023 11:39:33 -0500</pubDate><guid>https://kylrth.com/paper/unsolved-problems-ml-safety/</guid><description>&lt;p>&lt;em>This was a paper we presented about in Irina Rish&amp;rsquo;s neural scaling laws course (&lt;a href="https://sites.google.com/view/towards-agi-course">IFT6760A&lt;/a>) in winter 2023. You can view the slides we used &lt;a href="https://docs.google.com/presentation/d/11VtXg-sfLkjtIQWXEEEm875esK-8QrNFr09woMYzrV8/edit?usp=sharing">here&lt;/a>, and the recording &lt;a href="https://sites.google.com/view/towards-agi-course/schedule#h.lmlkbq72t3iz">here&lt;/a> (or my backup &lt;a href="https://dl.kylrth.com/videos/2023-02-02-ml-safety.mp4">here&lt;/a>).&lt;/em>&lt;/p></description></item><item><title>AlphaTensor: discovering faster matrix multiplication algorithms with reinforcement learning</title><link>https://kylrth.com/paper/alphatensor/</link><pubDate>Fri, 21 Oct 2022 14:18:00 -0400</pubDate><guid>https://kylrth.com/paper/alphatensor/</guid><description>&lt;p>&lt;em>This was a paper I presented about in Bang Liu&amp;rsquo;s research group meeting on 2022-10-21. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/1iei7rogURT0GFU_jPIktMat-Cusb0KJciW-WLRANfzc/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>Whisper: robust speech recognition via large-scale weak supervision</title><link>https://kylrth.com/paper/whisper/</link><pubDate>Fri, 30 Sep 2022 09:46:29 -0400</pubDate><guid>https://kylrth.com/paper/whisper/</guid><description>&lt;p>&lt;em>This was a paper I presented about in Bang Liu&amp;rsquo;s research group meeting on 2022-09-30. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/1bGjzq0f2KEh49F9eyaNYz_VNlEQVJW0HKR61d48Y9fs/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>Selective annotation makes language models better few-shot learners</title><link>https://kylrth.com/paper/selective-annotation/</link><pubDate>Tue, 13 Sep 2022 11:26:21 -0400</pubDate><guid>https://kylrth.com/paper/selective-annotation/</guid><description>&lt;p>Selective annotation chooses a pool of samples to annotate from a large set of unlabeled data. The main result of the paper is that when this is combined with item-specific prompt retrieval the performance drastically improves (&amp;gt;10% relative gain and lower performance variance). Interestingly, selective annotation does &lt;em>not&lt;/em> help for finetuning, or when the prompts are randomly selected. They call their selective annotation method &amp;ldquo;vote-&lt;code>\(k\)&lt;/code>&amp;rdquo;.&lt;/p>
&lt;h2 id="selective-annotation-method">selective annotation method &lt;a class="header-link" href="#selective-annotation-method">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>Vote-&lt;code>\(k\)&lt;/code> essentially creates a network of similar&lt;span class="sidenote-number">&lt;small class="sidenote">according to Sentence-BERT&lt;/small>&lt;/span>
unlabeled instances, and then selects from them with a network importance score that is discounted to promote diversity&lt;span class="sidenote-number">&lt;small class="sidenote">The discounting is performed by iteratively adding to the selection set, each time penalizing new nodes for being close to nodes that are already in the selection set.&lt;/small>&lt;/span>
.&lt;/p></description></item><item><title>Trivial or impossible—dichotomous data difficulty masks model differences (on ImageNet and beyond)</title><link>https://kylrth.com/paper/dichotomous-data-difficulty/</link><pubDate>Mon, 15 Aug 2022 14:21:56 -0400</pubDate><guid>https://kylrth.com/paper/dichotomous-data-difficulty/</guid><description>&lt;blockquote>
&lt;p>We observe that 48.2% [of] images [in ImageNet] are learned by all models regardless of their inductive bias; 14.3% [of] images are consistently misclassified by all models; only roughly a third (37.5%) of images are responsible for the differences between two models&amp;rsquo; decisions. We call this phenomenon dichotomous data difficulty (DDD).&lt;/p>&lt;/blockquote>
&lt;p>The authors varied hyperparameters, optimizers, architectures, supervision modes, and sampling methods, finding that models only varied in performance on about a third of the images in the dataset. And this isn&amp;rsquo;t specific to ImageNet; they found similar results for CIFAR-100 and a synthetic Gaussian dataset. They use this measure to divide the dataset into &amp;ldquo;trivials&amp;rdquo;, &amp;ldquo;impossibles&amp;rdquo;, and &amp;ldquo;in-betweens&amp;rdquo;.&lt;/p></description></item><item><title>Beyond neural scaling laws: beating power law scaling via data pruning</title><link>https://kylrth.com/paper/beyond_nsl/</link><pubDate>Thu, 11 Aug 2022 14:09:08 -0400</pubDate><guid>https://kylrth.com/paper/beyond_nsl/</guid><description>&lt;p>In this paper they show that we can achieve exponential performance scaling over dataset size, when the samples added are pruned to be only the best examples. This beats power law scaling in a big way. There is still no free lunch, in some sense, because in most cases it will become progressively harder to add new useful samples as the dataset gets bigger. But this is a big deal for computation, because it means that the number of samples in the dataset is not nearly as important as the coverage and quality that the dataset provides.&lt;span class="sidenote-number">&lt;small class="sidenote">This also means that scaling laws for &lt;em>compute&lt;/em> (usually expressed as a function of dataset and model size) are dataset-specific and not generalizable, because of how much sample quality affects data scaling.&lt;/small>&lt;/span>&lt;/p></description></item><item><title>LocoProp: enhancing backprop via local loss optimization</title><link>https://kylrth.com/paper/locoprop/</link><pubDate>Fri, 05 Aug 2022 09:52:06 -0400</pubDate><guid>https://kylrth.com/paper/locoprop/</guid><description>&lt;p>&lt;em>This was a paper I presented about in Bang Liu&amp;rsquo;s research group meeting on 2022-08-05. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/10f-vVG9yLaHHfZmGvKb2roaTkOVE6flbHv1yE5pkxX4/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>the effects of scale on worst-group performance</title><link>https://kylrth.com/post/worst-group-scale/</link><pubDate>Mon, 18 Jul 2022 15:31:12 -0400</pubDate><guid>https://kylrth.com/post/worst-group-scale/</guid><description>&lt;p>I think it&amp;rsquo;s valuable to be working in the open whenever possible, so I&amp;rsquo;m going to keep my research notes here. These notes will hopefully be full of good (and bad) ideas, so if someone borrows a good idea and publishes on it, that&amp;rsquo;s great!&lt;/p>
&lt;p>This post contains my research notes as I try to understand how model scaling affects worst-group performance. This started as a group project in the neural scaling laws course at Mila in winter 2022. We presented about an existing &lt;a href="https://kylrth.com/paper/effect-of-model-size-on-worst-group-generalization/">paper&lt;/a> and presented our preliminary results &lt;a href="https://sites.google.com/view/nsl-course/schedule#h.o7ntdr3dzoiv">in class&lt;/a>. The repository for this project is &lt;a href="https://github.com/kylrth/worst_group_scale/">here&lt;/a>.&lt;/p></description></item><item><title>Continual-T0: progressively instructing 50+ tasks to language models without forgetting</title><link>https://kylrth.com/paper/continual-t0/</link><pubDate>Thu, 02 Jun 2022 15:28:55 -0400</pubDate><guid>https://kylrth.com/paper/continual-t0/</guid><description>&lt;p>&lt;em>This was a paper I presented about in Bang Liu&amp;rsquo;s research group meeting on 2022-06-06. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/1-L5TnQvh-4WQHRSlIU-gcCyzudFxzZC0ur7vtLS28gs/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p>
&lt;p>Continual-T0 (CT0) extends &lt;a href="https://kylrth.com/paper/t0/">T0&lt;/a> by progressively training it on 8 unseen language generation tasks, while retaining a replay buffer of 1% of the original training data to preserve performance. The result is a model that maintains nearly all of its performance on previous tasks while learning the new tasks. In addition, CT0 maintains the original T0&amp;rsquo;s performance on unseen tasks (which is a big deal because those tasks could not appear in the replay buffer) and it extends the compositionality of T0 to even more unseen tasks.&lt;/p></description></item><item><title>Multitask prompted training enables zero-shot task generalization (T0)</title><link>https://kylrth.com/paper/t0/</link><pubDate>Fri, 27 May 2022 17:05:02 -0400</pubDate><guid>https://kylrth.com/paper/t0/</guid><description>&lt;p>T0 builds on T5 by fine-tuning on more natural prompts and testing the model&amp;rsquo;s generalization to held-out tasks.&lt;/p>
&lt;p>Compare the training format diagrams for T5 (top) and T0 (bottom):&lt;/p>
&lt;p>&lt;img src="t5.png" alt="Multitask prompted training enables zero-shot task generalization (T0) t5.png" class="img-zoomable">

&lt;br />&lt;br />&lt;br />
&lt;img src="prompt_format.png" alt="Multitask prompted training enables zero-shot task generalization (T0) prompt_format.png" class="img-zoomable">
&lt;/p>
&lt;p>Intuitively, the T0 prompts are more likely to be similar to implicit/explicit prompting that&amp;rsquo;s present in the pretraining data. The authors created several prompts for each dataset.&lt;/p></description></item><item><title>PaLM</title><link>https://kylrth.com/paper/palm/</link><pubDate>Mon, 11 Apr 2022 12:17:25 -0400</pubDate><guid>https://kylrth.com/paper/palm/</guid><description>&lt;p>&lt;em>This was a paper I presented about in Bang Liu&amp;rsquo;s research group meeting on 2022-04-11. You can view the slides I used &lt;a href="https://docs.google.com/presentation/d/19TWSV9rACztA2Umw6VU2Bo61EwWycjyx-AoNlFdsk64/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>QA-GNN: reasoning with language models and knowledge graphs for question answering</title><link>https://kylrth.com/paper/qa-gnn/</link><pubDate>Tue, 05 Apr 2022 22:54:43 -0400</pubDate><guid>https://kylrth.com/paper/qa-gnn/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Bang Liu&amp;rsquo;s &lt;a href="https://www-labs.iro.umontreal.ca/~liubang/IFT%206289%20-%20Winter%202022.htm">IFT6289&lt;/a> course in winter 2022. The structure of the post follows the structure of the assignment: summarization followed by my own comments.&lt;/em>&lt;/p>
&lt;p>The authors create a novel system for combining an LM and a knowledge graph by performing reasoning over a joint graph produced by the LM and the KG, thus solving the problem of irrelevant entities appearing in the knowledge graph and unifying the representations across the LM and KG.&lt;/p></description></item><item><title>Neural message passing for quantum chemistry</title><link>https://kylrth.com/paper/neural-message-passing/</link><pubDate>Fri, 25 Mar 2022 14:46:11 -0400</pubDate><guid>https://kylrth.com/paper/neural-message-passing/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Bang Liu&amp;rsquo;s &lt;a href="https://www-labs.iro.umontreal.ca/~liubang/IFT%206289%20-%20Winter%202022.htm">IFT6289&lt;/a> course in winter 2022. The structure of the post follows the structure of the assignment: summarization followed by my own comments.&lt;/em>&lt;/p>
&lt;p>To summarize, the authors create a unifying framework for describing message-passing neural networks, which they apply to the problem of predicting the structural properties of chemical compounds in the QM9 dataset.&lt;/p>
&lt;h2 id="paper-summarization">paper summarization &lt;a class="header-link" href="#paper-summarization">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>The authors first demonstrate that many of the recent works applying neural nets to this problem can fit into a message-passing neural network (MPNN) framework. Under the MPNN framework, at each time step &lt;code>\(t\)&lt;/code> a message is computed for each vertex by summing the output of a learned function &lt;code>\(M_t\)&lt;/code> over the vertex and all edges and vertices connected to it. Then the next state for each vertex is a learned function &lt;code>\(U_t\)&lt;/code> of the previous state and the message. Finally, the &amp;ldquo;readout&amp;rdquo; function &lt;code>\(R\)&lt;/code> is applied to all the vertices to compute the result.&lt;/p></description></item><item><title>The effect of model size on worst-group generalization</title><link>https://kylrth.com/paper/effect-of-model-size-on-worst-group-generalization/</link><pubDate>Thu, 17 Mar 2022 14:34:33 -0400</pubDate><guid>https://kylrth.com/paper/effect-of-model-size-on-worst-group-generalization/</guid><description>&lt;p>&lt;em>This was a paper we presented about in Irina Rish&amp;rsquo;s neural scaling laws course (&lt;a href="https://sites.google.com/view/nsl-course">IFT6167&lt;/a>) in winter 2022. You can view the slides we used &lt;a href="https://docs.google.com/presentation/d/1Fxs60aXvANsBj_k-m1h_KtjDfUrqPKY8f1UIh3j4XY0/edit?usp=sharing">here&lt;/a>, and the recording &lt;a href="https://sites.google.com/view/nsl-course/schedule#h.h6n8wkndidtb">here&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>Scaling laws for the few-shot adaptation of pre-trained image classifiers</title><link>https://kylrth.com/paper/scaling-laws-few-shot-image-classifiers/</link><pubDate>Tue, 22 Feb 2022 13:19:12 -0500</pubDate><guid>https://kylrth.com/paper/scaling-laws-few-shot-image-classifiers/</guid><description>&lt;p>The unsurprising result here is that few-shot performance scales predictably with pre-training dataset size under traditional fine-tuning, matching network, and prototypical network approaches.&lt;/p>
&lt;p>The interesting result is that the exponents of these three approaches were substantially different (see Table 1 in the paper), which says to me that the few-shot inference approach matters a lot.&lt;/p>
&lt;p>The surprising result was that while more training on the &amp;ldquo;non-natural&amp;rdquo; &lt;a href="https://github.com/brendenlake/omniglot">Omniglot&lt;/a> dataset did not improve few-shot accuracy on other datasets, training on &amp;ldquo;natural&amp;rdquo; datasets &lt;em>did&lt;/em> improve accuracy on few-shot Omniglot.&lt;/p></description></item><item><title>Learning explanations that are hard to vary</title><link>https://kylrth.com/paper/learning-explanations-hard-to-vary/</link><pubDate>Tue, 22 Feb 2022 12:29:17 -0500</pubDate><guid>https://kylrth.com/paper/learning-explanations-hard-to-vary/</guid><description>&lt;p>The big idea here is to use the geometric mean instead of the arithmetic mean across samples in the batch when computing the gradient for SGD. This overcomes the situation where averaging produces optima that are not actually optimal for any individual samples, as demonstrated in their toy example below:&lt;/p>
&lt;img src="example.png" alt="Learning explanations that are hard to vary example.png" class="img-zoomable">

&lt;p>In practice, the method the authors test is not exactly the geometric mean for numerical and performance reasons, but effectively accomplishes the same thing by avoiding optima that are &amp;ldquo;inconsistent&amp;rdquo; (meaning that gradients from relatively few samples actually point in that direction).&lt;/p></description></item><item><title>In search of robust measures of generalization</title><link>https://kylrth.com/paper/robust-measures-of-generalization/</link><pubDate>Mon, 21 Feb 2022 15:33:22 -0500</pubDate><guid>https://kylrth.com/paper/robust-measures-of-generalization/</guid><description>&lt;p>These authors define &lt;em>robust error&lt;/em> as the least upper bound on the expected loss over a family of environmental settings (including dataset, model architecture, learning algorithm, etc.):&lt;/p>
&lt;p>&lt;code>\[\sup_{e\in\mathcal F}\mathbb E_{\omega\in P^e}\left[\ell(\phi,\omega)\right]\]&lt;/code>&lt;/p>
&lt;p>The fact that this is an &lt;strong>upper bound&lt;/strong> and not an average is very important and is what makes this work unique from previous work in this direction. Indeed, what we should be concerned about is not how poorly a model performs on the &lt;em>average&lt;/em> sample but on the &lt;em>worst-case&lt;/em> sample.&lt;/p></description></item><item><title>It's not just size that matters: small language models are also few-shot learners</title><link>https://kylrth.com/paper/not-just-size-that-matters/</link><pubDate>Fri, 18 Feb 2022 13:13:54 -0500</pubDate><guid>https://kylrth.com/paper/not-just-size-that-matters/</guid><description>&lt;p>&lt;em>We presented this paper as a mini-lecture in Bang Liu&amp;rsquo;s &lt;a href="https://www-labs.iro.umontreal.ca/~liubang/IFT%206289%20-%20Winter%202022.htm">IFT6289&lt;/a> course in winter 2022. You can view the slides we used &lt;a href="https://docs.google.com/presentation/d/1XPRSLC24AQK0MeZY5zww4gZ0t0B_gqDl8eztKZpt_v8/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>Scaling laws for transfer</title><link>https://kylrth.com/paper/scaling-laws-for-transfer/</link><pubDate>Wed, 16 Feb 2022 14:12:26 -0500</pubDate><guid>https://kylrth.com/paper/scaling-laws-for-transfer/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Irina Rish&amp;rsquo;s neural scaling laws course (&lt;a href="https://sites.google.com/view/nsl-course">IFT6167&lt;/a>) in winter 2022. The post contains no summarization, only questions and thoughts.&lt;/em>&lt;/p>
&lt;p>Sometimes these scaling laws can feel like pseudoscience because they&amp;rsquo;re a post hoc attempt to place a trend line on data. How can we be confident that the trends we observe actually reflect the scaling laws that we&amp;rsquo;re after? In the limitations section they mention that they didn&amp;rsquo;t tune hyperparameters for fine-tuning or for the code data distribution. How can we know that a confounding hyperparameter is not responsible for the trend we see? I wonder if we aren&amp;rsquo;t really being statistically rigorous until we can predict generalization error on an unseen &lt;em>training setup&lt;/em>, rather than just an unseen model size/dataset size.&lt;/p></description></item><item><title>Deep learning scaling is predictable, empirically</title><link>https://kylrth.com/paper/scaling-predictable-empirically/</link><pubDate>Mon, 14 Feb 2022 10:38:11 -0500</pubDate><guid>https://kylrth.com/paper/scaling-predictable-empirically/</guid><description>&lt;p>&lt;em>This was a paper we presented about in Irina Rish&amp;rsquo;s neural scaling laws course (&lt;a href="https://sites.google.com/view/nsl-course">IFT6167&lt;/a>) in winter 2022. You can view the slides we used &lt;a href="https://docs.google.com/presentation/d/1e0SXonZiW6o8VyqXTnjyYlMs97YCcntBznaoiBwlWFE/edit?usp=sharing">here&lt;/a>.&lt;/em>&lt;/p>
&lt;p>It&amp;rsquo;s important to note that in the results for NMT (Figure 1) we would expect the lines in the graph on the left to curve as the capacity of the individual models is exhausted. That&amp;rsquo;s why the authors fit the curves with an extra constant added. Meanwhile, the results in the graph on the right are curved because as the data size grows, the optimal model size also grows and it becomes increasingly difficult to find the right hyperparameters to train the model down to the optimal generalization error. (See the last paragraph in Section 4.1.)&lt;/p></description></item><item><title>Masked autoencoders are scalable vision learners</title><link>https://kylrth.com/paper/masked-autoencoders-are-scalable-vision-learners/</link><pubDate>Fri, 11 Feb 2022 14:18:30 -0500</pubDate><guid>https://kylrth.com/paper/masked-autoencoders-are-scalable-vision-learners/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Irina Rish&amp;rsquo;s neural scaling laws course (&lt;a href="https://sites.google.com/view/nsl-course">IFT6167&lt;/a>) in winter 2022. The post contains no summarization, only questions and thoughts.&lt;/em>&lt;/p>
&lt;p>In this paper they mention that the mask vector is learned, and it sounds like the positional embeddings are also learned. I remember in &lt;a href="https://kylrth.com/paper/attention-all-you-need/">&lt;em>Attention is all you need&lt;/em>&lt;/a> they found that cosine positional embeddings worked better than learned ones, especially for sequences of longer length. But now it seems like most papers are doing learned embeddings. If anyone knows why, send me an email.&lt;/p></description></item><item><title>Data scaling laws in NMT: the effect of noise and architecture</title><link>https://kylrth.com/paper/data-scaling-laws-nmt/</link><pubDate>Wed, 09 Feb 2022 20:47:59 -0500</pubDate><guid>https://kylrth.com/paper/data-scaling-laws-nmt/</guid><description>&lt;p>This paper is all about trying a bunch of different changes to the training setup to see what affects the power law exponent &lt;strong>over dataset size&lt;/strong>. Here are some of the answers:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>encoder-decoder size asymmetry&lt;/strong>: exponent not affected, but effective model capacity affected&lt;/li>
&lt;li>&lt;strong>architecture (LSTM vs. Transformer)&lt;/strong>: exponent not affected, but effective model capacity affected&lt;/li>
&lt;li>&lt;strong>dataset quality (filtered vs. not)&lt;/strong>: exponent and effective model capacity not effected, losses on smaller datasets affected&lt;/li>
&lt;li>&lt;strong>dataset source (ParaCrawl vs. in-house dataset)&lt;/strong>: exponent not affected&lt;/li>
&lt;li>&lt;strong>adding independent noise&lt;/strong>: exponent not affected, but effective model capacity affected&lt;/li>
&lt;/ul>
&lt;p>Here are some other things to test that I thought of while I read this:&lt;/p></description></item><item><title>Parallel training of deep networks with local updates</title><link>https://kylrth.com/paper/parallel-training-with-local-updates/</link><pubDate>Wed, 09 Feb 2022 10:50:21 -0500</pubDate><guid>https://kylrth.com/paper/parallel-training-with-local-updates/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Irina Rish&amp;rsquo;s neural scaling laws course (&lt;a href="https://sites.google.com/view/nsl-course">IFT6167&lt;/a>) in winter 2022. The post contains no summarization, only questions and thoughts.&lt;/em>&lt;/p>
&lt;p>Once I learned how the loss functions worked for each chunk, my first question was whether the earlier chunks were going to be able to learn the low-level features that later chunks would need. Figure 7 seems to show that they do, although their quality apparently decreases with increasingly local updates.&lt;/p></description></item><item><title>A sensitivity analysis of (and practitioners’ guide to) convolutional neural networks for sentence classification</title><link>https://kylrth.com/paper/cnn-sentence/</link><pubDate>Wed, 02 Feb 2022 15:35:00 -0500</pubDate><guid>https://kylrth.com/paper/cnn-sentence/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Bang Liu&amp;rsquo;s &lt;a href="https://www-labs.iro.umontreal.ca/~liubang/IFT%206289%20-%20Winter%202022.htm">IFT6289&lt;/a> course in winter 2022. The structure of the post follows the structure of the assignment: summarization followed by my own comments.&lt;/em>&lt;/p>
&lt;h2 id="paper-summarization">paper summarization &lt;a class="header-link" href="#paper-summarization">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>Word embeddings have gotten so good that state-of-the-art sentence classification can often be achieved with just a one-layer convolutional network on top of those embeddings. This paper dials in on the specifics of training that convolutional layer for this downstream sentence classification task.&lt;/p></description></item><item><title>Learning transferable visual models from natural language supervision (CLIP)</title><link>https://kylrth.com/paper/clip/</link><pubDate>Wed, 02 Feb 2022 12:35:03 -0500</pubDate><guid>https://kylrth.com/paper/clip/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Irina Rish&amp;rsquo;s neural scaling laws course (&lt;a href="https://sites.google.com/view/nsl-course">IFT6167&lt;/a>) in winter 2022. The post contains no summarization, only questions and thoughts.&lt;/em>&lt;/p>
&lt;p>This concept of wide vs. narrow supervision (rather than binary &amp;ldquo;supervised&amp;rdquo; and &amp;ldquo;unsupervised&amp;rdquo;) is an interesting and flexible way to think about the way these training schemes leverage data.&lt;/p>
&lt;p>The zero-shot CLIP matches the performance of 4-shot CLIP, which is a surprising result. What do the authors mean when they make this guess about zero-shot&amp;rsquo;s advantage:&lt;/p></description></item><item><title>Distributed representations of words and phrases and their compositionality</title><link>https://kylrth.com/paper/distributed-representations/</link><pubDate>Tue, 01 Feb 2022 16:09:19 -0500</pubDate><guid>https://kylrth.com/paper/distributed-representations/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Bang Liu&amp;rsquo;s &lt;a href="https://www-labs.iro.umontreal.ca/~liubang/IFT%206289%20-%20Winter%202022.htm">IFT6289&lt;/a> course in winter 2022. The structure of the post follows the structure of the assignment: summarization followed by my own comments.&lt;/em>&lt;/p>
&lt;h2 id="paper-summarization">paper summarization &lt;a class="header-link" href="#paper-summarization">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>This paper describes multiple improvements that are made to the original &lt;a href="https://arxiv.org/abs/1301.3781">Skip-gram&lt;/a> model:&lt;/p>
&lt;ol>
&lt;li>Decreasing the rate of exposure to common words improves the training speed and increases the model&amp;rsquo;s accuracy on infrequent words.&lt;/li>
&lt;li>A new training target they call &amp;ldquo;negative sampling&amp;rdquo; improves the training speed and the model&amp;rsquo;s accuracy on frequent words.&lt;/li>
&lt;li>Allowing the model to use phrase vectors improves the expressivity of the model.&lt;/li>
&lt;/ol>
&lt;h3 id="negative-sampling">negative sampling &lt;a class="header-link" href="#negative-sampling">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h3>
&lt;p>The original Skip-gram model computed probabilities using a hierarchical softmax, which allowed the model to compute only &lt;code>\(O(\log_2(|V|))\)&lt;/code> probabilities when estimating the probability of a particular word, rather than &lt;code>\(O(|V|)\)&lt;/code>. Negative sampling, on the other hand, deals directly with the generated vector representations. The negative sampling loss function basically tries to maximize cosine similarity between the input representation of the input word with the output representation of the neighboring word, while decreasing cosine similarity between the input word and a few random vectors. They find that the required number of negative examples decreases as the dataset size increases.&lt;/p></description></item><item><title>Deep learning</title><link>https://kylrth.com/paper/deep-learning/</link><pubDate>Thu, 20 Jan 2022 15:11:00 -0500</pubDate><guid>https://kylrth.com/paper/deep-learning/</guid><description>&lt;p>&lt;em>This post was created as an assignment in Bang Liu&amp;rsquo;s &lt;a href="https://www-labs.iro.umontreal.ca/~liubang/IFT%206289%20-%20Winter%202022.htm">IFT6289&lt;/a> course in winter 2022. The structure of the post follows the structure of the assignment: summarization followed by my own comments.&lt;/em>&lt;/p>
&lt;h2 id="paper-summarization">paper summarization &lt;a class="header-link" href="#paper-summarization">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>The authors use the example of distinguishing between a Samoyed and a white wolf to talk about the importance of learning to rely on very small variations while ignoring others. While shallow classifiers must rely on human-crafted features which are expensive to build and always imperfect, deep classifiers are expected to learn their own features by applying a &amp;ldquo;general-purpose learning procedure&amp;rdquo; to learn the features and the classification layer from the data simultaneously.&lt;/p></description></item><item><title>avatarify</title><link>https://kylrth.com/post/avatarify/</link><pubDate>Wed, 24 Nov 2021 11:58:34 -0500</pubDate><guid>https://kylrth.com/post/avatarify/</guid><description>&lt;p>&lt;a href="https://github.com/alievk/avatarify-python">Avatarify&lt;/a> is a cool project that lets you create a relatively realistic avatar that you can use during video meetings. It works by creating a fake video input device and passing your video input through a neural network in PyTorch. My laptop doesn&amp;rsquo;t have a GPU, so I used the server/client setup.&lt;/p>
&lt;h2 id="setting-up-the-server">setting up the server &lt;a class="header-link" href="#setting-up-the-server">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>Be sure you&amp;rsquo;ve installed the Nvidia Docker runtime so that the Docker container can use the GPU. You can see how I did that &lt;a href="https://kylrth.com/post/jupyter-lab/">here&lt;/a>. Run the following on the server:&lt;/p></description></item><item><title>Inductive biases for deep learning of higher-level cognition</title><link>https://kylrth.com/paper/inductive-biases-higher-cognition/</link><pubDate>Tue, 08 Dec 2020 06:40:48 -0700</pubDate><guid>https://kylrth.com/paper/inductive-biases-higher-cognition/</guid><description>&lt;p>&lt;em>This is a long paper, so a lot of my writing here is an attempt to condense the discussion. I&amp;rsquo;ve taken the liberty to pull exact phrases and structure from the paper without explicitly using quotes.&lt;/em>&lt;/p>
&lt;blockquote>
&lt;p>Our main hypothesis is that deep learning succeeded in part because of a set of inductive biases, but that additional ones should be added in order to go from good in-distribution generalization in highly supervised learning tasks (or where strong and dense rewards are available), such as object recognition in images, to strong out-of-distribution generalization and transfer learning to new tasks with low sample complexity.&lt;/p></description></item><item><title>A closer look at memorization in deep networks</title><link>https://kylrth.com/paper/closer-look-at-memorization/</link><pubDate>Mon, 31 Aug 2020 11:52:35 -0600</pubDate><guid>https://kylrth.com/paper/closer-look-at-memorization/</guid><description>&lt;p>This paper builds on what we learned in &lt;a href="https://kylrth.com/paper/understanding-requires-rethinking-generalization/">&amp;ldquo;Understanding deep learning requires rethinking generalization&amp;rdquo;&lt;/a>. In that paper they showed that DNNs are able to fit pure noise in the same amount of time as it can fit real data, which means that our optimization algorithm (SGD, Adam, etc.) is not what&amp;rsquo;s keeping DNNs from overfitting.&lt;/p>
&lt;h2 id="experiments-for-detecting-easyhard-samples">experiments for detecting easy/hard samples &lt;a class="header-link" href="#experiments-for-detecting-easyhard-samples">&lt;svg class="c-links__icon">&lt;title>permalink&lt;/title>&lt;use xlink:href="#icon-permalink">&lt;/use>&lt;/svg>&lt;/a>&lt;/h2>
&lt;p>It looks like there are qualitative differences between a DNN that has memorized some data and a DNN that has seen real data. In experiments they found that real datasets contain &amp;ldquo;easy examples&amp;rdquo; that are more quickly learned than the hard examples. This is not the case for random data.&lt;/p></description></item></channel></rss>