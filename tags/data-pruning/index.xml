<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Data-Pruning on Kyle Roth</title><link>https://kylrth.com/tags/data-pruning/</link><description>Recent content in Data-Pruning on Kyle Roth</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Tue, 17 Sep 2024 20:52:39 -0400</lastBuildDate><atom:link href="https://kylrth.com/tags/data-pruning/index.xml" rel="self" type="application/rss+xml"/><item><title>Trivial or impossibleâ€”dichotomous data difficulty masks model differences (on ImageNet and beyond)</title><link>https://kylrth.com/paper/dichotomous-data-difficulty/</link><pubDate>Mon, 15 Aug 2022 14:21:56 -0400</pubDate><guid>https://kylrth.com/paper/dichotomous-data-difficulty/</guid><description>&lt;blockquote>
&lt;p>We observe that 48.2% [of] images [in ImageNet] are learned by all models regardless of their inductive bias; 14.3% [of] images are consistently misclassified by all models; only roughly a third (37.5%) of images are responsible for the differences between two models&amp;rsquo; decisions. We call this phenomenon dichotomous data difficulty (DDD).&lt;/p>
&lt;/blockquote>
&lt;p>The authors varied hyperparameters, optimizers, architectures, supervision modes, and sampling methods, finding that models only varied in performance on about a third of the images in the dataset. And this isn&amp;rsquo;t specific to ImageNet; they found similar results for CIFAR-100 and a synthetic Gaussian dataset. They use this measure to divide the dataset into &amp;ldquo;trivials&amp;rdquo;, &amp;ldquo;impossibles&amp;rdquo;, and &amp;ldquo;in-betweens&amp;rdquo;.&lt;/p></description></item><item><title>Beyond neural scaling laws: beating power law scaling via data pruning</title><link>https://kylrth.com/paper/beyond_nsl/</link><pubDate>Thu, 11 Aug 2022 14:09:08 -0400</pubDate><guid>https://kylrth.com/paper/beyond_nsl/</guid><description>&lt;p>In this paper they show that we can achieve exponential performance scaling over dataset size, when the samples added are pruned to be only the best examples. This beats power law scaling in a big way. There is still no free lunch, in some sense, because in most cases it will become progressively harder to add new useful samples as the dataset gets bigger. But this is a big deal for computation, because it means that the number of samples in the dataset is not nearly as important as the coverage and quality that the dataset provides.&lt;span class="sidenote-number">&lt;small class="sidenote">This also means that scaling laws for &lt;em>compute&lt;/em> (usually expressed as a function of dataset and model size) are dataset-specific and not generalizable, because of how much sample quality affects data scaling.&lt;/small>&lt;/span>&lt;/p></description></item></channel></rss>